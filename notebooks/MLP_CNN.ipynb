{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your first fully connected network and a CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple fully connected network (a Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the paths and make a dataset again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "currentdir = os.getcwd()\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.data_handling import WCH5Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's make our model. We'll talk about \n",
    "  - model parameters\n",
    "  - inputs and the forward method\n",
    "  - Modules containing modules\n",
    "  - Sequential Module  \n",
    "  Lets open [simpleMLP](/edit/models/simpleMLP.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simpleMLP import SimpleMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP=SimpleMLP(num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's look at the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of a parameter: fc1.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc1.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc2.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc2.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc5.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc5.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_MLP.named_parameters():\n",
    "    print(\"name of a parameter: {}, type: {}, parameter requires a gradient?: {}\".\n",
    "          format(name, type(param),param.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see by default the parameters have `requires_grad` set - i.e. we will be able to obtain gradient of the loss function with respect to these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly look at the [source](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear) for the linear module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters descend from the `Tensor` class. When `Parameter` object is instantiated as a member of a `Module` object class the parameter is added to `Module`s list of parameters automatically. This list and values are captured in the 'state dictionary' of a module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[ 2.6680e-03,  3.2562e-03, -3.4426e-03,  ...,  6.3970e-03,\n",
       "                       -5.7259e-03, -4.4637e-03],\n",
       "                      [ 1.7782e-03, -3.3476e-03,  1.1002e-03,  ...,  3.3208e-03,\n",
       "                       -2.1068e-03, -5.4390e-03],\n",
       "                      [ 1.5775e-05, -1.2682e-03,  3.8744e-03,  ..., -2.9397e-03,\n",
       "                       -5.8011e-03,  3.6649e-03],\n",
       "                      ...,\n",
       "                      [-5.7130e-03, -8.6100e-04,  3.4296e-03,  ...,  8.1327e-04,\n",
       "                        2.5296e-03,  3.6280e-04],\n",
       "                      [ 3.7208e-04,  3.8775e-03, -2.4209e-03,  ...,  3.5177e-03,\n",
       "                       -5.6878e-03,  1.8522e-03],\n",
       "                      [ 4.8859e-03,  6.2634e-03, -2.9589e-03,  ..., -2.3155e-04,\n",
       "                       -1.8440e-03,  5.8361e-03]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-2.5218e-03, -2.2686e-03, -2.4800e-03, -2.3030e-03,  5.7291e-03,\n",
       "                       4.3988e-03,  2.9257e-03,  3.2430e-03, -2.7270e-03, -2.5004e-03,\n",
       "                      -6.5969e-04, -6.3202e-03, -6.4015e-03,  2.6398e-03, -4.5060e-03,\n",
       "                       3.7911e-03,  4.1225e-03,  4.1730e-03, -2.1020e-04, -1.9984e-03,\n",
       "                       2.9553e-03, -2.3138e-03,  2.2418e-03,  5.1157e-03,  2.9813e-03,\n",
       "                       4.7682e-03,  6.1825e-03, -4.6742e-03,  4.5562e-03,  1.4931e-03,\n",
       "                       2.4083e-03, -3.2062e-03,  3.0266e-04,  7.3354e-04,  2.3200e-03,\n",
       "                      -3.2217e-03,  2.1952e-03, -4.0350e-03,  2.1622e-03,  3.1353e-03,\n",
       "                       4.4612e-03, -2.7491e-04, -1.4993e-03,  8.4156e-04, -3.3009e-03,\n",
       "                       9.8109e-04, -1.3846e-03,  4.9479e-03,  4.9276e-03,  5.8093e-03,\n",
       "                      -2.6513e-03, -6.0035e-03,  1.4376e-03, -4.9983e-04,  1.8158e-03,\n",
       "                       3.1828e-04,  6.1942e-03,  4.4078e-03,  3.0290e-03,  3.5764e-03,\n",
       "                      -2.3731e-03,  4.4146e-04, -5.5857e-03,  5.9539e-03, -3.9011e-03,\n",
       "                      -1.9349e-03, -3.6651e-05, -3.4595e-03, -4.6325e-03, -5.3757e-03,\n",
       "                      -9.8770e-04,  4.9363e-03,  3.8573e-03,  7.9695e-04,  5.3463e-03,\n",
       "                       3.8672e-03, -4.7034e-03,  5.7298e-03,  3.1812e-03, -2.8579e-03,\n",
       "                       5.4228e-03,  5.9208e-03,  6.2615e-03,  1.5127e-04, -1.9491e-03,\n",
       "                      -2.6018e-03,  1.5522e-03,  6.0318e-03,  1.0486e-03,  1.7205e-03,\n",
       "                       2.6190e-03,  4.2659e-03,  3.3438e-03,  2.1936e-03,  6.3756e-03,\n",
       "                      -6.4518e-04,  5.9444e-03,  6.2398e-03,  3.7880e-03,  3.7553e-03,\n",
       "                       4.3026e-03,  5.7036e-03, -3.8713e-03,  4.8548e-03,  2.4224e-03,\n",
       "                      -4.2567e-03, -1.8977e-03,  1.8510e-03, -1.0502e-03, -2.7517e-03,\n",
       "                       2.8334e-03,  6.1412e-03, -1.3057e-03, -1.5251e-03, -5.7760e-04,\n",
       "                      -2.9883e-03,  3.0244e-03, -5.4900e-03, -6.3418e-03, -4.7048e-03,\n",
       "                      -6.7429e-04,  3.6597e-03, -3.8789e-03,  1.9117e-03,  2.9411e-03,\n",
       "                       4.3850e-03,  3.0276e-03,  3.3555e-03,  2.7662e-04,  5.0247e-03,\n",
       "                      -4.2533e-03,  5.8059e-03, -4.9124e-03, -3.3031e-03, -4.8463e-04,\n",
       "                       5.3678e-03, -5.7993e-03, -2.2623e-03,  5.6640e-03,  2.7395e-03,\n",
       "                      -5.4212e-03, -6.3146e-03, -3.5159e-04,  3.0941e-03, -5.1078e-03,\n",
       "                       6.2465e-03,  3.9139e-03, -5.7883e-03,  1.2053e-03, -2.0103e-03,\n",
       "                       2.8324e-03,  1.7469e-03,  6.2604e-03,  2.4891e-03,  5.7461e-03,\n",
       "                      -2.3062e-06, -2.2076e-03, -8.1598e-04, -8.4972e-04, -8.5513e-05])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.0247, -0.0505, -0.0720,  ...,  0.0402, -0.0571, -0.0320],\n",
       "                      [ 0.0433, -0.0028,  0.0417,  ...,  0.0679,  0.0555,  0.0099],\n",
       "                      [-0.0678,  0.0118, -0.0671,  ..., -0.0388, -0.0472, -0.0608],\n",
       "                      ...,\n",
       "                      [-0.0186, -0.0148,  0.0335,  ...,  0.0304,  0.0634, -0.0493],\n",
       "                      [-0.0424,  0.0005, -0.0120,  ...,  0.0608, -0.0672,  0.0438],\n",
       "                      [-0.0333,  0.0333,  0.0142,  ..., -0.0041, -0.0766, -0.0006]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([ 4.2362e-02,  1.2253e-03, -8.6845e-05,  6.7404e-02,  6.4699e-02,\n",
       "                       1.0113e-02, -6.9068e-02,  3.1114e-02,  9.1390e-03,  6.7345e-02,\n",
       "                       7.7678e-02,  4.7677e-02, -4.2501e-02,  6.1005e-02,  6.2895e-02,\n",
       "                       7.6990e-02, -5.3724e-02,  4.5222e-02,  2.5794e-02,  7.0808e-02,\n",
       "                       1.6139e-02, -5.8864e-02, -6.2058e-02,  6.9113e-02, -1.6011e-03,\n",
       "                       6.6424e-02,  1.4283e-02, -5.3627e-03,  1.1124e-03,  6.3507e-02,\n",
       "                       5.8974e-02,  2.0057e-02, -6.5438e-02, -6.3652e-02, -1.9493e-03,\n",
       "                       5.7331e-02, -2.3474e-02,  4.9319e-02,  3.8411e-02, -6.9809e-02])),\n",
       "             ('fc5.weight',\n",
       "              tensor([[ 0.1054, -0.0733, -0.1227, -0.1305, -0.0283,  0.1493, -0.1129,  0.0209,\n",
       "                       -0.0901, -0.0943, -0.1484,  0.1473, -0.1576,  0.1419, -0.0981, -0.1397,\n",
       "                        0.0884,  0.0805, -0.1289, -0.0732,  0.0753,  0.0085, -0.0491, -0.0427,\n",
       "                       -0.0005,  0.1073,  0.1272,  0.0430, -0.0712, -0.0165, -0.1305, -0.0594,\n",
       "                        0.1408,  0.1132,  0.0064,  0.1191, -0.0177, -0.1349, -0.0559, -0.1514],\n",
       "                      [-0.0137, -0.0147,  0.0022, -0.0523,  0.0015,  0.0612,  0.0524,  0.0040,\n",
       "                        0.1553, -0.0135,  0.0513,  0.0374,  0.0031, -0.0349, -0.0014,  0.0813,\n",
       "                       -0.0694, -0.0727,  0.0191, -0.1184, -0.1580, -0.0117,  0.0914,  0.0451,\n",
       "                        0.1502, -0.0616, -0.0005, -0.0404, -0.1537, -0.1210,  0.1247,  0.0398,\n",
       "                        0.0599,  0.0203,  0.1088,  0.0619,  0.0200,  0.0887,  0.1136,  0.1568],\n",
       "                      [-0.0018, -0.1311,  0.1398, -0.0624, -0.1334,  0.0899, -0.0244, -0.1547,\n",
       "                        0.0608,  0.1369, -0.1424, -0.1374, -0.0474, -0.0035, -0.0624,  0.1053,\n",
       "                        0.0340, -0.1032,  0.1436,  0.1457,  0.0882,  0.0890,  0.0913, -0.0712,\n",
       "                        0.0880,  0.0584,  0.0835,  0.1359, -0.1140, -0.0092, -0.0710,  0.1186,\n",
       "                       -0.1539, -0.0316,  0.1239,  0.1535,  0.0123,  0.1093,  0.0800, -0.1464]])),\n",
       "             ('fc5.bias', tensor([-0.1332, -0.1280, -0.1183]))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_MLP.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that the values are not 0? This is actually by design - by default that initialization follows an accepted scheme - but many strategies are possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at sequential version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simpleMLP import SimpleMLPSEQ\n",
    "model_MLPSEQ=SimpleMLPSEQ(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of a parameter: _sequence.0.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.0.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.2.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.2.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.4.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.4.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.6.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.6.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.8.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.8.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_MLPSEQ.named_parameters():\n",
    "    print(\"name of a parameter: {}, type: {}, parameter requires a gradient?: {}\".\n",
    "          format(name, type(param),param.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('_sequence.0.weight', tensor([[ 2.8483e-03, -5.1011e-03, -2.7853e-04,  ...,  4.6351e-03,\n",
      "          2.0244e-03, -4.6997e-03],\n",
      "        [ 5.1449e-03,  2.9838e-03,  5.7291e-03,  ..., -5.7026e-03,\n",
      "         -4.9322e-04,  3.8901e-03],\n",
      "        [-1.9483e-03, -9.6000e-04, -5.0261e-03,  ..., -1.0807e-03,\n",
      "         -3.5274e-03, -9.9408e-04],\n",
      "        ...,\n",
      "        [-5.8123e-03, -5.2416e-03,  1.6391e-03,  ...,  4.5877e-05,\n",
      "         -6.2063e-03,  5.0585e-03],\n",
      "        [-4.8052e-03, -5.0750e-03, -1.1928e-03,  ..., -3.5907e-03,\n",
      "         -1.7558e-03,  6.1739e-03],\n",
      "        [ 6.1718e-03,  1.5871e-03,  4.4793e-03,  ...,  2.8583e-03,\n",
      "          8.3788e-04,  5.1749e-03]])), ('_sequence.0.bias', tensor([ 3.0710e-03, -4.4811e-03, -9.2219e-04,  5.0590e-03,  3.2422e-03,\n",
      "         1.2470e-03, -5.0901e-03, -1.9904e-03,  4.1131e-04,  3.6846e-03,\n",
      "         6.4616e-04, -1.3959e-03, -3.8742e-03,  8.9456e-04, -3.0524e-03,\n",
      "        -2.4453e-03, -2.8221e-03, -2.9666e-03,  1.8713e-03,  3.6737e-03,\n",
      "         4.1735e-03, -3.8488e-03, -4.0559e-03,  6.2315e-03,  3.9433e-03,\n",
      "         5.2797e-03, -3.8238e-03,  5.6244e-03, -3.1756e-03,  2.8153e-04,\n",
      "         4.4839e-03, -3.7825e-03,  3.6945e-03, -2.4488e-03, -3.9447e-03,\n",
      "        -2.7411e-03, -1.1137e-03,  2.4806e-04, -1.8600e-03,  2.9521e-03,\n",
      "        -7.6246e-04,  5.8193e-03, -8.5008e-05,  8.5605e-04, -1.6572e-04,\n",
      "         1.8170e-03, -5.7835e-03,  3.9169e-03, -1.4295e-04, -3.3631e-03,\n",
      "        -3.0386e-03,  1.7337e-03,  2.3668e-03,  3.4375e-03, -3.3414e-04,\n",
      "        -4.6649e-03, -6.4100e-03,  2.7429e-03, -6.0318e-03, -2.7474e-03,\n",
      "         2.2199e-03, -4.3380e-03,  2.0132e-04,  4.5811e-03,  7.0311e-04,\n",
      "         1.2746e-03,  8.6169e-04, -2.6283e-03, -5.1576e-03, -1.7197e-03,\n",
      "        -1.5830e-03, -2.4231e-03,  3.6993e-03,  8.3505e-04, -4.3286e-03,\n",
      "         3.4702e-03, -1.9909e-03,  6.0855e-04,  3.9285e-03,  6.2862e-03,\n",
      "         6.8258e-04,  5.0775e-03, -3.3335e-03, -4.5207e-03, -2.3432e-03,\n",
      "        -3.1854e-03, -4.8928e-03,  4.1138e-03,  6.3131e-04,  3.7966e-03,\n",
      "        -4.5444e-03,  1.5606e-03,  5.3530e-04,  4.3381e-03,  2.5158e-03,\n",
      "        -4.6146e-03,  2.0020e-03, -6.3750e-03, -6.6591e-04,  4.1108e-03,\n",
      "        -4.0551e-03, -6.3418e-03, -1.7052e-03,  6.3897e-03, -3.2259e-03,\n",
      "        -1.5341e-03,  5.6783e-04,  4.6022e-03,  3.4428e-03,  3.8090e-03,\n",
      "         1.8822e-03,  5.4196e-04,  1.4176e-03,  2.6228e-03,  2.9766e-04,\n",
      "        -1.6936e-03, -1.0792e-03, -5.9099e-03, -1.2534e-03,  4.4121e-03,\n",
      "         3.3652e-03,  4.3435e-03,  5.4152e-03,  6.3578e-03,  3.8294e-03,\n",
      "         3.5764e-03,  2.9298e-03,  3.1737e-03,  7.6818e-04, -5.3640e-03,\n",
      "        -5.1839e-04, -3.0695e-03, -5.6793e-03, -4.8736e-03, -6.1231e-03,\n",
      "         5.4631e-03, -4.1574e-04,  5.1845e-03,  2.9081e-03,  5.6540e-04,\n",
      "         5.1969e-03,  3.6258e-03,  1.2533e-03, -3.7018e-03, -5.3317e-03,\n",
      "        -3.5076e-03,  5.8971e-03, -4.7670e-03,  4.2052e-04, -8.1855e-04,\n",
      "         3.0042e-03,  5.2096e-03,  1.9228e-03, -1.8410e-03,  3.9732e-03,\n",
      "        -2.8712e-03,  4.3399e-04,  5.7153e-03,  3.9798e-03,  5.7464e-03,\n",
      "        -1.6841e-04, -4.8001e-03,  6.1717e-03, -3.6255e-04, -4.0600e-03,\n",
      "         5.1691e-03,  4.2802e-03, -6.1103e-03,  1.6486e-03, -8.7428e-05,\n",
      "        -3.3993e-03, -5.2300e-03, -5.1101e-03, -1.8030e-03, -5.8920e-03,\n",
      "         3.4808e-03, -2.8258e-03, -5.1177e-03, -4.6716e-03, -4.3036e-03,\n",
      "         6.4087e-03, -2.2775e-03,  3.0227e-03,  6.9250e-04,  4.0421e-03,\n",
      "         1.4515e-03,  4.3037e-03,  8.0581e-04, -4.1598e-04, -1.1480e-03,\n",
      "        -7.9587e-04,  4.1193e-03, -5.9778e-03,  2.9895e-03, -3.1651e-03,\n",
      "         3.5318e-03,  4.5278e-03,  2.4684e-03,  3.9751e-03, -4.9262e-03,\n",
      "         5.0174e-03, -3.9233e-03, -1.5928e-03, -2.0631e-03,  8.0507e-04,\n",
      "        -3.9364e-03, -5.5663e-03,  4.6685e-03,  5.0538e-03, -4.2860e-03,\n",
      "         4.4480e-03, -5.3123e-03, -5.8785e-03, -2.0438e-03, -5.8716e-03,\n",
      "        -5.3476e-03,  2.1579e-04, -2.1843e-03, -1.6498e-03, -3.9167e-03,\n",
      "         2.5958e-03, -3.1178e-03,  1.4245e-04,  6.8851e-04,  2.3616e-03,\n",
      "        -6.3560e-03, -4.7902e-03,  1.6123e-03,  2.9088e-03,  2.6733e-03,\n",
      "        -8.8462e-04,  6.3252e-03, -1.9914e-03, -2.6258e-03,  2.7599e-03,\n",
      "        -4.9600e-03,  5.3038e-03, -4.1674e-03,  2.0178e-03,  6.1068e-03,\n",
      "         6.9077e-04,  5.2544e-03,  6.0721e-03,  2.3439e-03, -2.6977e-03,\n",
      "         1.4701e-03,  5.5059e-03,  4.8111e-03, -5.6926e-03,  4.7871e-03,\n",
      "         5.9879e-03, -1.1172e-03,  3.6182e-03, -1.1776e-03,  1.1808e-04,\n",
      "         5.1612e-03,  1.7601e-03, -1.7129e-03, -4.1652e-03,  1.9021e-03,\n",
      "         2.8897e-03, -2.2939e-03, -5.0615e-03,  3.6424e-03, -2.1609e-04,\n",
      "         1.6850e-03, -5.0360e-03,  8.8250e-05, -2.1511e-03,  2.3445e-03,\n",
      "        -3.4113e-03, -8.2301e-04,  1.7346e-03,  5.8913e-03, -3.4109e-03,\n",
      "        -4.8825e-04,  4.8384e-03, -6.5256e-04, -5.3864e-03, -3.5639e-03,\n",
      "        -4.4495e-03,  4.0645e-03, -4.6332e-03, -5.7336e-03, -6.2140e-03,\n",
      "        -3.5654e-03, -2.8079e-03, -4.0822e-03, -3.5161e-03,  2.6101e-03,\n",
      "        -5.7095e-03, -9.3169e-04,  5.9651e-03,  3.9010e-03, -2.9023e-04,\n",
      "        -3.8452e-03,  2.1040e-03, -1.2688e-03,  1.1105e-03,  3.4678e-03,\n",
      "        -1.9622e-03,  1.0612e-03, -2.2089e-03,  2.0668e-03, -8.5096e-04,\n",
      "        -1.2152e-03, -2.7765e-03,  4.3749e-03,  4.9168e-03, -7.1247e-04,\n",
      "         3.2738e-03,  1.3884e-03, -4.9821e-03, -3.1007e-03, -2.3387e-03,\n",
      "        -3.9376e-03, -1.4783e-04,  3.4594e-03, -4.8124e-03,  5.4649e-03,\n",
      "         7.8751e-04,  4.3287e-04,  2.3070e-03, -2.1141e-04, -7.1696e-04,\n",
      "         1.1034e-03, -7.6765e-04, -3.1117e-03, -3.8726e-04,  2.2907e-03,\n",
      "        -4.2061e-03, -2.9026e-03, -5.3509e-03, -2.3240e-03, -1.0306e-04,\n",
      "         5.7705e-03, -2.4497e-03, -2.5615e-03,  3.4086e-03,  4.0074e-03,\n",
      "        -5.6783e-03, -1.9443e-04,  5.8436e-03, -4.2719e-03, -4.3859e-03,\n",
      "        -5.8766e-03,  7.7921e-04,  3.3397e-03, -2.9938e-03, -5.7308e-03,\n",
      "        -1.1643e-03,  4.3686e-03, -1.4519e-03,  3.5197e-03, -1.1673e-03,\n",
      "        -3.1305e-03,  1.1706e-03,  1.0598e-03,  1.7916e-03, -3.6406e-03,\n",
      "        -4.8988e-03, -3.5710e-03,  3.0666e-03,  4.2213e-03, -2.8458e-03,\n",
      "         1.9064e-03,  5.1260e-03, -5.5373e-04, -4.7868e-03,  3.3084e-03,\n",
      "        -3.8633e-03,  5.0910e-03,  1.0020e-03, -5.5501e-03, -4.3324e-03,\n",
      "        -1.5251e-03, -3.6008e-03,  4.3493e-03,  4.7495e-03,  1.0474e-03,\n",
      "         5.8013e-03,  2.4057e-03, -6.2064e-03,  6.4453e-04, -4.0247e-03,\n",
      "        -4.2287e-03, -5.4168e-03, -3.7947e-03,  5.4719e-03, -8.1152e-04,\n",
      "         5.5415e-03,  3.3470e-03,  2.1080e-03,  6.0976e-03, -5.6419e-03,\n",
      "         2.2374e-03, -3.3126e-03,  1.2786e-03,  4.8030e-03,  6.0462e-03,\n",
      "        -4.9774e-03,  7.1649e-04,  7.4863e-04,  2.6358e-03,  3.8623e-03,\n",
      "         2.8724e-03, -1.4119e-03,  4.9675e-03,  3.9634e-03,  2.1794e-03,\n",
      "         2.4012e-04, -5.6098e-03, -1.3061e-03,  4.1952e-03, -4.7395e-03,\n",
      "         1.7055e-03,  5.7887e-03,  1.5206e-03,  9.8056e-04, -5.9090e-03,\n",
      "         1.6038e-03, -8.3225e-04,  2.8037e-03,  2.8189e-03,  1.2052e-03,\n",
      "         2.6205e-03,  3.1767e-03,  4.9961e-03, -2.9703e-03, -5.8312e-03,\n",
      "        -1.8134e-03,  4.6295e-03, -5.2566e-03,  3.7488e-03, -1.3454e-03,\n",
      "        -4.5123e-03, -5.1063e-03,  2.1288e-03,  1.3641e-04, -1.7076e-03,\n",
      "        -2.2866e-03,  3.6190e-03, -4.8058e-03,  9.8907e-04, -5.2514e-03,\n",
      "        -4.5446e-03, -5.5586e-03, -4.6287e-03,  4.7290e-03,  2.1513e-03,\n",
      "        -4.3114e-03,  2.5322e-03, -2.0950e-03,  3.2770e-03, -4.8341e-03,\n",
      "        -2.8457e-03, -2.9064e-03,  4.8620e-03, -4.1440e-03,  2.8352e-03,\n",
      "        -3.1670e-03, -2.9513e-03, -2.4036e-03, -1.0266e-03,  3.6819e-04,\n",
      "         4.5535e-03,  6.2232e-03, -5.3538e-03,  3.8241e-03, -1.6651e-03,\n",
      "        -2.4962e-03, -6.5414e-04, -4.8148e-03, -2.5977e-03,  5.1364e-03,\n",
      "         5.0647e-03, -1.4090e-03, -2.3650e-03,  5.2053e-03,  5.6837e-03,\n",
      "         4.2595e-03, -4.9016e-03, -4.2045e-03, -6.2215e-03,  4.2280e-03,\n",
      "        -1.3642e-03,  5.7643e-03,  1.8805e-03,  2.5227e-03, -4.3496e-03,\n",
      "         1.8699e-03, -6.7125e-04,  2.4148e-03,  2.2756e-03,  6.3725e-03,\n",
      "         5.7612e-03, -6.2737e-03,  3.6691e-03, -1.7647e-03,  5.4219e-03,\n",
      "        -4.9902e-05, -3.1571e-03,  1.5873e-03,  1.5083e-03,  1.0035e-03,\n",
      "        -3.2700e-03, -2.5565e-03, -5.9919e-03, -5.2213e-03,  2.0735e-03,\n",
      "         5.9287e-03,  2.6708e-03, -3.2831e-03, -2.9179e-03,  2.5412e-04,\n",
      "         2.8526e-04, -2.5587e-03,  1.1024e-03,  2.6326e-03, -4.4301e-04,\n",
      "         5.8818e-03, -4.7158e-03,  4.1062e-03, -5.3298e-03, -4.5119e-03,\n",
      "         2.7991e-03,  4.5057e-03, -2.6200e-03,  6.2550e-03,  4.1366e-03,\n",
      "        -4.9930e-03,  3.5192e-03,  3.7981e-03,  2.3477e-03,  3.8759e-03,\n",
      "        -3.1704e-03,  7.8724e-04, -4.7265e-03, -2.9314e-03, -5.2129e-03,\n",
      "         2.4157e-03, -4.7068e-03,  3.4558e-03, -4.7464e-03, -3.1273e-03,\n",
      "         4.5840e-03,  6.2117e-03, -5.6256e-04,  2.0919e-03,  4.8523e-03,\n",
      "        -4.6082e-04,  4.9008e-03, -6.0186e-04,  4.6278e-03,  2.4305e-03,\n",
      "         3.0563e-03,  2.3032e-03, -3.5996e-03, -1.9187e-03, -8.5801e-04,\n",
      "         5.2162e-03,  3.1421e-03, -2.7510e-03,  5.0529e-03,  8.7970e-04,\n",
      "         2.4315e-03,  4.2471e-03,  1.4314e-03,  2.2565e-03,  5.4116e-03,\n",
      "        -3.3209e-03,  1.3727e-03,  5.5240e-03,  1.3447e-03,  1.9970e-03,\n",
      "         1.4730e-03, -4.3587e-03, -5.7343e-03, -1.7526e-03,  4.1490e-03,\n",
      "        -1.9343e-03, -4.9763e-03, -1.9860e-03, -5.5617e-03,  2.0466e-03,\n",
      "        -4.9807e-03, -3.4527e-03, -1.8922e-03, -1.0688e-03, -3.0165e-03,\n",
      "         5.6766e-03, -6.2203e-03, -2.8051e-03,  3.0178e-03, -6.3367e-03,\n",
      "        -4.6593e-04, -4.3312e-03,  8.8305e-04,  3.6266e-03, -4.3962e-04,\n",
      "         3.8436e-03, -3.4536e-04,  1.3955e-03,  1.7770e-03,  6.7169e-04,\n",
      "         1.0620e-03, -2.4245e-03, -1.8098e-03,  1.1162e-03, -4.6705e-03,\n",
      "         6.1651e-03, -1.5148e-03, -6.8651e-05, -5.0162e-03,  3.0192e-03,\n",
      "         4.2249e-03, -3.8720e-03,  6.0239e-03,  1.6774e-04, -5.3107e-03,\n",
      "         2.1381e-04,  3.3187e-03,  3.4032e-03,  4.3078e-03,  4.2535e-03,\n",
      "         1.1900e-03,  5.2716e-03, -8.3488e-04,  5.9090e-03,  4.9085e-03,\n",
      "         4.8810e-03, -4.6207e-03,  5.9880e-03,  2.1787e-04, -3.9301e-03,\n",
      "         4.7681e-03,  1.6759e-03, -5.5451e-03,  4.5308e-03, -3.8931e-03,\n",
      "         6.1166e-03,  5.2811e-03,  3.4829e-04, -3.1357e-03, -8.9301e-05,\n",
      "        -3.1887e-03,  1.3892e-03,  3.6066e-03, -2.5613e-03, -5.5502e-03,\n",
      "         5.7056e-03, -1.9078e-03, -1.4341e-03, -5.7558e-03,  1.8956e-03,\n",
      "         5.4467e-03,  3.7593e-03,  3.7367e-03, -1.0133e-03,  2.6521e-03,\n",
      "         1.4846e-03, -4.6801e-03, -4.5689e-03,  3.4520e-03, -5.2868e-03,\n",
      "         6.1815e-03,  4.1832e-03, -1.3076e-03,  5.4603e-03,  1.1032e-04,\n",
      "        -3.4738e-03, -3.8873e-03, -4.3599e-03,  1.8685e-04, -4.5589e-03,\n",
      "         2.9445e-03,  1.3688e-03,  6.0155e-03,  1.5531e-04,  2.0537e-04,\n",
      "         1.8440e-03, -5.2680e-03,  8.2071e-04, -2.2460e-03,  4.5410e-03,\n",
      "        -4.6979e-03,  1.4028e-03,  1.9289e-04,  4.7252e-03,  5.2753e-03,\n",
      "        -1.9019e-03,  2.5184e-03, -3.8470e-03,  4.6127e-04, -5.6674e-04,\n",
      "         4.5377e-03, -5.0698e-03,  1.2570e-03,  2.5816e-03, -1.5638e-03,\n",
      "        -6.1246e-03,  1.2238e-03, -2.0095e-03,  2.1887e-03,  3.3146e-03,\n",
      "        -3.1879e-03,  4.1863e-03,  4.4026e-03,  6.6402e-04, -3.3062e-03,\n",
      "        -3.6798e-03,  2.2727e-03, -3.6512e-03,  4.6376e-03,  3.9596e-03,\n",
      "         4.6398e-04, -4.5884e-03, -5.3978e-03,  1.5584e-03,  5.9934e-03,\n",
      "        -1.5808e-04, -5.0067e-04,  5.3253e-03,  3.8881e-03,  3.8601e-03,\n",
      "         4.2512e-03,  4.2349e-03,  8.2390e-04, -3.3396e-03, -3.8586e-03,\n",
      "        -2.0549e-03, -5.3563e-04,  4.5560e-03,  3.1187e-03, -1.4795e-03,\n",
      "         4.9864e-04,  9.8162e-04, -3.1763e-03,  3.1488e-03, -4.1229e-03,\n",
      "         5.0361e-04,  3.6198e-03, -1.0725e-03, -1.6177e-03, -3.2370e-03,\n",
      "         2.4562e-03,  2.1082e-04,  5.7730e-04,  3.5308e-03, -4.6733e-03,\n",
      "         1.7964e-03,  5.5924e-03,  6.2777e-03, -2.4096e-03, -3.8627e-03,\n",
      "         3.1951e-03,  2.2589e-03,  6.2937e-03, -2.8861e-03, -5.5001e-03,\n",
      "        -6.3682e-03, -6.3153e-03, -5.3965e-03, -6.3585e-03,  5.1295e-03,\n",
      "        -6.3703e-03, -9.5268e-04,  1.4075e-03, -1.5851e-03, -5.4215e-03,\n",
      "         5.0338e-03,  3.6756e-03, -3.2214e-03, -4.8946e-03, -4.5153e-03,\n",
      "         3.5562e-03,  6.3682e-03,  1.3382e-03, -7.2892e-04, -3.6212e-03,\n",
      "        -3.9539e-03, -2.1116e-03,  7.1766e-04, -4.4829e-03, -1.0227e-03,\n",
      "         2.6670e-03, -3.8203e-03,  4.6152e-03, -6.2026e-04, -5.7228e-04,\n",
      "         3.1059e-03, -1.1520e-03,  2.5734e-03, -7.9145e-05,  5.8598e-04,\n",
      "        -6.4033e-03,  3.2676e-03,  5.2999e-03,  1.1677e-03,  9.2676e-04,\n",
      "         2.8566e-03,  3.7233e-03, -5.6469e-04,  1.7367e-03,  5.3466e-04,\n",
      "         2.8423e-03,  5.3896e-03, -2.1132e-03,  4.1588e-03,  5.3177e-03,\n",
      "        -5.2926e-03, -2.7486e-04,  5.8723e-03, -2.9863e-03,  2.6593e-03,\n",
      "         1.2385e-03,  2.3318e-03, -2.1443e-03, -4.0573e-03,  4.2400e-03,\n",
      "        -1.3451e-03,  2.4662e-03, -3.3897e-04, -4.0710e-03,  3.8052e-03,\n",
      "        -1.3951e-03,  3.0669e-03, -5.2741e-03,  2.3387e-03, -9.8954e-04,\n",
      "         1.9476e-03, -1.4690e-03,  4.0311e-03,  4.2850e-03, -1.3419e-03,\n",
      "        -3.7557e-04,  1.5862e-03, -2.5632e-03,  3.1175e-03,  3.5634e-03,\n",
      "        -3.2944e-03,  3.8770e-03, -2.7825e-03, -5.0023e-04,  4.9510e-03,\n",
      "         4.8837e-03,  6.3823e-03,  1.4599e-03, -1.1482e-03,  1.6613e-03,\n",
      "        -8.3576e-04, -1.0923e-03, -5.9169e-03,  4.7673e-03, -3.8785e-03,\n",
      "        -3.0392e-03, -4.5207e-03, -3.7711e-04,  3.2066e-03, -3.9493e-03,\n",
      "        -2.0543e-03,  5.9357e-03, -3.7577e-03, -1.9824e-03, -1.2151e-03,\n",
      "        -5.1549e-03, -3.8174e-03,  1.1158e-04, -2.4966e-03,  4.2733e-04,\n",
      "         4.3263e-03, -1.2623e-03,  4.0156e-03,  8.1212e-04,  1.7565e-03,\n",
      "         3.6176e-03,  4.7778e-03,  2.3293e-03, -4.6681e-03,  5.2472e-03,\n",
      "        -6.2139e-03,  3.8260e-03,  5.0453e-03,  3.6307e-04,  2.5332e-03,\n",
      "        -1.7115e-06,  5.3168e-03,  4.5459e-03, -4.4832e-03, -1.6198e-03,\n",
      "         2.8185e-03,  4.9015e-03,  4.6161e-03,  1.1664e-03, -2.7111e-03,\n",
      "        -3.0754e-04, -2.9055e-03,  3.8220e-03,  1.2968e-03, -2.4385e-03,\n",
      "         1.4648e-03,  5.4397e-03, -2.9589e-03,  2.1691e-03,  3.5345e-04,\n",
      "        -1.9927e-03, -2.9749e-05,  1.6306e-03, -2.7647e-03,  1.8803e-03,\n",
      "        -4.5569e-03,  3.8271e-03,  3.3045e-03, -7.5391e-04,  3.9018e-03,\n",
      "        -5.5721e-03, -4.2586e-03,  5.2180e-03, -3.9788e-03, -4.4560e-03,\n",
      "         5.3235e-03, -5.7791e-03, -5.3108e-03,  1.7650e-03, -4.7192e-03,\n",
      "        -5.9568e-03,  2.8351e-03,  5.6646e-03, -5.9620e-04, -4.1403e-03,\n",
      "        -4.3481e-03,  1.8535e-03,  5.2707e-03, -5.9968e-03,  1.2305e-03,\n",
      "        -3.2767e-03,  1.7150e-04,  5.9814e-03,  1.9187e-03, -4.2028e-03,\n",
      "        -6.0241e-04, -3.5162e-03, -1.9669e-03,  6.0483e-03, -3.6194e-03,\n",
      "        -3.0650e-03, -3.7557e-03,  4.7586e-03, -3.7569e-03,  8.2259e-04,\n",
      "         1.0929e-03,  4.5473e-03,  4.7916e-04,  1.2987e-03,  5.5776e-03,\n",
      "         4.7376e-03,  1.4187e-03, -5.7413e-03,  3.9449e-03, -2.5884e-03,\n",
      "        -5.3806e-04, -1.7921e-03,  4.6809e-03, -3.9189e-03, -5.1660e-03,\n",
      "        -3.5659e-03,  3.1771e-03,  8.1345e-04,  5.0300e-03, -6.9445e-04,\n",
      "        -1.8981e-03,  2.0055e-03])), ('_sequence.2.weight', tensor([[ 0.0012, -0.0200,  0.0108,  ...,  0.0042, -0.0057, -0.0236],\n",
      "        [-0.0172, -0.0316,  0.0239,  ...,  0.0021, -0.0248,  0.0303],\n",
      "        [ 0.0218, -0.0042,  0.0047,  ..., -0.0285,  0.0179,  0.0291],\n",
      "        ...,\n",
      "        [ 0.0248, -0.0054,  0.0144,  ..., -0.0213,  0.0229, -0.0234],\n",
      "        [ 0.0078, -0.0157, -0.0229,  ..., -0.0172, -0.0174,  0.0257],\n",
      "        [-0.0285,  0.0211,  0.0031,  ..., -0.0113,  0.0209, -0.0188]])), ('_sequence.2.bias', tensor([ 1.0978e-03, -1.9187e-02,  1.0592e-02, -1.9588e-02,  6.0456e-03,\n",
      "        -9.6421e-03, -1.4711e-03, -2.2909e-02, -1.3027e-02,  2.6806e-02,\n",
      "         1.5651e-02,  1.7707e-02,  1.2179e-02,  2.5997e-02, -5.5857e-03,\n",
      "         1.7344e-03,  1.4409e-02, -6.7433e-03, -1.3719e-02,  1.6940e-02,\n",
      "        -7.2981e-03, -2.8572e-02,  1.8817e-02, -1.7402e-02, -2.2406e-03,\n",
      "        -1.5120e-02,  1.9523e-02,  2.6198e-02,  2.1526e-02, -9.6383e-03,\n",
      "         2.7929e-02,  2.6934e-02,  9.8512e-03,  1.9231e-02,  2.4117e-02,\n",
      "        -2.1907e-02,  4.8469e-03,  1.8989e-02, -1.4711e-02,  2.0953e-02,\n",
      "         1.3401e-02, -2.3328e-02,  9.4199e-03, -1.3656e-02,  2.7537e-02,\n",
      "        -3.1207e-03, -2.2469e-02, -1.2952e-02,  1.8666e-02,  1.4622e-02,\n",
      "        -1.1402e-02,  1.2219e-02,  1.9307e-02, -8.2438e-03, -1.5332e-02,\n",
      "        -8.3424e-03,  2.1859e-02, -2.1124e-02,  7.6976e-03, -7.9210e-03,\n",
      "         1.6337e-02,  2.0315e-02, -1.5235e-02, -1.8622e-02, -9.5883e-03,\n",
      "        -5.2510e-03,  2.0153e-02, -2.3010e-02, -2.5505e-02,  5.0996e-03,\n",
      "         1.9530e-02,  1.6059e-02, -1.1680e-02,  1.2398e-03,  1.7302e-02,\n",
      "         2.9568e-02, -1.9938e-02,  2.0919e-02,  2.2157e-02, -8.0906e-03,\n",
      "        -1.3146e-02,  2.8792e-02,  2.4472e-02,  3.0417e-02,  1.3158e-02,\n",
      "        -2.3365e-02,  2.2308e-03,  3.1291e-02,  2.5036e-02, -1.4239e-02,\n",
      "        -1.0718e-02,  3.4648e-03, -2.0239e-03, -3.1080e-02, -2.8182e-04,\n",
      "        -9.5813e-04, -4.1704e-03, -1.1551e-02, -1.5103e-02, -1.4262e-02,\n",
      "         9.4175e-03, -1.3956e-02, -2.4097e-02,  1.7303e-02,  1.0947e-02,\n",
      "        -1.2306e-02, -1.1825e-02, -2.7712e-02,  1.2256e-02, -2.5519e-02,\n",
      "         6.2862e-03,  1.0347e-02, -1.8960e-03,  1.3043e-02,  1.1953e-02,\n",
      "         2.5200e-02, -1.0337e-02, -1.3178e-02,  2.7268e-02,  1.6178e-02,\n",
      "        -1.1457e-02, -6.6393e-03,  5.6879e-03, -8.2846e-03, -1.1887e-03,\n",
      "        -1.7208e-03, -1.1006e-02,  2.3084e-02, -2.6943e-02, -2.8473e-02,\n",
      "         1.8022e-02,  3.1628e-02,  2.9102e-02,  9.9642e-03, -2.9330e-03,\n",
      "        -7.3266e-03,  2.4311e-02,  5.6209e-03,  1.7039e-02, -1.9164e-03,\n",
      "         2.0141e-02, -9.4328e-03, -2.1736e-02,  8.3928e-03, -6.3264e-03,\n",
      "        -1.6301e-02,  2.3414e-02, -9.0237e-03,  7.0764e-03,  3.0008e-02,\n",
      "        -2.5622e-02, -2.1211e-03,  2.7392e-02,  2.9124e-02, -2.2632e-02,\n",
      "         1.0637e-02,  1.2097e-02, -5.4498e-03, -7.9129e-03, -3.7661e-04,\n",
      "        -3.0183e-02,  3.0089e-02,  1.4055e-02, -3.1265e-02, -1.7836e-02,\n",
      "         4.9038e-03,  2.8115e-02,  2.0817e-02, -2.6898e-02,  3.0365e-03,\n",
      "         8.2691e-03,  2.6618e-02, -2.0554e-02, -8.2886e-03,  1.9659e-02,\n",
      "        -4.9918e-03, -1.4322e-02,  1.5079e-03,  6.5999e-03,  2.3323e-02,\n",
      "         4.3826e-03, -1.7166e-02,  2.7839e-02, -1.0793e-02, -1.4204e-02,\n",
      "         1.0428e-02, -1.9614e-02,  3.2931e-03,  1.1838e-02, -1.5632e-03,\n",
      "         8.7021e-03, -7.8842e-04,  1.5692e-02, -4.9199e-03,  1.7857e-02,\n",
      "         1.5687e-02, -2.5848e-02, -1.8670e-04,  2.6258e-02, -3.0372e-02,\n",
      "         5.7620e-03,  7.1325e-03,  2.3246e-02,  8.3589e-03,  1.2998e-02,\n",
      "        -2.7998e-02,  2.0950e-02,  1.5096e-02, -2.2308e-02, -1.2636e-02,\n",
      "         1.4820e-02, -7.3144e-03, -2.3270e-02, -1.6089e-02, -2.6155e-02,\n",
      "        -1.6242e-02,  1.6735e-02,  2.2160e-02, -2.7409e-02,  9.6821e-03,\n",
      "        -2.3940e-02,  5.8250e-03,  1.8591e-02, -1.7227e-02,  1.8763e-02,\n",
      "         1.2251e-02, -2.9753e-02, -2.7990e-02,  2.4191e-02,  2.1913e-02,\n",
      "         8.9777e-03,  1.4274e-02, -7.7188e-03, -2.1048e-02,  6.0227e-03,\n",
      "        -3.0811e-02,  3.0534e-02,  3.0041e-03, -2.2132e-02, -6.9120e-03,\n",
      "         3.0654e-02,  2.3886e-02,  1.2133e-02,  2.0275e-02,  1.0010e-02,\n",
      "        -6.4172e-03,  1.5955e-02,  1.4651e-03,  3.0743e-02, -2.9933e-02,\n",
      "         2.1634e-02, -4.4115e-03,  1.0148e-02,  2.5948e-02,  1.0573e-02,\n",
      "        -2.3602e-02,  2.2725e-02,  4.6987e-03,  1.9647e-02,  1.6312e-02,\n",
      "         1.8392e-03, -2.3582e-02,  1.3341e-02, -1.7587e-02,  6.4824e-03,\n",
      "        -2.3610e-02,  2.8090e-02,  2.7467e-02,  3.1502e-02, -5.2242e-03,\n",
      "         1.8807e-02, -1.8802e-02,  1.9151e-02, -1.6555e-02, -2.1617e-03,\n",
      "        -9.9460e-03, -7.3607e-03,  2.8364e-02,  1.3855e-02,  1.7666e-02,\n",
      "        -1.4115e-02, -2.9203e-02, -9.4860e-03,  2.1686e-02,  1.7186e-03,\n",
      "        -1.3384e-02, -3.1695e-02,  1.9923e-02, -6.2153e-05,  1.2345e-02,\n",
      "        -2.4575e-02, -1.0236e-02, -1.8501e-02,  1.6375e-02, -2.7955e-03,\n",
      "        -2.3395e-03,  1.2032e-02,  2.7439e-02,  9.4249e-03,  9.5279e-03,\n",
      "         6.5413e-04,  2.9182e-03,  2.5875e-02, -2.3117e-02,  5.7201e-03,\n",
      "         1.4432e-02,  2.1434e-02,  6.0239e-03,  7.9256e-03, -1.1660e-02,\n",
      "        -1.2135e-02, -6.6021e-03, -2.8758e-02, -1.5979e-02, -3.1778e-02,\n",
      "         2.7834e-02,  5.3178e-03,  3.5366e-03, -1.5378e-03, -1.1852e-02,\n",
      "        -2.1474e-02,  1.0260e-03,  5.8564e-03,  1.1003e-02,  2.3158e-02,\n",
      "        -3.2978e-03,  1.3463e-02, -2.4587e-02,  4.0429e-03,  2.3302e-03,\n",
      "        -1.4418e-02,  2.1919e-02, -1.4853e-02, -2.9464e-02, -9.0395e-03,\n",
      "         2.2401e-02,  3.1497e-02,  6.6495e-03,  2.9281e-02, -2.7038e-02,\n",
      "        -1.3743e-03,  3.6033e-03, -2.4200e-02, -1.4319e-02,  1.5977e-02,\n",
      "         2.1725e-02,  1.6480e-02,  2.7875e-02, -2.7702e-04, -9.9154e-03,\n",
      "        -3.1570e-02, -8.2678e-03, -2.4790e-02,  8.2702e-03,  8.9404e-03,\n",
      "        -1.0835e-02,  2.5223e-02,  7.1014e-03, -1.4659e-03,  2.1872e-02,\n",
      "        -1.0586e-02,  1.2664e-02, -6.2837e-03,  6.6988e-03,  4.8156e-03,\n",
      "        -1.8171e-02,  1.0805e-02,  1.5603e-02, -3.1612e-02, -1.1111e-02,\n",
      "        -4.0212e-03, -2.8546e-02, -1.4834e-02,  8.6166e-03,  6.3669e-03,\n",
      "         7.3171e-03, -2.4971e-02,  8.4654e-03, -1.6857e-02,  2.6840e-02,\n",
      "        -6.2563e-03, -9.8036e-04,  3.9155e-03, -7.5380e-03,  2.3552e-02,\n",
      "         6.5119e-03, -7.4958e-03, -2.8520e-02, -2.0652e-02, -9.2166e-03,\n",
      "        -1.4674e-02,  2.5536e-02,  2.5987e-02,  6.2984e-03, -1.6427e-02,\n",
      "         2.8251e-02, -3.1894e-02, -1.8173e-03, -1.0332e-02,  1.5347e-02,\n",
      "         5.1024e-03, -8.7194e-03,  1.4277e-02, -3.1760e-02, -2.1933e-02,\n",
      "         1.5680e-02,  1.6122e-02,  2.4843e-02,  8.4719e-03,  4.8474e-03,\n",
      "        -1.2025e-02,  1.4095e-02, -1.3106e-02, -1.0245e-02,  1.5463e-02,\n",
      "        -5.3201e-03, -1.4282e-02,  1.1699e-02,  2.6160e-02, -1.9774e-02,\n",
      "         2.5462e-02,  2.1821e-02,  9.4357e-03,  9.0570e-03,  2.4841e-02,\n",
      "        -1.7154e-03, -3.6801e-03,  1.2855e-02,  3.1204e-02,  2.3962e-02,\n",
      "         1.1250e-02, -1.8488e-02,  2.7929e-02,  9.5594e-03, -6.4930e-03,\n",
      "        -2.2083e-02, -6.4464e-03,  6.5535e-03,  1.4500e-02,  1.8885e-03,\n",
      "        -2.3632e-02, -2.6661e-02, -3.5031e-03, -2.9283e-02, -1.4795e-02,\n",
      "         6.2482e-03, -5.5487e-03, -2.6116e-02,  2.8486e-02,  1.8092e-03,\n",
      "         2.9970e-02, -2.1299e-02,  1.2432e-02,  8.3384e-03,  6.1158e-03,\n",
      "        -2.7161e-02,  9.0640e-03,  1.8852e-02, -7.7535e-03, -2.3155e-02,\n",
      "        -6.5514e-03,  8.6988e-03,  9.5087e-04,  1.6261e-02, -2.4726e-02,\n",
      "         5.8787e-03,  2.6487e-02,  1.0405e-02, -3.1057e-02,  1.3605e-02,\n",
      "         1.9348e-02, -1.9702e-02,  3.8836e-03,  6.2572e-03, -2.6340e-02,\n",
      "         2.7563e-02,  1.2907e-02,  2.6257e-02,  1.3486e-02,  1.0105e-02,\n",
      "         2.2300e-02, -2.2194e-02, -2.0513e-02, -1.7552e-04,  2.4386e-02,\n",
      "        -2.2609e-02])), ('_sequence.4.weight', tensor([[ 0.0215, -0.0217,  0.0336,  ..., -0.0200, -0.0366, -0.0440],\n",
      "        [-0.0349, -0.0165,  0.0422,  ...,  0.0327, -0.0340,  0.0173],\n",
      "        [ 0.0429,  0.0448,  0.0131,  ...,  0.0235, -0.0232, -0.0307],\n",
      "        ...,\n",
      "        [ 0.0399, -0.0242, -0.0342,  ..., -0.0115, -0.0043,  0.0003],\n",
      "        [-0.0392,  0.0059, -0.0451,  ..., -0.0271, -0.0094,  0.0258],\n",
      "        [-0.0205, -0.0110,  0.0187,  ...,  0.0295,  0.0164,  0.0387]])), ('_sequence.4.bias', tensor([ 0.0237,  0.0009, -0.0433, -0.0240, -0.0348, -0.0039,  0.0058,  0.0022,\n",
      "        -0.0250,  0.0028, -0.0157,  0.0038, -0.0097, -0.0193, -0.0057, -0.0353,\n",
      "        -0.0196,  0.0245, -0.0118,  0.0095,  0.0402, -0.0296, -0.0113, -0.0438,\n",
      "        -0.0311, -0.0270, -0.0389, -0.0034,  0.0270, -0.0041, -0.0328, -0.0389,\n",
      "         0.0182, -0.0151,  0.0229,  0.0010, -0.0212, -0.0001, -0.0007,  0.0124,\n",
      "        -0.0186,  0.0263, -0.0086, -0.0337, -0.0183,  0.0011, -0.0175,  0.0156,\n",
      "        -0.0102, -0.0238,  0.0167,  0.0101, -0.0117, -0.0021,  0.0380, -0.0059,\n",
      "         0.0235,  0.0087, -0.0185, -0.0183, -0.0114,  0.0431,  0.0012, -0.0382,\n",
      "        -0.0242,  0.0037,  0.0030,  0.0070,  0.0287,  0.0407, -0.0425,  0.0258,\n",
      "        -0.0279, -0.0229,  0.0102, -0.0434,  0.0351, -0.0176,  0.0132, -0.0202,\n",
      "         0.0276,  0.0215, -0.0307, -0.0293,  0.0028, -0.0366,  0.0446, -0.0052,\n",
      "        -0.0295, -0.0331,  0.0168, -0.0425, -0.0113, -0.0341,  0.0135,  0.0270,\n",
      "         0.0306, -0.0341, -0.0095,  0.0233,  0.0116,  0.0154, -0.0071, -0.0454,\n",
      "         0.0393,  0.0353, -0.0342, -0.0380, -0.0228,  0.0142, -0.0181,  0.0004,\n",
      "         0.0445,  0.0123, -0.0416,  0.0136,  0.0269, -0.0245, -0.0281, -0.0316,\n",
      "        -0.0084,  0.0339, -0.0019,  0.0289, -0.0159,  0.0151, -0.0173, -0.0130,\n",
      "        -0.0454, -0.0261,  0.0262,  0.0058, -0.0444,  0.0362, -0.0203, -0.0241,\n",
      "        -0.0380, -0.0388, -0.0218,  0.0042,  0.0089,  0.0094,  0.0317, -0.0409,\n",
      "         0.0156,  0.0451, -0.0436,  0.0218, -0.0132,  0.0055, -0.0014, -0.0348,\n",
      "         0.0226,  0.0123, -0.0189,  0.0177, -0.0125,  0.0361,  0.0166, -0.0338])), ('_sequence.6.weight', tensor([[ 0.0191,  0.0596, -0.0317,  ...,  0.0152,  0.0503,  0.0432],\n",
      "        [-0.0115,  0.0567, -0.0496,  ...,  0.0103, -0.0785, -0.0698],\n",
      "        [ 0.0747, -0.0787,  0.0093,  ..., -0.0134,  0.0280,  0.0150],\n",
      "        ...,\n",
      "        [-0.0599, -0.0022, -0.0443,  ...,  0.0544,  0.0427,  0.0397],\n",
      "        [-0.0787, -0.0544, -0.0297,  ..., -0.0168,  0.0570, -0.0680],\n",
      "        [ 0.0615,  0.0506,  0.0501,  ..., -0.0156, -0.0325,  0.0138]])), ('_sequence.6.bias', tensor([ 0.0370, -0.0396,  0.0478,  0.0709,  0.0151,  0.0024, -0.0130, -0.0700,\n",
      "         0.0054,  0.0727, -0.0627,  0.0221,  0.0502,  0.0481, -0.0419,  0.0578,\n",
      "        -0.0601, -0.0328, -0.0077, -0.0683,  0.0558,  0.0232, -0.0180, -0.0285,\n",
      "        -0.0312,  0.0175,  0.0458,  0.0046,  0.0375,  0.0270,  0.0459,  0.0702,\n",
      "        -0.0160, -0.0600, -0.0297, -0.0028, -0.0649, -0.0083,  0.0027,  0.0731])), ('_sequence.8.weight', tensor([[-0.0151,  0.0837,  0.1180, -0.0921, -0.1125, -0.1261, -0.0643,  0.1339,\n",
      "          0.0746,  0.1071, -0.0495,  0.0163, -0.0155,  0.1218, -0.1433, -0.0777,\n",
      "          0.0817, -0.1103, -0.1536,  0.1326, -0.1066,  0.0527, -0.1339,  0.1188,\n",
      "          0.1391,  0.1145,  0.0070, -0.1562,  0.0975,  0.0974, -0.1453,  0.0671,\n",
      "         -0.1516, -0.1452, -0.0201, -0.1313,  0.1321,  0.1443,  0.0851,  0.1544],\n",
      "        [-0.0676,  0.1008,  0.0792,  0.1411, -0.0050,  0.1395, -0.1511,  0.1489,\n",
      "          0.1189, -0.0602,  0.0587, -0.0783, -0.1460,  0.1569,  0.1163,  0.0021,\n",
      "          0.0478,  0.0068,  0.1041, -0.0986, -0.0456, -0.0638,  0.1406, -0.0336,\n",
      "         -0.0391,  0.1211, -0.0835, -0.1557, -0.1489, -0.0851, -0.1377, -0.0878,\n",
      "          0.0038,  0.1259, -0.0259, -0.0915, -0.1142,  0.1462,  0.0839, -0.0294],\n",
      "        [-0.0405,  0.0130, -0.0821, -0.0498,  0.0708, -0.1256,  0.0438,  0.0348,\n",
      "         -0.0343, -0.0747, -0.0521,  0.0823,  0.0879,  0.1336, -0.0527,  0.1418,\n",
      "          0.0362,  0.1521,  0.1538, -0.1574,  0.0181,  0.0740,  0.0637,  0.0731,\n",
      "          0.0765, -0.1476, -0.0284, -0.0858, -0.0136,  0.0680,  0.0930,  0.0555,\n",
      "         -0.1423, -0.1156,  0.1225,  0.0217, -0.0580, -0.0718,  0.0471, -0.0366]])), ('_sequence.8.bias', tensor([-0.0458,  0.0048, -0.0041]))])\n"
     ]
    }
   ],
   "source": [
    "print(model_MLPSEQ.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the parameters look similar but have different names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's make a dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced size: 100000\n"
     ]
    }
   ],
   "source": [
    "dset=WCH5Dataset(\"/scratch/fcormier/Public/NUPRISM.h5\",reduced_dataset_size=100000,val_split=0.1,test_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a dataloader and grab a first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "train_dldr=DataLoader(dset,\n",
    "                      batch_size=32,\n",
    "                      shuffle=False,\n",
    "                      sampler=SubsetRandomSampler(dset.train_indices))\n",
    "train_iter=iter(train_dldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch0=next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=batch0[0]\n",
    "labels=batch0[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the model output on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out=model_MLP(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 0, 2, 2, 0, 1, 2, 2, 0, 1, 0, 2, 1, 2, 1, 1, 0, 0, 1, 1, 2, 1, 0,\n",
      "        2, 1, 2, 2, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10.5870,  20.1918,  21.9317],\n",
      "        [-12.2087,   7.7353,  12.2677],\n",
      "        [-42.1053,  19.5736,  -2.9641],\n",
      "        [-16.5042,   0.4974,  -7.8518],\n",
      "        [ -1.9450,   5.5483,  -5.1083],\n",
      "        [ -2.5612,   7.1446,  25.9561],\n",
      "        [ -3.5834,  19.2639,  30.9361],\n",
      "        [-74.8336,  17.4859, -25.8750],\n",
      "        [-22.4322,  -7.6299, -13.1747],\n",
      "        [-25.6413,   8.8435,   6.8586],\n",
      "        [-20.1189,  -5.1615, -13.1563],\n",
      "        [-58.6281,  27.3112,  44.7698],\n",
      "        [  1.2991,  19.9859,  -8.3300],\n",
      "        [-18.5540, -25.4850,  30.3851],\n",
      "        [ -3.7894,   7.3229, -11.2501],\n",
      "        [ -0.2566,  18.5016,  37.4023],\n",
      "        [-36.6636,  11.8829, -31.3037],\n",
      "        [-22.7839,  44.0696,  62.1200],\n",
      "        [-60.5168, -30.5942,  17.6290],\n",
      "        [-16.8623,  17.9748,   7.2353],\n",
      "        [-31.2869, -13.7888, -13.3608],\n",
      "        [ -4.4971,   2.4517,  -2.9462],\n",
      "        [-31.2688,  -7.2606,  -0.9698],\n",
      "        [  2.6306, -14.6262,  50.3731],\n",
      "        [-15.1132,   8.6552,  -1.4031],\n",
      "        [-35.3494,   5.0967,   6.5175],\n",
      "        [ -5.3888,   3.2303,  -6.9277],\n",
      "        [-25.5713,  22.0410,  16.0843],\n",
      "        [-35.4237,  13.4083,  -6.9288],\n",
      "        [-25.0458,  28.9110,  -0.9095],\n",
      "        [-20.9629,   0.2873, -28.6097],\n",
      "        [-23.9366,  -5.3717, -14.7328]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have model's predictions and we above got 'true' labels from the dataset, so we can now compute the loss - CrossEntropyLoss is the apropropriate one to use here. We will use `CrossEntropyLoss` from `torch.nn` - btw it is also a `Module`. First create it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "loss_module=CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_tensor=loss_module(model_out,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.8749, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a 'forward pass'. We should now have a computational graph available - let's plot it for the kicks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can't get torchivz in compute canada\n",
    "#from torchviz import make_dot\n",
    "#make_dot(loss_tensor,params=dict(model_MLP.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we calculate the gradients - let's check what they are now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of a parameter: fc1.weight, gradient: None\n",
      "name of a parameter: fc1.bias, gradient: None\n",
      "name of a parameter: fc2.weight, gradient: None\n",
      "name of a parameter: fc2.bias, gradient: None\n",
      "name of a parameter: fc5.weight, gradient: None\n",
      "name of a parameter: fc5.bias, gradient: None\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_MLP.named_parameters():\n",
    "    print(\"name of a parameter: {}, gradient: {}\".\n",
    "          format(name, param.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No wonder - let's calculate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tensor.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of a parameter: fc1.weight, gradient: tensor([[ 0.0000e+00,  0.0000e+00,  1.3393e-04,  ...,  1.3313e+00,\n",
      "         -4.1290e-20,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  3.1685e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -2.9847e-08,  ...,  3.7166e-01,\n",
      "         -7.0487e-20,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00, -3.1952e-04,  ...,  4.8372e-02,\n",
      "          3.5567e-21,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  5.5135e-09,  ...,  1.0225e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  1.1017e-04,  ..., -8.3645e-01,\n",
      "         -1.2418e-19,  0.0000e+00]])\n",
      "name of a parameter: fc1.bias, gradient: tensor([-2.9871e-04,  1.1825e-02,  3.8144e-03,  3.2843e-03, -7.2096e-03,\n",
      "         4.8469e-03,  4.3653e-03, -6.0873e-04, -1.2929e-03, -1.0235e-02,\n",
      "        -6.2651e-03, -2.1556e-03, -3.5231e-03,  8.6886e-03,  2.4749e-03,\n",
      "        -3.2984e-03, -5.7716e-04,  5.5906e-03, -1.4169e-03,  1.0803e-03,\n",
      "         2.1713e-03, -1.1894e-02,  2.6767e-03, -1.6175e-03, -1.5509e-03,\n",
      "         2.0723e-02,  6.3405e-06,  1.8705e-03,  8.3413e-05,  6.5761e-05,\n",
      "         3.6270e-03,  1.1547e-03, -7.2673e-04, -5.3020e-03, -4.3190e-03,\n",
      "        -2.6062e-03,  1.2239e-02,  3.6135e-03, -1.5045e-03,  4.5116e-03,\n",
      "        -2.9638e-03, -7.0890e-03, -5.2598e-04,  1.3758e-03,  3.6076e-03,\n",
      "        -5.5377e-03,  5.1048e-04,  3.9422e-04, -3.3307e-04,  6.9321e-03,\n",
      "        -6.5974e-04,  5.3352e-03,  1.1323e-02,  3.9713e-03, -1.4003e-03,\n",
      "        -1.3993e-03,  3.4922e-03,  9.5254e-03,  1.8954e-03,  4.6008e-03,\n",
      "         7.9358e-04,  2.3940e-03,  4.8825e-03,  1.4220e-03,  9.9447e-04,\n",
      "        -1.7352e-03,  3.8574e-03,  2.0317e-03, -2.4816e-03,  5.0378e-03,\n",
      "         1.6394e-03,  6.6452e-03, -1.0325e-03, -1.0102e-02,  3.0786e-03,\n",
      "         1.5450e-03,  3.2657e-03,  6.4566e-04, -1.7566e-02,  3.8978e-03,\n",
      "        -1.0242e-04,  3.4860e-03, -8.9708e-03,  1.0463e-02,  2.4341e-03,\n",
      "         2.8799e-03,  8.9738e-04,  4.2855e-03,  6.8426e-04, -2.8340e-03,\n",
      "         1.2624e-03,  7.5183e-03, -6.1140e-04, -1.3557e-02, -7.6481e-03,\n",
      "        -2.9912e-03,  1.2021e-03,  2.9636e-03,  9.7681e-03, -9.1946e-04,\n",
      "        -3.9749e-03, -3.8968e-03, -6.5874e-03,  2.6448e-03,  2.3885e-03,\n",
      "        -2.0866e-03, -3.2632e-03,  5.9389e-03,  2.6607e-03,  9.7201e-04,\n",
      "        -2.3158e-03, -1.3616e-03,  3.8824e-03,  2.1056e-03,  1.4614e-02,\n",
      "         2.9592e-03, -4.9343e-04,  4.8888e-03,  3.1890e-03,  8.1032e-03,\n",
      "        -2.2807e-03,  2.7605e-03,  1.0097e-02, -4.0376e-03,  1.3842e-03,\n",
      "         2.9693e-03,  1.5740e-03, -1.3480e-02, -4.9821e-04,  6.6384e-03,\n",
      "        -4.4385e-03,  5.0023e-03, -9.1783e-04, -8.1457e-03, -1.8090e-02,\n",
      "        -5.6917e-03,  7.0803e-03,  1.7129e-03, -1.5637e-03,  2.4474e-03,\n",
      "        -1.8736e-03, -8.1113e-03,  5.8286e-03, -1.2979e-02,  1.3683e-03,\n",
      "         1.3629e-02,  3.2560e-03, -4.6056e-04, -8.5175e-03,  1.1288e-02,\n",
      "         1.1262e-03, -1.8928e-03, -2.9116e-04, -8.8325e-04,  5.4118e-03,\n",
      "         1.4580e-03,  7.7266e-03, -3.2274e-04,  1.1517e-02, -4.8472e-03])\n",
      "name of a parameter: fc2.weight, gradient: tensor([[-6.1258, -0.9327, -1.6596,  ..., -2.0815, -2.5602, -0.1204],\n",
      "        [ 1.8569, -0.0857, -0.9604,  ...,  0.5691,  2.8974, -0.7752],\n",
      "        [ 4.6306,  3.8404,  7.1316,  ...,  2.5787,  1.3129,  1.6088],\n",
      "        ...,\n",
      "        [12.1125,  3.9470,  6.4805,  ...,  5.1606,  3.6355,  0.7918],\n",
      "        [ 8.1414,  2.2957,  3.7145,  ...,  3.6706,  2.9374,  0.2986],\n",
      "        [13.6684,  1.4690,  3.4641,  ...,  7.5745,  9.6898, -1.7781]])\n",
      "name of a parameter: fc2.bias, gradient: tensor([-0.0253,  0.0151,  0.0544,  0.0123,  0.0000, -0.0036,  0.0159, -0.0104,\n",
      "         0.0508,  0.0047,  0.0533, -0.0163,  0.0398, -0.0604,  0.0057,  0.0425,\n",
      "        -0.0223, -0.0530,  0.0488, -0.0221, -0.0321, -0.0039,  0.0439,  0.0182,\n",
      "         0.0404, -0.0311, -0.0217,  0.0056, -0.0204, -0.0311,  0.0345,  0.0378,\n",
      "         0.0138, -0.0207,  0.0336, -0.0121,  0.0090,  0.0637,  0.0449,  0.0933])\n",
      "name of a parameter: fc5.weight, gradient: tensor([[-9.3239e+00, -3.9850e+01, -1.9983e+01, -8.3840e+00,  0.0000e+00,\n",
      "          1.3288e-03, -1.0408e+00, -7.4117e-01, -5.5156e+00,  1.8615e-17,\n",
      "         -3.7863e+01, -4.4152e+00, -2.0826e+01, -3.5499e+01, -1.4453e+00,\n",
      "         -2.7990e+00, -8.1727e+00, -3.3490e+01, -4.5939e+00, -1.5918e+00,\n",
      "         -9.9198e+00, -4.5005e+01, -2.7649e+01, -4.8737e+00, -1.5456e+01,\n",
      "         -1.3138e+01, -7.3786e+00, -9.0345e+00, -1.3713e+01, -3.9141e+01,\n",
      "         -1.4795e+00, -1.7905e+01, -9.3385e-01, -1.2072e+01, -1.5201e+01,\n",
      "         -1.9995e+01, -1.8707e+01, -1.8721e+01, -1.5605e+01, -1.5471e+01],\n",
      "        [ 9.3821e+00,  2.7134e+01, -2.5924e+00,  9.1273e+00,  0.0000e+00,\n",
      "          5.2097e+00,  3.1842e-02, -7.8781e-01,  5.4437e+00, -4.2159e-01,\n",
      "          2.6884e+01,  2.2325e+00,  1.2930e+01,  2.1164e+01,  3.0725e+00,\n",
      "          1.0921e+00,  4.8421e+00,  3.0007e+01,  1.8683e+00,  2.2991e+00,\n",
      "          4.0524e+00,  2.5920e+01,  1.6165e+01,  2.6220e+00,  1.2838e+01,\n",
      "          3.4118e+00, -3.3335e+00,  2.0712e+00,  8.4676e+00,  2.1068e+01,\n",
      "          4.1841e+00,  1.0395e+01,  5.4244e+00,  8.2885e+00,  4.4115e+00,\n",
      "          9.6348e+00,  1.3130e+01,  7.9615e+00,  5.2915e+00,  2.3018e+01],\n",
      "        [-5.8128e-02,  1.2716e+01,  2.2575e+01, -7.4325e-01,  0.0000e+00,\n",
      "         -5.2110e+00,  1.0090e+00,  1.5290e+00,  7.1854e-02,  4.2159e-01,\n",
      "          1.0979e+01,  2.1827e+00,  7.8953e+00,  1.4335e+01, -1.6272e+00,\n",
      "          1.7069e+00,  3.3306e+00,  3.4825e+00,  2.7256e+00, -7.0734e-01,\n",
      "          5.8674e+00,  1.9085e+01,  1.1484e+01,  2.2516e+00,  2.6177e+00,\n",
      "          9.7257e+00,  1.0712e+01,  6.9633e+00,  5.2458e+00,  1.8073e+01,\n",
      "         -2.7046e+00,  7.5101e+00, -4.4906e+00,  3.7839e+00,  1.0790e+01,\n",
      "          1.0361e+01,  5.5769e+00,  1.0759e+01,  1.0314e+01, -7.5464e+00]])\n",
      "name of a parameter: fc5.bias, gradient: tensor([-0.3437,  0.3005,  0.0432])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_MLP.named_parameters():\n",
    "    print(\"name of a parameter: {}, gradient: {}\".\n",
    "          format(name, param.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we have to do now is subtract the gradient of a given parameter from the parameter tensor itself and do it for all parameters of the model - that should decrease the loss. Normally the gradient is multiplied by a learning rate parameter $\\lambda$ so we don't go too far in the loss landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "for param in model_MLP.parameters():\n",
    "    param.data.add_(-lr*param.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "call to backward **accumulates** gradients - so we also need to zero the gradient tensors if we want to keep going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_MLP.parameters():\n",
    "    param.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a much simpler way of doing this - we can use the pytorch [optim](https://pytorch.org/docs/stable/optim.html) classes. This allows us to easily use more advanced optimization options (like momentum or adaptive optimizers like [Adam](https://arxiv.org/abs/1412.6980)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD(model_MLP.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get a new batch of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1=next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=batch1[0]\n",
    "labels=batch1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 2, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 2, 2, 0, 2,\n",
      "        0, 0, 2, 1, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out=model_MLP(data)\n",
    "loss_tensor=loss_module(model_out,labels)\n",
    "loss_tensor.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could just put the code above in a loop and be done with it, but the usual practice would be to wrap this functionality in a training object. Here we'll use the [engine](/edit/utils/engine.py) class. Let's examine it. We'll talk about:\n",
    "  1. Implementation of the training loop\n",
    "  2. Evaluation on validation set and training and test modes.\n",
    "  3. Turning evaluation of gradients on and off.\n",
    "  4. Saving and retrieving the model and optimizer state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.engine import Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a configuration object -we'll use this to set up our training engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    pass\n",
    "config=CONFIG()\n",
    "config.batch_size_test =512\n",
    "config.batch_size_train = 32\n",
    "config.batch_size_val = 512\n",
    "config.lr=0.0001\n",
    "config.device = 'cpu'\n",
    "config.num_workers_train=2\n",
    "config.num_workers_val=1\n",
    "config.num_workers_test=1\n",
    "config.dump_path = '../model_state_dumps'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sticking to CPU\n",
      "Creating a directory for run dump: ../model_state_dumps/20240426_100901/\n"
     ]
    }
   ],
   "source": [
    "engine=Engine(model_MLP,dset,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size_test': 512, 'batch_size_train': 32, 'batch_size_val': 512, 'lr': 0.0001, 'device': 'cpu', 'num_workers_train': 2, 'num_workers_val': 1, 'num_workers_test': 1, 'dump_path': '../model_state_dumps'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Starting @ 2024-04-26 10:09:03\n",
      "... Iteration 0 ... Epoch 0.00 ... Validation Loss 32.713 ... Validation Accuracy 0.354\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "best validation loss so far!: 32.712890625\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLPBEST.pth\n",
      "... Iteration 1 ... Epoch 0.00 ... Loss 27.706 ... Accuracy 0.375\n",
      "... Iteration 51 ... Epoch 0.02 ... Loss 1.302 ... Accuracy 0.594\n",
      "... Iteration 100 ... Epoch 0.04 ... Validation Loss 1.131 ... Validation Accuracy 0.484\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "best validation loss so far!: 1.1314603090286255\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLPBEST.pth\n",
      "... Iteration 101 ... Epoch 0.04 ... Loss 0.798 ... Accuracy 0.531\n",
      "... Iteration 151 ... Epoch 0.06 ... Loss 1.105 ... Accuracy 0.312\n",
      "... Iteration 200 ... Epoch 0.08 ... Validation Loss 0.950 ... Validation Accuracy 0.512\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "best validation loss so far!: 0.9504539370536804\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLPBEST.pth\n",
      "... Iteration 201 ... Epoch 0.08 ... Loss 0.810 ... Accuracy 0.531\n",
      "... Iteration 251 ... Epoch 0.10 ... Loss 0.729 ... Accuracy 0.656\n",
      "... Iteration 300 ... Epoch 0.12 ... Validation Loss 0.685 ... Validation Accuracy 0.605\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "best validation loss so far!: 0.6852691769599915\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLPBEST.pth\n",
      "... Iteration 301 ... Epoch 0.12 ... Loss 0.762 ... Accuracy 0.594\n",
      "... Iteration 351 ... Epoch 0.14 ... Loss 0.773 ... Accuracy 0.469\n",
      "... Iteration 400 ... Epoch 0.16 ... Validation Loss 0.649 ... Validation Accuracy 0.641\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "best validation loss so far!: 0.6493943929672241\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLPBEST.pth\n",
      "... Iteration 401 ... Epoch 0.16 ... Loss 0.637 ... Accuracy 0.719\n",
      "... Iteration 451 ... Epoch 0.18 ... Loss 0.890 ... Accuracy 0.656\n",
      "... Iteration 500 ... Epoch 0.20 ... Validation Loss 0.684 ... Validation Accuracy 0.643\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 501 ... Epoch 0.20 ... Loss 0.693 ... Accuracy 0.531\n",
      "... Iteration 551 ... Epoch 0.22 ... Loss 0.524 ... Accuracy 0.750\n",
      "... Iteration 600 ... Epoch 0.24 ... Validation Loss 0.626 ... Validation Accuracy 0.648\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "best validation loss so far!: 0.626433253288269\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLPBEST.pth\n",
      "... Iteration 601 ... Epoch 0.24 ... Loss 0.623 ... Accuracy 0.656\n",
      "... Iteration 651 ... Epoch 0.26 ... Loss 0.628 ... Accuracy 0.594\n",
      "... Iteration 700 ... Epoch 0.28 ... Validation Loss 0.640 ... Validation Accuracy 0.646\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 701 ... Epoch 0.28 ... Loss 0.773 ... Accuracy 0.594\n",
      "... Iteration 751 ... Epoch 0.30 ... Loss 0.764 ... Accuracy 0.625\n",
      "... Iteration 800 ... Epoch 0.32 ... Validation Loss 0.655 ... Validation Accuracy 0.643\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 801 ... Epoch 0.32 ... Loss 0.553 ... Accuracy 0.688\n",
      "... Iteration 851 ... Epoch 0.34 ... Loss 0.736 ... Accuracy 0.562\n",
      "... Iteration 900 ... Epoch 0.36 ... Validation Loss 0.600 ... Validation Accuracy 0.652\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "best validation loss so far!: 0.5996984839439392\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLPBEST.pth\n",
      "... Iteration 901 ... Epoch 0.36 ... Loss 0.457 ... Accuracy 0.812\n",
      "... Iteration 951 ... Epoch 0.38 ... Loss 0.628 ... Accuracy 0.688\n",
      "... Iteration 1000 ... Epoch 0.40 ... Validation Loss 0.570 ... Validation Accuracy 0.662\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "best validation loss so far!: 0.5697634220123291\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLPBEST.pth\n",
      "... Iteration 1001 ... Epoch 0.40 ... Loss 0.585 ... Accuracy 0.750\n",
      "... Iteration 1051 ... Epoch 0.42 ... Loss 0.582 ... Accuracy 0.656\n",
      "... Iteration 1100 ... Epoch 0.44 ... Validation Loss 0.573 ... Validation Accuracy 0.707\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 1101 ... Epoch 0.44 ... Loss 0.556 ... Accuracy 0.625\n",
      "... Iteration 1151 ... Epoch 0.46 ... Loss 0.764 ... Accuracy 0.594\n",
      "... Iteration 1200 ... Epoch 0.48 ... Validation Loss 0.503 ... Validation Accuracy 0.729\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "best validation loss so far!: 0.5031485557556152\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLPBEST.pth\n",
      "... Iteration 1201 ... Epoch 0.48 ... Loss 0.519 ... Accuracy 0.688\n",
      "... Iteration 1251 ... Epoch 0.50 ... Loss 0.584 ... Accuracy 0.688\n",
      "... Iteration 1300 ... Epoch 0.52 ... Validation Loss 0.576 ... Validation Accuracy 0.705\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 1301 ... Epoch 0.52 ... Loss 0.557 ... Accuracy 0.625\n",
      "... Iteration 1351 ... Epoch 0.54 ... Loss 0.506 ... Accuracy 0.750\n",
      "... Iteration 1400 ... Epoch 0.56 ... Validation Loss 0.569 ... Validation Accuracy 0.660\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 1401 ... Epoch 0.56 ... Loss 0.431 ... Accuracy 0.781\n",
      "... Iteration 1451 ... Epoch 0.58 ... Loss 0.618 ... Accuracy 0.469\n",
      "... Iteration 1500 ... Epoch 0.60 ... Validation Loss 0.546 ... Validation Accuracy 0.684\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 1501 ... Epoch 0.60 ... Loss 0.671 ... Accuracy 0.688\n",
      "... Iteration 1551 ... Epoch 0.62 ... Loss 0.555 ... Accuracy 0.719\n",
      "... Iteration 1600 ... Epoch 0.64 ... Validation Loss 0.521 ... Validation Accuracy 0.719\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 1601 ... Epoch 0.64 ... Loss 0.575 ... Accuracy 0.688\n",
      "... Iteration 1651 ... Epoch 0.66 ... Loss 0.529 ... Accuracy 0.750\n",
      "... Iteration 1700 ... Epoch 0.68 ... Validation Loss 0.548 ... Validation Accuracy 0.695\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 1701 ... Epoch 0.68 ... Loss 0.590 ... Accuracy 0.781\n",
      "... Iteration 1751 ... Epoch 0.70 ... Loss 0.578 ... Accuracy 0.750\n",
      "... Iteration 1800 ... Epoch 0.72 ... Validation Loss 0.604 ... Validation Accuracy 0.656\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 1801 ... Epoch 0.72 ... Loss 0.800 ... Accuracy 0.625\n",
      "... Iteration 1851 ... Epoch 0.74 ... Loss 0.611 ... Accuracy 0.594\n",
      "... Iteration 1900 ... Epoch 0.76 ... Validation Loss 0.504 ... Validation Accuracy 0.673\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 1901 ... Epoch 0.76 ... Loss 0.500 ... Accuracy 0.781\n",
      "... Iteration 1951 ... Epoch 0.78 ... Loss 0.507 ... Accuracy 0.781\n",
      "starting over on the validation set\n",
      "... Iteration 2000 ... Epoch 0.80 ... Validation Loss 0.507 ... Validation Accuracy 0.705\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 2001 ... Epoch 0.80 ... Loss 0.617 ... Accuracy 0.594\n",
      "... Iteration 2051 ... Epoch 0.82 ... Loss 0.513 ... Accuracy 0.750\n",
      "... Iteration 2100 ... Epoch 0.84 ... Validation Loss 0.495 ... Validation Accuracy 0.697\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "best validation loss so far!: 0.49467870593070984\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLPBEST.pth\n",
      "... Iteration 2101 ... Epoch 0.84 ... Loss 0.430 ... Accuracy 0.656\n",
      "... Iteration 2151 ... Epoch 0.86 ... Loss 0.565 ... Accuracy 0.719\n",
      "... Iteration 2200 ... Epoch 0.88 ... Validation Loss 0.522 ... Validation Accuracy 0.709\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 2201 ... Epoch 0.88 ... Loss 0.438 ... Accuracy 0.719\n",
      "... Iteration 2251 ... Epoch 0.90 ... Loss 0.638 ... Accuracy 0.750\n",
      "... Iteration 2300 ... Epoch 0.92 ... Validation Loss 0.525 ... Validation Accuracy 0.668\n",
      "Saved checkpoint as: ../model_state_dumps/20240426_100901/SimpleMLP.pth\n",
      "... Iteration 2301 ... Epoch 0.92 ... Loss 0.420 ... Accuracy 0.781\n",
      "... Iteration 2351 ... Epoch 0.94 ... Loss 0.616 ... Accuracy 0.594\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "engine.train(epochs=1,report_interval=50,valid_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a simple Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open [simpleCNN](http://localhost:8888/edit/models/simpleCNN.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simpleCNN import SimpleCNN\n",
    "model_CNN=SimpleCNN(num_input_channels=38,num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def rotate_chan(x):\n",
    "    return np.transpose(x,(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset=WCH5Dataset(\"/scratch/fcormier/Public/NUPRISM.h5\",val_split=0.1,test_split=0.1,transform=rotate_chan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sticking to CPU\n",
      "Creating a directory for run dump: ../model_state_dumps/20240424_223542/\n"
     ]
    }
   ],
   "source": [
    "engine=Engine(model_CNN,dset,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of a parameter: f_embed.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_embed.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv1.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv1.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv2a.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv2a.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv2b.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv2b.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv3a.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv3a.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv3b.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv3b.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv4.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv4.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc1.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc1.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc2.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc2.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc3.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc3.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_CNN.named_parameters():\n",
    "    print(\"name of a parameter: {}, type: {}, parameter requires a gradient?: {}\".\n",
    "          format(name, type(param),param.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Starting @ 2024-04-24 22:35:47\n",
      "tensor([[-0.2139,  0.2188,  0.1590],\n",
      "        [-0.2356,  0.1918,  0.1596]])\n",
      "... Iteration 0 ... Epoch 0.00 ... Validation Loss 1.004 ... Validation Accuracy 0.000\n",
      "Saved checkpoint as: ../model_state_dumps/20240424_223542/SimpleCNN.pth\n",
      "best validation loss so far!: 1.0036206245422363\n",
      "Saved checkpoint as: ../model_state_dumps/20240424_223542/SimpleCNNBEST.pth\n",
      "tensor([[-0.2629,  0.2181,  0.2056]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Unfortunately this seems to hang and not train\n",
    "%%time\n",
    "engine.train(epochs=5,report_interval=1,valid_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HKCA2 Python 3.x Kernel",
   "language": "python",
   "name": "hk_ca_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
