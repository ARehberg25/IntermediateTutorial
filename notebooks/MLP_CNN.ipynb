{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your first fully connected network and a CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple fully connected network (a Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the paths and make a dataset again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "currentdir = os.getcwd()\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.data_handling import WCH5Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's make our model. We'll talk about \n",
    "  - model parameters\n",
    "  - inputs and the forward method\n",
    "  - Modules containing modules\n",
    "  - Sequential Module  \n",
    "  Lets open [simpleMLP](/edit/models/simpleMLP.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simpleMLP import SimpleMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP=SimpleMLP(num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's look at the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of a parameter: fc1.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc1.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc2.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc2.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc3.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc3.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc4.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc4.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc5.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc5.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_MLP.named_parameters():\n",
    "    print(\"name of a parameter: {}, type: {}, parameter requires a gradient?: {}\".\n",
    "          format(name, type(param),param.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see by default the parameters have `requires_grad` set - i.e. we will be able to obtain gradient of the loss function with respect to these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly look at the [source](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear) for the linear module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters descend from the `Tensor` class. When `Parameter` object is instantiated as a member of a `Module` object class the parameter is added to `Module`s list of parameters automatically. This list and values are captured in the 'state dictionary' of a module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[ 0.0057,  0.0003,  0.0003,  ...,  0.0026, -0.0030,  0.0006],\n",
       "                      [-0.0004,  0.0002, -0.0049,  ..., -0.0060,  0.0018,  0.0029],\n",
       "                      [-0.0051,  0.0022, -0.0015,  ..., -0.0038, -0.0008,  0.0051],\n",
       "                      ...,\n",
       "                      [ 0.0003, -0.0045,  0.0050,  ..., -0.0064,  0.0036,  0.0058],\n",
       "                      [ 0.0050,  0.0001, -0.0041,  ...,  0.0007,  0.0044,  0.0041],\n",
       "                      [ 0.0055, -0.0026, -0.0062,  ..., -0.0029,  0.0047,  0.0021]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 6.1454e-03, -3.8093e-03,  3.7682e-03,  1.2780e-03, -4.2391e-03,\n",
       "                      -1.6603e-03,  4.3777e-03,  3.7202e-03, -1.6040e-03,  2.9438e-03,\n",
       "                      -2.7895e-04, -4.2055e-03, -1.3591e-03, -3.8535e-04,  1.7130e-03,\n",
       "                      -2.5235e-03,  3.5872e-03, -5.5334e-03, -4.4186e-03, -6.3407e-03,\n",
       "                       2.4332e-03, -3.0362e-04,  2.9208e-03,  5.0250e-03,  3.2255e-03,\n",
       "                       2.6493e-03,  8.7132e-04, -2.1264e-03,  5.6918e-03,  5.6451e-03,\n",
       "                      -4.4237e-03,  8.7693e-05,  3.4299e-03, -2.7917e-03, -5.5828e-03,\n",
       "                       5.4341e-03, -2.6220e-03,  1.2740e-03,  2.6054e-03, -2.1733e-03,\n",
       "                       4.8668e-03,  2.5534e-04, -3.8557e-03,  5.5611e-03, -3.0644e-03,\n",
       "                       2.7949e-03, -4.5890e-03,  6.1951e-03, -5.4786e-03,  2.1270e-03,\n",
       "                      -5.1161e-03,  3.0485e-03, -1.2644e-03,  2.3816e-03, -2.1382e-03,\n",
       "                      -3.8961e-04, -1.4638e-03, -1.3953e-03, -2.9426e-03, -4.6418e-03,\n",
       "                       2.9138e-03, -2.2954e-03,  3.7230e-03,  4.6854e-03, -2.9201e-03,\n",
       "                       4.1045e-03, -2.2486e-03, -4.7118e-03, -1.8692e-03,  2.9453e-04,\n",
       "                       1.8643e-03,  3.1173e-03,  2.8603e-03,  1.1433e-03,  5.6614e-03,\n",
       "                      -6.2929e-03,  4.7057e-03,  5.0705e-03,  5.6751e-03,  2.4543e-03,\n",
       "                       9.4212e-04,  4.4784e-03, -2.1655e-03, -4.8696e-05, -5.5854e-03,\n",
       "                       5.7445e-03, -2.0388e-03,  4.3322e-03, -4.0855e-03, -5.7663e-03,\n",
       "                      -2.4749e-03, -2.1319e-03,  3.5760e-04, -1.5441e-03, -1.7257e-03,\n",
       "                       7.1801e-04,  3.5564e-03,  5.5307e-03, -2.4674e-03,  4.1110e-03,\n",
       "                      -5.5016e-04,  1.0444e-03, -2.8801e-03,  3.2368e-03, -5.6870e-04,\n",
       "                       3.8640e-03, -5.7529e-03, -4.2040e-03,  5.1341e-03, -5.5015e-03,\n",
       "                      -5.4719e-03, -2.7585e-03, -9.7633e-04,  5.7349e-03,  2.3384e-03,\n",
       "                       1.9672e-03,  2.8215e-03,  2.6720e-03, -2.8556e-03,  8.4391e-04,\n",
       "                      -5.0907e-03, -2.7756e-03,  2.6969e-03, -6.0533e-03,  3.5007e-03,\n",
       "                       6.0959e-03, -2.5990e-04, -4.9264e-04,  9.8948e-04, -3.3325e-03,\n",
       "                       1.1422e-03,  2.5613e-03,  4.1360e-03, -5.8419e-03,  1.5160e-03,\n",
       "                      -6.0805e-03,  1.9480e-04,  2.3885e-03, -2.5530e-03, -6.1497e-03,\n",
       "                      -1.8484e-03,  1.5984e-03,  1.7650e-03, -3.6872e-04,  4.6806e-03,\n",
       "                      -3.4630e-03,  4.3967e-03, -9.3068e-04, -6.1536e-03,  5.0623e-03,\n",
       "                      -4.5584e-05,  4.0209e-03,  6.2540e-04,  6.1663e-03, -2.2754e-03,\n",
       "                       5.4638e-03,  6.2805e-03,  2.4769e-03,  5.1815e-03,  1.8212e-03,\n",
       "                       6.3710e-03, -5.6443e-03, -9.2806e-04, -5.5731e-03, -1.7964e-03,\n",
       "                       2.8385e-03, -2.4162e-03,  3.8717e-03,  2.9346e-03, -3.7646e-03,\n",
       "                      -2.6207e-03, -3.0657e-03, -4.0873e-03,  6.1259e-03,  3.0686e-03,\n",
       "                       4.1130e-03, -3.6160e-03,  5.3989e-03,  4.2715e-04,  2.0123e-03,\n",
       "                       8.8960e-04,  6.3299e-03,  1.0181e-03,  5.0473e-03, -4.8628e-03,\n",
       "                      -1.1962e-03,  4.3875e-03,  1.8881e-03, -4.3563e-03, -5.7304e-03,\n",
       "                      -1.6225e-03, -5.5492e-03, -3.9907e-03,  4.5660e-03, -1.5793e-03,\n",
       "                      -5.3954e-03,  5.1397e-03, -3.5985e-03, -5.7229e-03,  1.7004e-03,\n",
       "                       3.7993e-03,  3.3118e-03,  1.3093e-03,  2.5741e-04,  2.1618e-03,\n",
       "                       2.5331e-03, -3.9996e-03, -3.4629e-03,  4.3962e-03, -3.9871e-03,\n",
       "                       3.1562e-03, -5.7638e-03, -1.8843e-03,  1.5443e-03,  5.3405e-03,\n",
       "                       3.1792e-03,  1.5257e-03,  5.3147e-03,  4.0781e-03, -1.6835e-03,\n",
       "                      -5.0299e-03, -3.1690e-03,  3.4603e-03,  3.8110e-03, -2.5571e-03,\n",
       "                      -5.8376e-04, -2.1105e-03,  5.5435e-04,  4.2307e-03,  5.4122e-03,\n",
       "                      -3.4144e-03, -1.2656e-05,  5.1445e-03, -1.5133e-03,  2.8871e-03,\n",
       "                       3.1365e-03,  1.3330e-03,  5.5454e-03, -2.6605e-03,  5.1927e-03,\n",
       "                      -1.5331e-03, -2.9292e-03, -1.5965e-03, -3.0514e-03,  3.8393e-03,\n",
       "                       5.8047e-03,  4.3629e-03, -4.4961e-03, -2.5303e-03, -2.1052e-03,\n",
       "                      -5.0450e-03, -2.5981e-03,  1.1368e-03,  1.6702e-03, -4.6050e-03,\n",
       "                      -5.9821e-03,  4.9666e-03, -6.3451e-03,  1.5457e-03,  5.6440e-03,\n",
       "                       7.9913e-04,  4.3133e-03, -5.6823e-03,  4.8349e-03, -5.6339e-03,\n",
       "                       3.4963e-03,  2.0775e-03, -3.9100e-03, -4.2734e-03, -3.8260e-03,\n",
       "                      -7.3439e-05, -3.2404e-03, -2.2980e-03, -4.4781e-03, -6.0858e-03,\n",
       "                      -5.7345e-03,  1.6108e-04, -6.2314e-03,  1.4766e-04, -4.4674e-03,\n",
       "                       2.1682e-03,  3.3430e-03,  2.9350e-03, -4.0799e-03,  6.2393e-03,\n",
       "                      -5.3350e-03, -3.7046e-03,  3.8015e-03, -1.8900e-03,  6.1008e-03,\n",
       "                       5.7240e-03, -3.2269e-03, -4.3172e-03, -4.4202e-04,  6.2764e-03,\n",
       "                      -4.3361e-04, -4.3092e-03, -1.6069e-03,  5.5099e-03, -1.6429e-03,\n",
       "                      -6.3907e-04, -1.0394e-03,  1.7433e-03,  9.5280e-04,  5.2553e-03,\n",
       "                      -3.1523e-03, -1.0021e-04,  3.4002e-03, -9.9779e-04, -1.8934e-03,\n",
       "                       1.1006e-03, -5.7276e-04,  3.5040e-03, -5.5202e-03,  4.4838e-03,\n",
       "                      -8.8799e-04,  8.4773e-04, -3.0354e-03, -5.4730e-03,  5.1786e-03,\n",
       "                       1.1787e-03,  2.7183e-03, -5.9633e-03,  3.4060e-03, -1.2442e-03,\n",
       "                      -1.2621e-03, -1.1507e-03, -4.9354e-03, -5.9772e-03, -2.1481e-04,\n",
       "                      -3.7594e-06, -5.4039e-03,  4.4731e-03, -8.6589e-04, -4.3361e-03,\n",
       "                      -1.4834e-04,  5.3815e-03, -5.8169e-03, -3.3943e-03, -6.2345e-03,\n",
       "                      -9.9007e-04,  4.5952e-03,  4.2226e-03, -2.8548e-03,  3.9355e-03,\n",
       "                       9.8367e-04,  5.7181e-03, -3.3831e-03, -1.3910e-03,  1.8154e-03,\n",
       "                      -1.3312e-03, -4.8401e-03, -4.4402e-04, -4.9751e-03,  1.6117e-03,\n",
       "                      -4.6594e-04, -3.3692e-03,  8.7063e-04,  2.6472e-04, -2.9634e-03,\n",
       "                       2.4130e-03, -3.3736e-03, -1.4237e-04, -6.4103e-03,  2.0623e-03,\n",
       "                      -3.9947e-03, -1.0626e-03,  3.9631e-03,  1.1124e-04,  4.7380e-03,\n",
       "                      -4.6870e-03,  1.1256e-03, -3.2221e-04,  2.1961e-03,  4.9318e-03,\n",
       "                      -3.6351e-03,  5.1611e-03,  4.5612e-03,  2.6888e-03, -3.7900e-03,\n",
       "                      -4.1157e-03,  8.4853e-04, -3.0033e-03, -9.5071e-04,  3.0496e-03,\n",
       "                      -3.5070e-03,  5.0237e-03,  4.0340e-03,  4.7060e-03,  1.5752e-03,\n",
       "                       5.1271e-03,  4.4100e-03,  3.7894e-03,  6.1569e-03, -2.6077e-03,\n",
       "                       1.1697e-03,  8.1895e-05,  1.8922e-03,  2.7313e-03,  4.8905e-03,\n",
       "                       1.9861e-03,  6.9004e-04,  6.3242e-03, -6.1979e-03,  1.2038e-03,\n",
       "                      -4.4462e-03, -6.2453e-04, -2.7502e-03,  3.5060e-03,  2.3568e-03,\n",
       "                       5.6209e-03, -9.2422e-04,  6.2887e-03, -6.2107e-03, -2.4511e-03,\n",
       "                       5.9436e-03,  4.8124e-03, -5.1858e-03, -1.2088e-03,  6.2735e-03,\n",
       "                       1.7230e-03,  5.2601e-03,  2.9756e-03,  3.2249e-03, -6.0731e-03,\n",
       "                      -3.4766e-03,  3.7303e-03, -1.9278e-03, -5.1632e-03, -1.1596e-03,\n",
       "                       3.5306e-03, -4.1801e-03,  6.2532e-03, -7.2910e-04,  5.9609e-03,\n",
       "                       5.3026e-03,  3.5057e-03,  5.4562e-03, -2.5636e-03,  5.4447e-03,\n",
       "                      -1.8270e-03, -2.5184e-03,  6.1564e-03,  2.3471e-03, -3.0471e-03,\n",
       "                      -4.3922e-03, -6.0237e-03, -5.4746e-03,  1.7916e-03, -5.4176e-03,\n",
       "                       2.4669e-03,  3.7791e-03,  1.7010e-04,  3.0128e-03,  3.1013e-03,\n",
       "                       4.9966e-03, -1.0798e-03,  4.9634e-03, -5.6725e-03,  5.0757e-03,\n",
       "                       9.2516e-04, -3.6780e-03,  1.7813e-03, -5.9741e-03,  5.8539e-03,\n",
       "                       5.8916e-03, -1.3083e-03,  3.4521e-03, -4.0059e-03, -5.9642e-03,\n",
       "                       5.6614e-03, -4.4859e-03,  6.9426e-05,  4.1088e-03, -4.9765e-03,\n",
       "                      -1.4628e-03,  1.5033e-03, -1.1039e-03, -5.0520e-03,  2.5179e-03,\n",
       "                      -2.9769e-03, -4.3676e-03, -3.3405e-03,  1.1651e-03,  4.4217e-03,\n",
       "                       1.9742e-03, -8.9022e-04, -3.5406e-03,  6.1459e-03, -7.4282e-04,\n",
       "                      -4.8062e-03,  1.3151e-03, -4.3794e-03,  3.1786e-04,  2.8325e-03,\n",
       "                       3.9628e-04,  5.6613e-03, -2.1690e-03, -4.6430e-03, -4.2714e-03,\n",
       "                      -4.9589e-03, -2.7355e-03,  4.4872e-03, -2.0193e-03,  4.3841e-04,\n",
       "                      -5.3441e-03,  3.0592e-03,  4.9055e-03,  2.9946e-03, -4.4244e-03,\n",
       "                      -4.6815e-03, -3.1834e-03,  1.2430e-03,  1.9107e-03,  3.9865e-03,\n",
       "                       1.3432e-03, -4.2372e-03, -4.1760e-03, -3.4453e-03, -3.1436e-03,\n",
       "                       6.0343e-03,  3.3947e-03, -5.1934e-04, -1.6405e-04,  6.2262e-03,\n",
       "                      -2.0994e-03,  6.1343e-03,  3.4235e-05, -3.9500e-03,  2.0685e-06,\n",
       "                       1.2911e-03,  6.3929e-03, -1.8438e-03,  2.7021e-03, -5.6030e-03,\n",
       "                      -8.5823e-04,  4.5517e-03,  1.7239e-04, -5.9231e-03,  4.6174e-03,\n",
       "                      -4.5607e-03,  3.6130e-03, -5.0739e-03,  7.9334e-04,  5.1810e-03,\n",
       "                       3.0931e-03, -3.9161e-03,  5.9636e-03, -5.2678e-03,  1.5440e-03,\n",
       "                       9.9454e-04,  1.1378e-03,  5.0485e-04,  1.5238e-03, -4.3353e-03,\n",
       "                       5.0705e-03,  1.5535e-03,  4.4587e-03, -6.1613e-03,  5.1707e-03,\n",
       "                      -3.2658e-03, -2.6627e-03, -4.4176e-03, -2.0251e-03,  3.6215e-03,\n",
       "                       5.2280e-03, -3.8444e-03, -6.3790e-03,  4.3466e-03,  5.0527e-03,\n",
       "                       3.5781e-03,  2.3299e-03,  2.9643e-03,  5.3941e-03, -2.4665e-03,\n",
       "                       5.4247e-03,  5.8770e-03,  2.3191e-03, -3.1211e-04,  5.5639e-03,\n",
       "                      -2.7941e-03,  3.4623e-04,  8.9550e-04, -1.7509e-03,  3.7700e-03,\n",
       "                      -1.2702e-03, -3.7194e-03,  4.3221e-03, -4.6602e-03, -1.2770e-03,\n",
       "                      -2.1147e-03,  9.0373e-04,  4.7289e-03,  5.2100e-03,  1.1292e-03,\n",
       "                      -2.8932e-03,  5.6598e-03,  1.0936e-03,  1.7417e-03, -4.2101e-03,\n",
       "                       3.0590e-03, -2.2886e-03,  4.8945e-03, -2.5441e-03,  3.3979e-03,\n",
       "                      -3.2048e-03,  2.6150e-03,  5.2549e-03, -1.9413e-03, -4.9969e-03,\n",
       "                      -6.1759e-03,  4.2126e-03,  4.7580e-03,  3.0165e-03,  5.6917e-03,\n",
       "                       5.9387e-03,  5.9815e-03, -3.6133e-03,  9.2431e-04,  2.2886e-03,\n",
       "                      -5.0256e-03, -1.5386e-04, -3.6836e-03,  5.9440e-03,  3.9945e-03,\n",
       "                       1.0770e-03,  3.6182e-03,  6.0071e-03, -5.1300e-03,  6.0892e-05,\n",
       "                      -6.2676e-03,  4.7345e-03, -6.1147e-03,  2.4387e-03,  5.9435e-04,\n",
       "                       5.2442e-04, -5.1721e-03, -1.2752e-03, -1.8481e-03,  2.8739e-03,\n",
       "                      -5.2060e-03, -1.9402e-03, -5.0826e-03,  3.5286e-03,  1.2745e-03,\n",
       "                      -4.4512e-04,  6.0271e-03,  2.7853e-03, -6.0796e-03, -2.5337e-03,\n",
       "                      -1.1530e-03,  3.4869e-03, -3.9845e-03,  2.6504e-03,  3.4072e-03,\n",
       "                      -4.1281e-03,  2.4702e-03, -4.5192e-04,  3.7239e-03,  4.3823e-03,\n",
       "                       6.3580e-03,  5.5850e-03,  2.8871e-04, -4.0683e-03,  5.9867e-03,\n",
       "                       4.4268e-04,  5.3787e-03,  1.8953e-03,  3.1888e-03,  3.0899e-03,\n",
       "                      -1.9354e-03,  3.6485e-03,  1.3082e-03, -6.3528e-03,  5.0200e-03,\n",
       "                       5.5052e-04, -2.6206e-03,  6.2237e-03,  1.9237e-03, -6.2097e-03,\n",
       "                       3.4924e-04,  4.1641e-03,  2.3712e-03,  3.2840e-04, -3.3129e-03,\n",
       "                       3.1150e-03,  1.2678e-03,  4.7478e-03, -5.7601e-03,  5.1365e-03,\n",
       "                       3.1287e-03,  5.8028e-03, -5.2772e-03,  5.4300e-03, -1.0966e-03,\n",
       "                      -3.4783e-03,  2.5546e-03,  6.3349e-03, -6.2029e-03, -5.3842e-03,\n",
       "                      -1.7120e-03, -6.1565e-03, -2.3199e-04, -4.2183e-04,  1.7508e-03,\n",
       "                      -3.4211e-03, -2.3617e-03,  2.3276e-03, -5.1474e-03,  5.7607e-03,\n",
       "                       3.6918e-03,  1.5571e-03,  5.6692e-03, -2.8940e-03,  3.0120e-03,\n",
       "                       5.1082e-03, -1.7746e-03,  1.0012e-04, -3.4535e-03,  1.1099e-03,\n",
       "                       6.2039e-03,  3.4537e-04, -2.2788e-03, -2.4445e-03,  6.1253e-03,\n",
       "                       3.5076e-03,  4.8023e-03, -6.1480e-03,  1.6970e-03,  3.7393e-03,\n",
       "                      -5.4631e-03, -3.5485e-03, -3.1753e-03,  2.1860e-03,  6.2298e-03,\n",
       "                       9.1977e-04, -2.4313e-03,  6.1712e-03, -5.2564e-03,  9.1326e-04,\n",
       "                       5.9574e-03, -5.9029e-04,  2.5148e-03, -2.7464e-03, -5.7511e-03,\n",
       "                      -4.4237e-04, -6.2823e-03,  1.9253e-03, -1.7355e-03, -6.5011e-05,\n",
       "                      -4.1805e-03,  4.6262e-03,  7.4792e-04,  6.2931e-03,  4.6138e-03,\n",
       "                       1.9452e-03, -3.6827e-03,  2.4914e-03, -1.0593e-03,  2.5776e-03,\n",
       "                      -5.4776e-03, -3.5702e-03, -4.0205e-03, -2.6331e-03,  7.4575e-04,\n",
       "                      -5.0150e-03,  2.4615e-03,  1.2181e-03, -2.0392e-03,  2.6152e-03,\n",
       "                      -4.8140e-03,  1.0535e-03,  3.0990e-03, -4.9272e-03,  7.7733e-04,\n",
       "                       1.6917e-03, -6.3887e-03, -3.8372e-03,  1.8318e-03, -3.5108e-03,\n",
       "                       3.2895e-03,  3.6512e-03, -1.6178e-03, -4.3932e-03,  4.9558e-03,\n",
       "                      -3.7925e-03,  2.5583e-03,  4.7872e-03,  5.5087e-03, -4.4250e-03,\n",
       "                       3.5600e-03,  3.2493e-04,  3.2841e-03, -1.0610e-03,  3.4353e-03,\n",
       "                      -4.2727e-03,  2.7728e-03, -5.3493e-03, -2.0050e-03, -3.3032e-03,\n",
       "                      -2.8559e-03, -2.5432e-03, -5.3548e-03,  5.5119e-03, -2.9170e-03,\n",
       "                      -5.5722e-03, -2.7165e-03, -3.4963e-03, -4.7736e-04,  5.4922e-03,\n",
       "                      -4.1371e-03,  3.0100e-03,  5.0757e-03, -3.3820e-03,  4.4716e-03,\n",
       "                      -6.3922e-03,  3.6064e-03, -2.9739e-03, -4.7863e-03, -1.4957e-03,\n",
       "                       8.0335e-04,  6.3191e-03,  3.4759e-03,  5.1134e-03,  1.7103e-03,\n",
       "                       3.1798e-03,  5.2332e-03, -1.9419e-03,  4.3850e-03,  4.7624e-03,\n",
       "                       2.9784e-03, -4.3368e-03,  1.5223e-03, -2.6289e-03, -2.5139e-03,\n",
       "                      -5.8119e-04,  1.6007e-03,  4.6625e-03, -9.6831e-04,  1.8921e-03,\n",
       "                       1.1447e-03,  6.2195e-03,  4.4837e-03,  4.3585e-04,  4.4877e-03,\n",
       "                      -4.3013e-03, -5.2924e-04, -5.0586e-03, -4.2770e-03,  8.4218e-04,\n",
       "                      -3.2462e-03,  4.9620e-03, -3.0738e-03,  2.8010e-03, -5.2341e-03,\n",
       "                       2.6666e-03,  3.0954e-03,  1.8131e-03,  8.2452e-04,  5.0353e-03,\n",
       "                       2.1654e-03,  1.7422e-03, -2.0606e-03, -6.3564e-03, -6.1172e-03,\n",
       "                       3.0544e-03, -2.0211e-03, -4.6976e-03, -3.8494e-04, -5.6137e-03,\n",
       "                      -4.8984e-03, -5.3950e-03,  4.6780e-04,  4.1439e-03,  4.5378e-03,\n",
       "                      -4.7339e-03,  2.7469e-03,  3.1432e-03, -6.6668e-04,  1.7799e-03,\n",
       "                      -4.3486e-03,  1.4320e-03,  1.1848e-03,  3.6634e-03, -5.9213e-03,\n",
       "                       5.2196e-03,  4.3155e-03, -1.0952e-03, -6.1078e-03, -1.8937e-03,\n",
       "                       6.0363e-03, -1.4049e-03,  3.0216e-03, -4.8358e-03, -1.0437e-03,\n",
       "                       3.6088e-03, -5.9911e-03, -5.1240e-03,  4.8456e-03, -4.5751e-03,\n",
       "                       2.8578e-03, -5.8586e-03, -6.1223e-03,  5.0437e-03,  2.7229e-03,\n",
       "                      -5.9932e-03,  2.0580e-03, -4.3146e-03, -2.8744e-03, -3.8040e-03,\n",
       "                       3.6978e-03, -2.4007e-04, -3.7227e-03, -9.1704e-05, -1.6019e-03,\n",
       "                      -1.8568e-03,  4.9422e-03, -2.1948e-03,  2.9119e-03,  4.7300e-03,\n",
       "                       4.3602e-03,  4.2938e-03,  3.4817e-03, -3.9736e-03,  4.3653e-03,\n",
       "                      -2.3070e-03, -5.5068e-04, -5.9233e-03, -5.4553e-03, -4.5712e-03,\n",
       "                      -1.7025e-03,  3.2363e-03, -3.3206e-03,  6.4317e-04, -3.1388e-03,\n",
       "                       1.6892e-03, -5.4335e-03, -6.2463e-03,  3.3920e-03, -6.3898e-03,\n",
       "                      -2.2996e-03,  5.5927e-03,  4.6385e-03, -3.7138e-03,  3.7845e-03,\n",
       "                       3.7722e-03,  4.2237e-03,  4.9046e-03, -4.7484e-03,  6.3132e-03,\n",
       "                      -3.9084e-03, -2.1736e-03,  3.0516e-03,  6.3675e-03,  1.1853e-03,\n",
       "                       2.7175e-03,  2.7328e-03, -2.3489e-04,  5.6185e-03, -5.4846e-03,\n",
       "                      -1.8816e-03, -2.6501e-03,  1.5405e-04, -2.7954e-03,  4.1162e-03,\n",
       "                      -2.0702e-03, -2.0211e-03, -3.3824e-03,  1.4977e-03,  5.8602e-04,\n",
       "                      -2.7438e-04, -9.9488e-06])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.0299,  0.0084,  0.0204,  ..., -0.0152,  0.0217,  0.0048],\n",
       "                      [-0.0193,  0.0196,  0.0034,  ..., -0.0014,  0.0289, -0.0311],\n",
       "                      [ 0.0266, -0.0057, -0.0011,  ..., -0.0140, -0.0221,  0.0059],\n",
       "                      ...,\n",
       "                      [ 0.0185, -0.0261, -0.0029,  ...,  0.0024, -0.0150, -0.0178],\n",
       "                      [ 0.0048, -0.0166, -0.0105,  ..., -0.0315, -0.0186, -0.0228],\n",
       "                      [ 0.0261,  0.0274,  0.0071,  ...,  0.0024, -0.0311,  0.0040]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([ 2.6595e-02,  3.5873e-03,  3.4981e-03, -7.6067e-03, -2.7120e-02,\n",
       "                       1.5247e-02,  3.0984e-02,  1.4619e-02, -2.0182e-02,  3.0577e-02,\n",
       "                       6.3027e-03, -1.5378e-02, -1.8643e-02,  2.5948e-02, -4.1759e-03,\n",
       "                       1.6249e-02,  9.2970e-04,  2.2530e-02, -2.2292e-02, -6.5986e-03,\n",
       "                       2.3807e-02, -2.9361e-02, -4.2232e-03,  1.2216e-02,  1.8670e-02,\n",
       "                       1.8004e-02,  1.7831e-02,  4.8148e-03,  2.8183e-02, -2.3411e-03,\n",
       "                      -2.4413e-02,  1.7420e-02, -2.5507e-02,  5.7934e-03, -1.5301e-02,\n",
       "                      -2.3335e-02, -2.9780e-02, -1.7693e-02, -1.0459e-02,  4.3754e-03,\n",
       "                      -9.3030e-03, -2.3913e-02,  1.2695e-02, -2.6631e-02,  3.7919e-03,\n",
       "                      -1.3916e-02,  8.6526e-03, -1.4728e-02,  2.4706e-03,  1.8742e-02,\n",
       "                      -2.9472e-03, -2.6845e-03, -8.7138e-03,  2.6865e-02,  3.0016e-02,\n",
       "                       1.9108e-02,  1.6705e-03, -1.4091e-02, -3.1241e-02, -2.6985e-03,\n",
       "                      -7.4053e-03,  1.7752e-03, -1.9641e-02, -2.4730e-02,  3.0785e-02,\n",
       "                       2.3853e-02, -1.3572e-02,  1.3372e-02, -2.6718e-02, -1.4826e-02,\n",
       "                      -2.2611e-02, -1.4449e-02,  2.3165e-02, -2.7693e-02, -2.0173e-02,\n",
       "                       1.2368e-03, -1.1791e-02, -5.0349e-03,  1.9928e-02, -8.7429e-04,\n",
       "                       2.4367e-02, -7.5577e-04, -8.1131e-03, -2.7153e-02, -1.6164e-02,\n",
       "                       1.3396e-02,  2.2001e-02,  2.2146e-02, -1.2315e-02,  3.1820e-02,\n",
       "                       1.1216e-02,  2.8357e-02, -7.1095e-03, -9.1565e-04, -1.1262e-02,\n",
       "                       1.3353e-02,  3.3009e-03,  4.2808e-03,  1.6377e-02,  1.1097e-02,\n",
       "                      -1.0168e-02, -1.2977e-02, -7.2766e-03,  1.8625e-02,  1.3784e-02,\n",
       "                      -3.5124e-03, -3.5474e-03, -2.6590e-02,  2.2508e-02, -3.2062e-02,\n",
       "                       1.4025e-02,  2.3692e-02,  7.4628e-04, -9.0469e-03, -2.4840e-02,\n",
       "                       8.0349e-03, -1.1458e-02, -2.8981e-02, -2.8989e-02,  1.3828e-02,\n",
       "                       9.5703e-03, -1.2945e-02, -1.0133e-02, -4.0159e-03, -1.4870e-02,\n",
       "                      -1.5102e-03, -5.2130e-03, -1.0413e-02, -2.3812e-02,  1.4621e-02,\n",
       "                      -5.2006e-03, -1.1986e-02,  2.2949e-02, -2.9776e-02, -6.0382e-03,\n",
       "                      -1.8489e-02,  8.9938e-03,  1.9626e-02, -1.0174e-02, -1.8869e-03,\n",
       "                      -1.4228e-02, -1.2129e-02,  2.2260e-02, -4.8513e-03, -2.2178e-02,\n",
       "                       1.8454e-02, -1.4767e-02,  1.5427e-02,  2.5215e-03,  1.5613e-04,\n",
       "                       5.1418e-03,  1.9110e-02,  2.2013e-02,  8.6451e-03,  9.6703e-03,\n",
       "                       2.4841e-02,  4.1938e-03, -5.1809e-03, -1.2748e-02,  1.8734e-02,\n",
       "                       2.7966e-02,  2.3051e-02,  2.1154e-02,  2.3939e-02,  2.7570e-02,\n",
       "                      -9.3773e-03,  1.5547e-02,  2.8995e-02,  1.7880e-02, -2.0966e-02,\n",
       "                       2.8348e-02,  3.6942e-03,  2.5075e-02,  1.2412e-02, -3.0106e-02,\n",
       "                      -3.1521e-02, -1.1916e-02, -8.2519e-03, -2.4177e-02, -1.8028e-02,\n",
       "                      -6.2744e-03,  2.3644e-03,  9.4141e-03, -2.4557e-02,  3.2904e-03,\n",
       "                       1.3648e-02, -7.0397e-03,  8.1578e-03, -2.4474e-02,  2.0455e-02,\n",
       "                       2.3323e-02, -3.0211e-02, -3.4928e-03, -2.0983e-02, -2.5696e-03,\n",
       "                       3.2735e-03,  2.9872e-02,  2.9022e-02,  1.3931e-02, -1.0117e-02,\n",
       "                       1.4408e-02, -1.5184e-02, -2.6472e-02, -1.5431e-02,  1.1133e-02,\n",
       "                      -1.6639e-03, -2.8334e-02,  1.7454e-02, -1.4690e-02,  2.1746e-02,\n",
       "                      -1.3429e-02,  1.0418e-02, -1.4112e-02,  2.4691e-02,  2.6810e-02,\n",
       "                      -8.8535e-03,  1.6801e-02, -1.5538e-03, -2.8193e-02, -1.2432e-02,\n",
       "                       2.4600e-02, -2.0999e-02,  2.6506e-02,  6.6772e-03, -1.4509e-02,\n",
       "                      -1.7926e-02,  1.0534e-02, -1.1265e-02,  2.0750e-02, -3.0027e-02,\n",
       "                      -7.9777e-03, -2.8590e-02,  1.4266e-02, -7.6106e-03,  2.6853e-02,\n",
       "                       2.3318e-03,  1.6452e-02, -1.4461e-02,  1.5624e-02, -3.1752e-02,\n",
       "                       1.5475e-02, -2.5553e-02, -1.8658e-02,  7.2697e-03,  2.0731e-02,\n",
       "                       2.6503e-02, -8.4021e-03, -1.4125e-02, -5.3780e-03,  2.4573e-02,\n",
       "                      -2.4782e-02, -8.0332e-03, -1.3033e-02, -5.7400e-03, -3.1650e-02,\n",
       "                       1.0452e-02, -3.1589e-03,  3.4701e-03, -4.1061e-03,  2.5026e-02,\n",
       "                       1.7663e-02, -1.0529e-02, -6.1699e-03, -2.1975e-02,  5.2397e-03,\n",
       "                       7.1728e-03,  2.3961e-02,  3.7519e-03, -1.3291e-02,  3.1677e-02,\n",
       "                       2.5864e-02, -2.7398e-02,  1.6529e-02, -2.1482e-02,  1.7663e-02,\n",
       "                      -2.5614e-02,  3.2003e-02, -2.7567e-02, -3.0177e-02, -1.4233e-02,\n",
       "                       2.5165e-02,  3.6802e-03, -8.4172e-03, -2.4453e-02, -1.4906e-02,\n",
       "                      -1.6892e-02, -1.7870e-03,  1.0333e-02,  1.4346e-02,  1.2620e-02,\n",
       "                       1.8500e-02, -2.3820e-02, -3.1830e-02, -2.4710e-02,  8.3973e-03,\n",
       "                       3.6592e-03, -2.7127e-02, -4.3796e-03, -1.9342e-02, -2.5601e-02,\n",
       "                       2.8415e-02,  1.6714e-02,  1.6424e-02, -2.1926e-03, -2.0163e-02,\n",
       "                      -1.4376e-02,  2.1767e-03, -1.8309e-02,  2.7871e-02,  2.5952e-02,\n",
       "                       2.7441e-02,  1.5574e-02, -1.1892e-02,  3.0734e-02,  2.2307e-02,\n",
       "                      -1.9666e-02, -1.3078e-02,  3.0413e-02,  1.0894e-02, -3.2984e-03,\n",
       "                      -1.7716e-02, -9.7356e-03, -2.6321e-02,  1.0708e-02,  5.2905e-03,\n",
       "                       4.9703e-03,  2.1703e-02, -3.1559e-02, -1.6369e-02, -2.8311e-02,\n",
       "                      -1.8448e-02, -1.8260e-02,  1.7240e-02,  1.3090e-02,  3.1691e-02,\n",
       "                       1.2747e-02, -1.9125e-02,  2.8787e-02, -5.9632e-03,  8.4369e-04,\n",
       "                       1.6238e-03, -2.8331e-02, -2.4430e-02,  2.4285e-02, -1.2267e-02,\n",
       "                      -3.1757e-02,  1.9619e-02,  1.1761e-02, -6.1231e-03,  1.9305e-02,\n",
       "                      -2.7483e-02, -1.8732e-02,  2.1626e-02, -2.8307e-02,  2.5443e-02,\n",
       "                       2.0680e-02, -1.7695e-02, -2.2299e-02, -2.7143e-02,  2.3193e-02,\n",
       "                       1.5429e-02,  3.9448e-03,  1.4914e-02, -1.4685e-02,  4.1206e-04,\n",
       "                      -1.6024e-02,  1.6484e-02,  1.5738e-02, -2.2183e-02,  3.0525e-02,\n",
       "                       6.3925e-04,  3.0364e-02, -2.2403e-02, -8.6258e-03,  2.2589e-02,\n",
       "                      -1.2498e-02, -1.4179e-02,  1.8705e-02,  1.8972e-03, -1.3089e-02,\n",
       "                      -2.1608e-02, -1.4649e-02, -6.0348e-03, -8.7155e-03,  1.4449e-02,\n",
       "                      -2.7143e-02,  1.4123e-02,  1.2687e-02, -2.3158e-02, -4.7237e-03,\n",
       "                       3.1491e-02,  2.8706e-03,  3.0183e-02,  1.9907e-02,  2.2187e-02,\n",
       "                      -7.5094e-03,  3.2799e-03, -1.2348e-03, -2.7047e-02,  6.1258e-03,\n",
       "                       2.2327e-03, -9.0592e-03,  3.1380e-02,  6.0525e-03, -1.9612e-02,\n",
       "                       2.9501e-03, -4.0037e-04, -1.8172e-02, -1.2711e-02, -2.9758e-03,\n",
       "                       2.3477e-02, -3.1912e-02, -1.7508e-02, -5.9470e-03,  3.1087e-02,\n",
       "                      -1.3112e-02, -1.7354e-02, -1.8201e-02, -2.4651e-02, -2.4278e-02,\n",
       "                      -5.7249e-03,  2.8635e-02, -3.0665e-02, -1.9588e-02,  2.6464e-02,\n",
       "                      -1.3300e-04, -1.7630e-02, -2.7915e-02,  1.8150e-02,  1.2278e-02,\n",
       "                       1.1004e-03, -2.2775e-02,  5.4412e-03, -1.0707e-02,  1.8673e-02,\n",
       "                       1.4728e-02, -2.1323e-02,  8.6216e-03,  2.3230e-02,  1.0891e-02,\n",
       "                      -3.7827e-03,  1.9303e-02,  1.0127e-02, -2.7518e-02, -2.4084e-02,\n",
       "                      -3.1122e-02,  2.5757e-02,  2.9188e-02, -1.8220e-02, -2.3684e-03,\n",
       "                       2.4627e-02, -2.7615e-02,  1.9287e-02,  1.7906e-02,  2.2854e-02,\n",
       "                      -4.8103e-03,  2.3713e-02, -1.5224e-02,  3.9934e-03, -2.3979e-02,\n",
       "                       5.1547e-03, -1.7629e-02, -2.9691e-02,  2.0772e-02,  3.0780e-02,\n",
       "                      -9.3301e-05,  8.7270e-03, -2.2374e-02, -2.4122e-02, -2.2571e-02,\n",
       "                       1.0708e-02, -2.8629e-02, -1.6195e-02, -2.4883e-02, -1.5492e-02,\n",
       "                       2.1864e-02,  2.0536e-02, -2.7228e-02,  1.4532e-03,  5.4342e-03,\n",
       "                       2.9139e-02,  2.7230e-02,  2.2955e-02, -2.1739e-02, -6.3040e-04,\n",
       "                      -2.1777e-02])),\n",
       "             ('fc3.weight',\n",
       "              tensor([[-0.0090,  0.0181, -0.0049,  ...,  0.0125,  0.0430,  0.0283],\n",
       "                      [-0.0313, -0.0153, -0.0178,  ..., -0.0142, -0.0298, -0.0097],\n",
       "                      [ 0.0076,  0.0336,  0.0385,  ..., -0.0371,  0.0305,  0.0259],\n",
       "                      ...,\n",
       "                      [ 0.0047, -0.0262, -0.0050,  ...,  0.0239,  0.0122,  0.0123],\n",
       "                      [-0.0211, -0.0434, -0.0382,  ..., -0.0046, -0.0321,  0.0171],\n",
       "                      [-0.0262,  0.0382, -0.0416,  ...,  0.0326, -0.0388, -0.0353]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([ 4.3782e-02, -9.8346e-03, -3.7183e-02, -1.6021e-02,  1.2499e-02,\n",
       "                       1.3330e-02,  2.7281e-02,  9.4626e-03,  4.3610e-02, -1.9255e-02,\n",
       "                       3.0745e-02,  2.5952e-02,  2.8762e-02, -1.2026e-02, -4.2118e-02,\n",
       "                       3.8359e-02,  3.4743e-02, -2.9268e-02,  1.4819e-02, -2.3833e-02,\n",
       "                       1.9766e-02,  2.9141e-02,  4.9181e-03,  9.0992e-03, -2.7148e-02,\n",
       "                       1.3947e-02,  1.4738e-02, -4.2254e-02, -2.2474e-02, -2.4574e-02,\n",
       "                      -2.0580e-02,  2.9488e-03, -2.0708e-03, -9.2505e-03, -1.9225e-02,\n",
       "                      -3.3002e-02,  1.6853e-03,  8.9091e-03, -1.7839e-03,  3.6788e-02,\n",
       "                       3.1096e-02,  2.9935e-02, -1.0172e-02, -2.6143e-02,  5.5043e-03,\n",
       "                       3.6518e-02, -4.4039e-02, -2.2291e-03, -4.1270e-02,  3.8600e-02,\n",
       "                       1.2719e-02,  2.4828e-02, -3.6773e-03, -3.9866e-02,  4.0018e-02,\n",
       "                       5.6026e-03, -3.4923e-02, -4.0739e-02,  3.6643e-02, -1.8028e-02,\n",
       "                      -1.3311e-03,  6.6049e-03, -3.5677e-02, -4.2399e-02, -1.1234e-02,\n",
       "                      -3.3138e-02,  3.9232e-03, -1.0941e-02, -1.7420e-02, -3.9858e-02,\n",
       "                       4.5008e-02,  2.4374e-02,  3.2486e-02,  3.3791e-02,  2.7488e-02,\n",
       "                      -4.1453e-02,  1.4422e-02,  2.9204e-02, -3.7984e-02,  3.7531e-03,\n",
       "                       2.9769e-02,  6.9267e-03, -1.8442e-02,  3.5876e-02, -4.1639e-02,\n",
       "                       4.1343e-02,  4.5154e-02, -1.6740e-02, -4.8175e-03, -3.5471e-02,\n",
       "                       1.1892e-02,  1.9123e-03,  2.3519e-02,  2.8104e-02,  2.9536e-02,\n",
       "                      -4.5161e-02, -5.0399e-03, -1.4066e-02, -3.1953e-02, -3.0021e-02,\n",
       "                      -6.7246e-03,  1.1127e-02,  2.5065e-02,  4.2137e-02,  3.5214e-02,\n",
       "                       1.9764e-03, -7.2618e-03, -2.2803e-03, -6.5083e-04,  3.0698e-02,\n",
       "                       1.6726e-02, -4.2329e-02,  1.2281e-02,  2.6158e-02,  3.5082e-03,\n",
       "                       2.1574e-02, -1.9124e-02, -3.6804e-02, -3.9737e-02, -3.7997e-02,\n",
       "                       3.9961e-02,  3.8948e-02,  2.3577e-02, -3.4333e-03,  2.2843e-02,\n",
       "                       1.1843e-02, -4.6017e-05,  2.7333e-02,  2.9188e-02,  3.8889e-02,\n",
       "                      -2.7240e-02, -4.1223e-02,  8.3345e-05, -2.8768e-03, -3.1658e-02,\n",
       "                      -3.9557e-02, -3.4145e-03,  1.6650e-02, -3.0028e-02, -7.3721e-03,\n",
       "                      -4.1596e-03, -1.4797e-02,  3.8821e-02,  1.8222e-02,  2.9611e-02,\n",
       "                      -4.1808e-02,  1.5769e-02,  1.0228e-02, -2.6099e-02, -1.7884e-02,\n",
       "                      -2.1660e-02, -1.6702e-02,  1.9716e-02, -1.2802e-02, -1.0873e-02,\n",
       "                       1.5216e-02,  3.6722e-03, -2.0507e-02, -3.3235e-02,  3.5809e-02])),\n",
       "             ('fc4.weight',\n",
       "              tensor([[-0.0430,  0.0239, -0.0079,  ...,  0.0618,  0.0113, -0.0365],\n",
       "                      [ 0.0639,  0.0422, -0.0176,  ...,  0.0738,  0.0237,  0.0750],\n",
       "                      [ 0.0302, -0.0207, -0.0524,  ...,  0.0029,  0.0045,  0.0482],\n",
       "                      ...,\n",
       "                      [ 0.0520, -0.0268, -0.0285,  ..., -0.0672, -0.0706, -0.0476],\n",
       "                      [ 0.0724,  0.0109,  0.0423,  ...,  0.0546,  0.0710, -0.0126],\n",
       "                      [-0.0143,  0.0242,  0.0030,  ..., -0.0650, -0.0066, -0.0417]])),\n",
       "             ('fc4.bias',\n",
       "              tensor([-0.0661,  0.0184, -0.0185,  0.0167,  0.0425, -0.0610, -0.0089,  0.0750,\n",
       "                      -0.0123, -0.0748, -0.0043, -0.0111, -0.0536, -0.0526,  0.0292, -0.0511,\n",
       "                       0.0218,  0.0089, -0.0171, -0.0568,  0.0394, -0.0021,  0.0302, -0.0504,\n",
       "                       0.0242, -0.0378, -0.0749,  0.0125, -0.0209, -0.0475,  0.0467, -0.0202,\n",
       "                      -0.0131,  0.0604, -0.0011, -0.0682, -0.0397, -0.0742,  0.0378, -0.0470])),\n",
       "             ('fc5.weight',\n",
       "              tensor([[-0.0656, -0.0121, -0.1390, -0.0051,  0.0138,  0.1469,  0.1483, -0.0436,\n",
       "                       -0.1205,  0.0942,  0.1160, -0.1020,  0.0686,  0.0407,  0.0841, -0.0800,\n",
       "                       -0.1191,  0.0138,  0.0078, -0.0264, -0.0876, -0.0662,  0.0268, -0.1187,\n",
       "                       -0.1218, -0.0499,  0.0294, -0.0234,  0.0864, -0.1441,  0.1412,  0.1216,\n",
       "                       -0.1194, -0.0049, -0.1344,  0.1548,  0.1179,  0.0181, -0.1069, -0.0716],\n",
       "                      [-0.1408,  0.0211,  0.1182,  0.0718, -0.0270, -0.0735,  0.0314,  0.0993,\n",
       "                        0.1083, -0.0316,  0.0703, -0.0711,  0.0998,  0.0144, -0.1075,  0.0944,\n",
       "                        0.0855,  0.1380, -0.1378, -0.1077, -0.1176, -0.0435,  0.0999,  0.0356,\n",
       "                       -0.1322,  0.0382, -0.0705,  0.0690,  0.1337, -0.1454, -0.0174, -0.1083,\n",
       "                       -0.0663,  0.1540, -0.1170,  0.0404,  0.0273,  0.1441,  0.1277, -0.0805],\n",
       "                      [ 0.0365,  0.1569,  0.0833,  0.1401, -0.1238, -0.0294, -0.0770,  0.1311,\n",
       "                       -0.0853,  0.0997,  0.0549, -0.1072,  0.0490,  0.1235, -0.0363,  0.1029,\n",
       "                       -0.0794, -0.0688, -0.0392,  0.1563,  0.0098,  0.1570, -0.0042, -0.0568,\n",
       "                       -0.1094,  0.0241, -0.1559,  0.1034,  0.0549,  0.1329,  0.0745, -0.0207,\n",
       "                       -0.0903,  0.1279, -0.0918,  0.1152,  0.1398,  0.0797,  0.0476, -0.0416]])),\n",
       "             ('fc5.bias', tensor([-0.0604,  0.1310,  0.0598]))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_MLP.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that the values are not 0? This is actually by design - by default that initialization follows an accepted scheme - but many strategies are possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at sequential version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simpleMLP import SimpleMLPSEQ\n",
    "model_MLPSEQ=SimpleMLPSEQ(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of a parameter: _sequence.0.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.0.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.2.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.2.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.4.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.4.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.6.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.6.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.8.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: _sequence.8.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_MLPSEQ.named_parameters():\n",
    "    print(\"name of a parameter: {}, type: {}, parameter requires a gradient?: {}\".\n",
    "          format(name, type(param),param.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('_sequence.0.weight', tensor([[ 1.1240e-03, -5.0516e-03, -4.5424e-03,  ...,  1.3199e-03,\n",
      "         -5.0092e-03, -4.8747e-04],\n",
      "        [ 2.4317e-03,  1.6055e-04,  3.8558e-03,  ..., -9.1580e-04,\n",
      "         -2.0747e-03, -6.1886e-04],\n",
      "        [ 3.0045e-03, -3.4215e-03,  3.0121e-03,  ...,  3.5183e-03,\n",
      "          2.1762e-03,  1.1849e-03],\n",
      "        ...,\n",
      "        [-3.2619e-03,  1.5118e-04, -1.4491e-03,  ...,  4.7094e-03,\n",
      "         -5.8456e-03,  4.2822e-03],\n",
      "        [ 4.4278e-03, -8.9968e-04, -6.4112e-03,  ...,  5.8452e-03,\n",
      "         -1.5233e-03,  2.3909e-03],\n",
      "        [-8.7048e-05,  1.0830e-03, -2.7301e-03,  ...,  3.2229e-03,\n",
      "         -1.5551e-04, -4.6648e-03]])), ('_sequence.0.bias', tensor([-5.4659e-03,  5.6283e-04, -3.1679e-03, -5.2000e-03, -3.8694e-03,\n",
      "        -4.4652e-03, -6.5826e-04,  1.6300e-03,  4.4858e-03,  1.5823e-03,\n",
      "        -6.3505e-03, -3.8530e-03, -4.0163e-04, -4.4037e-03,  2.7814e-03,\n",
      "        -4.6127e-03,  1.0991e-03,  3.1771e-03,  3.3054e-03,  4.4030e-03,\n",
      "         3.3014e-03, -5.7809e-03, -3.1965e-03,  2.3211e-03, -3.7805e-03,\n",
      "        -6.3919e-04,  6.3071e-03,  2.0043e-03, -2.1911e-03, -3.2227e-03,\n",
      "        -4.0719e-03, -3.9361e-03, -1.0452e-03,  9.9769e-04,  2.3139e-03,\n",
      "        -3.6727e-03, -5.2911e-03, -2.1568e-03,  6.0235e-03, -2.4461e-03,\n",
      "         3.4462e-03,  2.5665e-03, -4.6455e-03, -8.6657e-04, -6.3343e-03,\n",
      "         3.5604e-03,  6.2707e-03,  4.6411e-03, -3.7959e-03,  2.1821e-04,\n",
      "        -2.7234e-03, -2.3080e-03, -5.6458e-03,  4.4857e-03,  2.2220e-03,\n",
      "         1.2127e-03,  1.6091e-03, -2.1947e-03, -6.1063e-03,  1.4382e-03,\n",
      "        -5.7097e-03,  1.9946e-03,  7.8986e-04,  1.7774e-03,  1.1413e-03,\n",
      "         1.0012e-03, -1.0237e-03, -4.5569e-03,  8.6906e-04,  5.2276e-03,\n",
      "         4.2142e-06, -6.8061e-04,  4.8589e-03,  2.3173e-03, -4.0212e-03,\n",
      "        -2.2587e-03,  3.7722e-03, -9.6646e-04, -2.1446e-03,  5.8422e-03,\n",
      "         6.3355e-03, -8.7616e-04, -1.4839e-03,  1.6860e-03, -1.4618e-03,\n",
      "         4.8041e-03, -1.7863e-03, -2.7352e-03,  1.2118e-03, -6.3177e-03,\n",
      "         4.1256e-04, -1.4808e-03,  6.1153e-03,  1.6328e-03, -2.7511e-03,\n",
      "         3.4515e-03, -4.4222e-03,  4.0606e-03,  1.4163e-03,  2.9377e-03,\n",
      "         4.9081e-03,  1.6485e-03,  5.5852e-03, -3.7592e-03, -1.2560e-03,\n",
      "         2.4800e-03,  2.9400e-03, -2.4013e-03, -4.7523e-04, -3.9036e-03,\n",
      "        -4.3913e-03, -1.4441e-03, -5.5278e-03, -2.7181e-03,  1.1853e-03,\n",
      "        -6.1358e-03,  6.5249e-04, -4.7506e-03,  3.3011e-03,  4.7768e-03,\n",
      "         2.3215e-03, -6.1061e-03,  3.8084e-03, -5.4340e-03,  4.2685e-03,\n",
      "        -3.4754e-03, -5.2767e-04, -2.0515e-03, -4.9544e-03,  4.0535e-03,\n",
      "         5.1436e-03,  5.4442e-03,  4.4285e-03,  4.3566e-03,  4.0808e-03,\n",
      "         2.9720e-03,  1.9751e-03, -4.3910e-04,  4.0045e-03, -3.6886e-03,\n",
      "         1.3243e-03,  1.9612e-04,  9.4035e-04, -4.5929e-04, -4.2034e-03,\n",
      "         5.0630e-03, -4.2166e-03,  4.0168e-03, -1.6393e-03,  4.1559e-03,\n",
      "         6.2345e-03, -3.2510e-04, -3.5902e-03,  4.2482e-03, -3.8125e-03,\n",
      "         4.0245e-03, -4.6060e-03, -1.1416e-03,  3.4817e-04, -1.2502e-03,\n",
      "        -2.3614e-03, -1.4339e-03,  2.1254e-03,  6.1495e-03,  6.2760e-04,\n",
      "         4.4942e-04,  8.3410e-04,  1.1591e-03, -4.8815e-04,  5.3950e-03,\n",
      "        -3.7183e-03, -5.5564e-03,  5.3730e-03,  2.7081e-03, -5.6165e-03,\n",
      "         6.2045e-03, -4.8676e-03,  4.6969e-03, -5.7836e-03, -1.8849e-03,\n",
      "         4.6161e-03,  1.8179e-04, -9.6030e-04, -4.5498e-03, -4.1825e-03,\n",
      "        -6.0140e-03,  3.3812e-03,  1.1573e-03, -4.5269e-03, -3.4969e-03,\n",
      "         5.7547e-03,  3.6371e-03,  2.7128e-03,  5.7993e-03, -5.9136e-03,\n",
      "        -2.1285e-03, -3.6107e-03, -3.3936e-03,  6.4055e-03, -3.5507e-03,\n",
      "        -8.6508e-04, -1.3577e-03, -1.8668e-03, -6.2707e-03, -5.1316e-03,\n",
      "        -4.0608e-03, -5.8201e-03, -5.5505e-03, -2.9423e-04,  6.0249e-04,\n",
      "        -6.3094e-03, -4.7909e-03,  4.0978e-04, -5.4505e-03,  4.6423e-03,\n",
      "        -3.7724e-03,  3.4193e-03,  1.1578e-03,  5.3259e-03, -2.7314e-03,\n",
      "        -1.4843e-03,  3.2662e-03, -2.8919e-03,  5.3142e-03, -4.9312e-03,\n",
      "        -1.7598e-03, -1.5314e-03,  3.7869e-03, -5.0378e-05, -4.1221e-03,\n",
      "         4.2835e-04, -4.8120e-03,  5.0153e-03, -1.1244e-03, -5.5713e-04,\n",
      "         1.3635e-03, -4.5760e-03,  1.8729e-03,  2.0548e-03,  6.0473e-03,\n",
      "        -2.1634e-03, -5.2083e-04,  6.6764e-04,  5.6113e-03, -1.8301e-03,\n",
      "        -1.4705e-03,  6.3266e-03, -7.9528e-04,  2.2109e-03, -4.0006e-04,\n",
      "         3.7214e-03, -3.7433e-03, -1.7867e-03, -1.5497e-04, -1.9637e-04,\n",
      "        -1.3216e-03, -6.0355e-03, -4.0825e-03, -4.8369e-03,  3.8015e-03,\n",
      "        -7.6992e-04,  1.8124e-03, -4.3226e-03, -5.9056e-03, -5.6755e-03,\n",
      "        -2.8077e-03, -5.1228e-04,  4.8972e-03, -1.2927e-03, -1.7644e-03,\n",
      "         1.3024e-05, -2.7671e-03,  5.3528e-03,  4.6455e-03,  1.4014e-03,\n",
      "        -4.3776e-03,  5.4705e-03, -4.1449e-03,  5.1556e-03, -8.1888e-04,\n",
      "         8.8113e-04, -7.0734e-05,  1.5617e-03, -1.8839e-03, -4.3128e-04,\n",
      "         3.9447e-03,  3.4508e-03, -3.7654e-03,  5.5766e-03, -3.7456e-03,\n",
      "         4.7243e-03, -4.5082e-03,  3.4177e-03,  5.1364e-03, -1.7316e-03,\n",
      "         1.3015e-03, -4.4977e-03,  4.5227e-03, -3.5716e-03, -5.3636e-03,\n",
      "        -5.0379e-03,  5.1001e-03, -3.0539e-03,  2.9022e-03, -2.7551e-03,\n",
      "         4.9875e-03, -2.6912e-03,  8.0209e-04, -2.2508e-03, -4.3855e-04,\n",
      "        -2.4323e-04,  1.2017e-03,  2.2825e-03, -3.7967e-03,  2.5336e-03,\n",
      "         4.2060e-03,  6.1460e-03, -1.7378e-03,  5.7781e-03, -5.1074e-03,\n",
      "        -5.3887e-03, -4.4124e-03, -6.2011e-03,  5.2201e-03,  4.9063e-03,\n",
      "         5.9127e-03, -3.3316e-03,  3.3529e-04,  2.7000e-03,  1.0832e-03,\n",
      "         5.6792e-04, -5.0483e-03,  3.4855e-03,  1.4170e-03, -2.2480e-03,\n",
      "         2.4367e-04, -5.3214e-03, -6.8719e-04, -1.5595e-03,  3.6263e-03,\n",
      "         2.9536e-03,  5.9435e-03,  5.1373e-04, -3.1872e-03, -2.9732e-03,\n",
      "        -4.3144e-03, -2.0515e-03,  2.4714e-03,  1.4976e-03,  1.7702e-03,\n",
      "        -1.1353e-03,  6.2557e-03, -1.2156e-04, -3.7052e-03, -8.9263e-04,\n",
      "         1.4061e-03,  6.3661e-03, -3.5530e-03,  5.8919e-03,  4.3176e-03,\n",
      "         5.2871e-03,  5.9547e-03, -2.6328e-04, -2.3694e-03,  5.4981e-03,\n",
      "        -4.7648e-03,  6.2564e-03, -3.2770e-03, -1.9156e-03,  2.6057e-03,\n",
      "         4.4306e-03, -9.6679e-04, -3.3283e-03,  3.9486e-03, -3.5202e-04,\n",
      "         2.2376e-03, -6.0153e-03,  6.3610e-04, -3.0655e-03, -2.8887e-03,\n",
      "         3.8074e-03, -4.3435e-03, -3.7910e-03,  2.4650e-05,  3.4192e-03,\n",
      "         3.2719e-03,  3.7593e-03,  6.1582e-03, -1.5842e-03, -3.1437e-03,\n",
      "         4.6555e-03,  3.9144e-03,  2.2446e-03, -2.8797e-03, -6.0413e-03,\n",
      "         4.9054e-03,  5.2075e-03,  7.8205e-04, -5.9906e-03,  3.4736e-03,\n",
      "        -2.3884e-03, -1.2199e-03,  2.5057e-03, -7.8586e-04, -5.5511e-03,\n",
      "        -1.2414e-03, -3.0782e-03, -4.5306e-03,  5.7362e-03, -4.6785e-04,\n",
      "         2.5036e-03,  5.7127e-03,  5.8554e-04,  1.5585e-03,  3.9559e-03,\n",
      "        -5.5914e-03,  4.9128e-03,  3.6942e-03,  3.5847e-03, -5.9843e-03,\n",
      "         2.1429e-03, -2.2331e-03, -3.4169e-03, -4.2654e-03,  1.8531e-03,\n",
      "        -4.8444e-03, -5.2829e-03, -4.1758e-03,  5.4150e-03, -6.2480e-03,\n",
      "         3.4555e-03,  2.7250e-03, -3.4381e-03, -6.1341e-03, -4.6367e-03,\n",
      "        -6.7475e-04, -1.9479e-04, -2.5220e-03, -4.5158e-04,  6.2918e-03,\n",
      "        -3.7301e-03,  4.1997e-03, -1.7351e-03,  5.2139e-03, -1.8681e-04,\n",
      "        -1.3772e-03, -3.0518e-03,  2.9390e-03, -5.1497e-03,  4.9421e-03,\n",
      "         1.9344e-03,  3.6315e-03,  3.1088e-03, -3.6616e-03, -2.5112e-03,\n",
      "        -3.7018e-03,  8.3682e-04,  6.1312e-03, -2.5836e-03, -3.6057e-03,\n",
      "         4.7054e-03, -4.1344e-03,  3.0729e-03,  3.6359e-03,  5.3106e-03,\n",
      "        -6.0587e-03, -4.1066e-03,  2.4067e-03,  6.0997e-03,  4.7206e-03,\n",
      "         1.1333e-03, -4.2733e-03,  4.3853e-03, -5.3255e-03, -1.6714e-04,\n",
      "         3.7478e-03, -5.5306e-03, -1.8232e-03,  1.0180e-03,  3.4047e-03,\n",
      "        -5.7111e-03,  1.8495e-03,  3.3275e-03, -2.5548e-03, -4.6756e-03,\n",
      "         1.7491e-03,  5.9482e-03, -3.8192e-03,  2.1383e-03,  6.8081e-05,\n",
      "         2.7931e-03, -5.9733e-04,  5.9808e-03, -2.5260e-03, -2.5377e-04,\n",
      "        -3.6721e-03,  3.6720e-03, -1.4609e-03, -1.9443e-03,  5.9149e-03,\n",
      "         2.2179e-03,  6.3425e-03,  5.5878e-03, -4.7818e-03, -9.6706e-04,\n",
      "         5.0786e-03,  1.0532e-03,  3.1110e-03,  3.1854e-03, -1.4206e-03,\n",
      "        -3.0337e-05,  2.6986e-03, -5.0511e-03,  1.1424e-03, -2.6892e-03,\n",
      "        -3.7095e-03, -6.1750e-03,  2.1420e-03,  5.9198e-04,  1.3396e-03,\n",
      "        -4.9249e-03,  2.3524e-03,  3.4994e-03, -3.4398e-03,  3.8314e-03,\n",
      "        -5.3633e-03,  1.0688e-03,  5.6285e-03, -5.8770e-03, -4.5715e-03,\n",
      "         4.2030e-03, -1.7129e-03, -2.0575e-03, -3.0136e-03,  1.1570e-03,\n",
      "         2.2513e-03,  5.3876e-03,  3.6875e-03, -3.4970e-03, -1.8461e-03,\n",
      "        -5.9131e-03, -6.2576e-03,  6.2063e-03,  6.3902e-03,  5.8946e-03,\n",
      "        -5.9118e-03, -1.3668e-03,  5.4142e-03,  1.7436e-03, -8.9753e-04,\n",
      "        -5.3656e-04, -2.2964e-03, -5.1171e-04,  4.0034e-03, -1.3391e-03,\n",
      "         5.2241e-03, -3.9873e-03,  4.2785e-03,  3.9912e-04, -4.7776e-03,\n",
      "         1.0027e-03,  5.3507e-03, -3.8201e-03,  2.1599e-03, -5.0049e-03,\n",
      "        -8.5581e-04, -4.4649e-03, -4.5413e-03, -4.8715e-03, -1.6032e-04,\n",
      "         1.0938e-03, -1.6164e-04,  4.4558e-04,  1.2988e-03, -1.5740e-03,\n",
      "         3.9191e-03,  4.7095e-03, -6.9397e-04, -5.4528e-03, -1.6047e-03,\n",
      "        -3.6030e-03, -6.0805e-03,  2.3203e-03, -3.0280e-03, -5.2061e-04,\n",
      "         2.0589e-03,  1.3763e-03,  4.3927e-03, -4.0651e-03, -4.2229e-03,\n",
      "        -4.9113e-03, -3.4560e-03,  9.0708e-04,  5.4251e-03,  4.1661e-03,\n",
      "         4.4371e-03,  3.9832e-03,  1.3441e-03, -2.7480e-03, -3.7759e-03,\n",
      "         8.8348e-04,  5.6631e-03, -3.8963e-03,  2.6145e-03, -5.3695e-03,\n",
      "         2.9532e-03,  9.8066e-04, -4.0962e-03, -5.5428e-03, -4.3742e-03,\n",
      "        -4.2623e-03, -4.0352e-03,  1.5824e-03, -6.0512e-03,  6.1036e-03,\n",
      "        -3.5467e-03,  3.2227e-03, -4.1907e-03, -2.0545e-03,  3.2459e-03,\n",
      "         4.4366e-03,  3.8935e-03,  4.0204e-03,  6.0264e-03, -2.3307e-03,\n",
      "         3.5525e-03, -1.4154e-03, -2.9258e-03,  1.1301e-03, -2.3954e-03,\n",
      "        -6.8849e-04, -2.6870e-04, -1.9432e-03,  3.6830e-03, -5.5510e-03,\n",
      "         5.4261e-03, -5.4985e-03,  3.2962e-03,  4.4679e-03, -1.0805e-03,\n",
      "        -4.2092e-03,  4.9633e-03, -3.6299e-03, -1.8347e-03,  5.4629e-03,\n",
      "         2.4384e-04,  3.7323e-03, -6.0009e-03, -5.2812e-03, -9.3369e-05,\n",
      "        -1.4293e-03,  2.6959e-03,  5.4459e-03,  4.0631e-03, -3.8339e-03,\n",
      "        -1.6662e-03,  6.9021e-04, -4.5614e-03,  4.5599e-03, -5.7920e-03,\n",
      "        -5.6314e-03,  9.8946e-04, -1.8827e-03,  6.1851e-03, -1.8539e-03,\n",
      "         1.4520e-03,  2.5884e-03, -3.0996e-03, -4.6189e-03,  5.4921e-03,\n",
      "        -2.1653e-03, -3.0959e-03,  6.4078e-03,  2.4730e-03,  2.1793e-03,\n",
      "        -5.8313e-03,  1.6751e-03, -4.0926e-03, -3.3207e-03,  3.0704e-03,\n",
      "        -1.3178e-03,  8.4156e-04, -3.7869e-03, -2.7736e-03, -3.0707e-03,\n",
      "        -3.4612e-03, -2.6592e-03, -1.2219e-03, -1.8800e-03, -5.8258e-03,\n",
      "         1.6675e-03,  9.6604e-05, -2.3789e-04,  5.8216e-03,  4.6442e-03,\n",
      "         4.9222e-04, -4.8256e-03,  1.2416e-03,  6.1658e-03, -2.2943e-03,\n",
      "         6.0301e-03, -2.9067e-03,  2.0923e-03, -4.9000e-03, -2.8104e-03,\n",
      "         1.0841e-03,  7.7818e-04,  6.7322e-04,  2.0862e-03,  5.9048e-03,\n",
      "        -3.3687e-04,  2.3937e-03,  1.2344e-03, -1.3708e-03,  5.9175e-03,\n",
      "         1.9843e-03, -3.8462e-04, -2.9576e-03,  1.7834e-03,  1.2192e-03,\n",
      "        -5.7165e-03,  5.3106e-03, -4.2652e-03, -9.9441e-04, -2.1055e-03,\n",
      "         1.0489e-03, -4.1209e-03,  2.1516e-03, -5.5514e-03, -1.0867e-03,\n",
      "        -4.8921e-03, -3.8829e-03, -5.0149e-03,  1.7113e-03, -1.1598e-03,\n",
      "         1.2449e-03,  3.4007e-03,  5.5979e-03, -5.5264e-03,  6.1652e-03,\n",
      "         3.5242e-03,  2.0781e-03, -5.1022e-03, -2.6737e-03,  2.7947e-03,\n",
      "        -3.0753e-03,  3.1170e-03,  5.5620e-03,  2.5381e-03, -5.4948e-03,\n",
      "        -4.5281e-03,  6.0900e-03, -2.2300e-03, -1.3520e-03,  3.3402e-03,\n",
      "         3.8726e-03, -1.4539e-03, -4.4121e-03, -3.3339e-03,  4.7458e-03,\n",
      "         2.7205e-03,  5.4300e-03,  2.0850e-04,  3.0226e-03,  3.6497e-03,\n",
      "        -4.2434e-03, -5.6218e-03,  8.6922e-04,  5.5693e-03,  4.0920e-03,\n",
      "         5.3869e-03, -2.7843e-03,  1.2248e-03, -6.3659e-03, -5.3460e-03,\n",
      "         4.6885e-03,  3.4316e-03, -2.1120e-03, -1.7709e-03, -1.2681e-03,\n",
      "        -6.3703e-03, -4.3890e-05,  3.5387e-03,  4.6180e-03,  1.1968e-03,\n",
      "        -5.5816e-03, -3.0275e-03,  4.3258e-03, -2.2178e-03,  2.8219e-03,\n",
      "         2.7398e-03, -1.2458e-03,  1.9058e-03,  2.7223e-03,  1.2239e-03,\n",
      "         3.9218e-03, -3.7711e-03, -2.3894e-03,  6.2449e-03,  3.8493e-03,\n",
      "        -1.4100e-03,  6.1742e-03,  2.9490e-03,  2.9158e-03, -1.3105e-03,\n",
      "         1.5664e-03,  1.7886e-03, -1.0366e-03, -6.1223e-03, -2.0347e-03,\n",
      "         5.7337e-03,  1.0177e-03, -3.9528e-04, -4.0424e-03, -5.1047e-03,\n",
      "        -5.7580e-03, -2.5896e-03,  3.6162e-03, -1.0860e-03, -1.1387e-03,\n",
      "         5.8834e-03,  9.8693e-04, -3.7261e-03, -2.2038e-03, -4.2897e-03,\n",
      "         1.3379e-03, -4.1731e-03, -5.7154e-03,  4.7050e-03, -3.7786e-03,\n",
      "        -1.6792e-03,  1.4926e-04, -6.9148e-04,  6.2308e-03, -4.0049e-03,\n",
      "        -5.6881e-03, -5.3302e-03,  3.2182e-03, -2.9721e-03,  5.9045e-03,\n",
      "        -1.1559e-05, -3.2242e-03,  2.1446e-03,  4.8874e-03,  4.4540e-03,\n",
      "        -1.4166e-03, -8.2739e-04, -2.2325e-03,  5.7780e-03, -4.9370e-03,\n",
      "        -2.2669e-03,  7.5856e-04,  2.5511e-03,  2.9628e-03, -1.5314e-03,\n",
      "         5.2064e-03,  5.5408e-03,  3.2733e-03, -6.4970e-04,  2.8652e-03,\n",
      "         2.2885e-03,  5.0376e-03, -2.7422e-03,  9.3825e-04, -1.9948e-03,\n",
      "        -2.6698e-03, -1.4356e-03,  6.2224e-03,  1.2028e-03, -2.2614e-03,\n",
      "        -3.9637e-03, -5.6633e-03, -5.7199e-03,  3.8082e-03, -5.7558e-03,\n",
      "         4.4515e-03, -1.6305e-04,  1.5780e-05, -2.9437e-03,  5.1840e-04,\n",
      "        -2.6497e-03,  5.0515e-03, -2.6577e-03, -1.6133e-03, -4.8246e-03,\n",
      "         8.8071e-04,  3.4272e-03, -2.6340e-03, -5.8872e-03,  3.1844e-03,\n",
      "         5.7113e-03, -3.8969e-03, -3.8747e-03,  2.7563e-03,  5.5210e-03,\n",
      "         4.8404e-03, -1.0966e-03, -5.1004e-03, -2.8244e-03,  2.9958e-03,\n",
      "         2.8793e-04,  5.6447e-03,  1.0432e-03,  5.0567e-03,  1.5128e-03,\n",
      "         6.0594e-03,  3.3629e-03,  1.1259e-03,  6.3167e-03, -1.6989e-03,\n",
      "        -4.8369e-03,  5.7574e-03,  5.7813e-03, -6.1010e-03,  1.8040e-03,\n",
      "         5.2765e-04, -2.6945e-03, -3.9109e-03,  3.7212e-03,  3.8755e-03,\n",
      "        -6.2389e-03, -1.4974e-03, -5.7777e-03, -3.0406e-03,  1.2631e-03,\n",
      "         1.8532e-04, -1.0799e-03,  5.1841e-03,  6.6250e-04,  2.1069e-03,\n",
      "         5.9349e-03,  2.2526e-05, -5.7535e-03, -2.0240e-03,  4.1569e-03,\n",
      "         1.6033e-03, -2.6792e-03, -8.0599e-04,  5.5070e-03, -2.4716e-03,\n",
      "        -5.0796e-03,  2.4021e-03, -4.8427e-03,  9.8633e-04,  5.4856e-03,\n",
      "         1.9274e-03,  1.1486e-04,  2.1207e-03,  1.7327e-03,  2.6787e-04,\n",
      "         2.0322e-03, -5.3107e-03,  2.6943e-03, -2.3830e-03,  9.3959e-04,\n",
      "         7.4473e-04,  1.2706e-03, -3.3773e-03, -2.3262e-03, -3.8883e-03,\n",
      "        -1.5871e-03, -5.2412e-03, -5.3673e-03,  5.2739e-04, -5.9712e-04,\n",
      "        -1.9524e-04,  4.4472e-03, -2.7623e-03,  4.3858e-03,  6.2026e-03,\n",
      "        -5.4245e-03,  5.1480e-05])), ('_sequence.2.weight', tensor([[-0.0072, -0.0022, -0.0133,  ...,  0.0100, -0.0259,  0.0054],\n",
      "        [-0.0125,  0.0161, -0.0279,  ...,  0.0060,  0.0235, -0.0059],\n",
      "        [-0.0283, -0.0093, -0.0297,  ...,  0.0307, -0.0006,  0.0195],\n",
      "        ...,\n",
      "        [ 0.0045, -0.0041,  0.0177,  ..., -0.0304,  0.0270,  0.0182],\n",
      "        [-0.0241, -0.0035, -0.0195,  ..., -0.0100, -0.0145, -0.0276],\n",
      "        [-0.0173, -0.0063,  0.0273,  ...,  0.0270,  0.0110, -0.0240]])), ('_sequence.2.bias', tensor([ 0.0060, -0.0175,  0.0229,  0.0033, -0.0214, -0.0204,  0.0164,  0.0151,\n",
      "         0.0282,  0.0121, -0.0307,  0.0117,  0.0197, -0.0074,  0.0030, -0.0188,\n",
      "        -0.0165, -0.0185,  0.0003, -0.0085,  0.0304,  0.0167, -0.0048,  0.0246,\n",
      "         0.0298,  0.0185,  0.0191, -0.0051,  0.0191,  0.0154, -0.0229, -0.0094,\n",
      "         0.0289,  0.0063, -0.0107,  0.0029, -0.0030,  0.0090, -0.0162,  0.0077,\n",
      "         0.0140,  0.0142, -0.0195, -0.0093, -0.0303, -0.0279,  0.0207,  0.0300,\n",
      "        -0.0102, -0.0248,  0.0096,  0.0287,  0.0235, -0.0286, -0.0130,  0.0305,\n",
      "        -0.0008,  0.0141, -0.0116,  0.0107, -0.0011, -0.0292, -0.0178,  0.0061,\n",
      "         0.0128, -0.0313, -0.0045,  0.0054, -0.0254, -0.0142, -0.0220,  0.0186,\n",
      "        -0.0290, -0.0118, -0.0289,  0.0247,  0.0095,  0.0063,  0.0063, -0.0275,\n",
      "        -0.0005, -0.0164,  0.0052, -0.0058, -0.0154, -0.0183, -0.0012,  0.0303,\n",
      "        -0.0163,  0.0054, -0.0104,  0.0035, -0.0156, -0.0312, -0.0131, -0.0201,\n",
      "        -0.0179, -0.0166,  0.0034,  0.0212, -0.0242, -0.0308, -0.0305,  0.0103,\n",
      "        -0.0007,  0.0193,  0.0156, -0.0170, -0.0072, -0.0276, -0.0310, -0.0065,\n",
      "         0.0122,  0.0004, -0.0062,  0.0118,  0.0217, -0.0119,  0.0272, -0.0069,\n",
      "         0.0314,  0.0214, -0.0277, -0.0189,  0.0104, -0.0061,  0.0316,  0.0298,\n",
      "         0.0127,  0.0178,  0.0230,  0.0054, -0.0269,  0.0170,  0.0089,  0.0166,\n",
      "        -0.0002, -0.0278, -0.0175,  0.0291, -0.0180, -0.0123, -0.0164, -0.0312,\n",
      "         0.0260,  0.0255,  0.0240, -0.0223,  0.0154,  0.0315,  0.0229, -0.0299,\n",
      "        -0.0148, -0.0189,  0.0311, -0.0001,  0.0096, -0.0084,  0.0234, -0.0305,\n",
      "         0.0290, -0.0024,  0.0129, -0.0106, -0.0165, -0.0043, -0.0303, -0.0315,\n",
      "         0.0299,  0.0108, -0.0144, -0.0242,  0.0172,  0.0141,  0.0013,  0.0294,\n",
      "         0.0187,  0.0168,  0.0163,  0.0017, -0.0069, -0.0182,  0.0092, -0.0134,\n",
      "        -0.0298,  0.0159,  0.0005,  0.0283,  0.0292,  0.0106,  0.0191, -0.0063,\n",
      "        -0.0250,  0.0082, -0.0290, -0.0201, -0.0144, -0.0124, -0.0253, -0.0078,\n",
      "        -0.0067, -0.0254,  0.0140,  0.0133, -0.0086,  0.0125,  0.0296, -0.0147,\n",
      "        -0.0279, -0.0032, -0.0277,  0.0243,  0.0200,  0.0137,  0.0216,  0.0113,\n",
      "         0.0063,  0.0200,  0.0080, -0.0069,  0.0093, -0.0246, -0.0287,  0.0227,\n",
      "        -0.0307,  0.0176, -0.0089,  0.0277, -0.0187, -0.0117, -0.0162, -0.0285,\n",
      "        -0.0249, -0.0032,  0.0106, -0.0237,  0.0008, -0.0228,  0.0076,  0.0161,\n",
      "        -0.0201, -0.0161, -0.0111, -0.0271,  0.0249, -0.0074, -0.0290,  0.0219,\n",
      "         0.0124, -0.0239,  0.0165,  0.0200,  0.0276, -0.0172,  0.0108,  0.0163,\n",
      "         0.0145, -0.0207,  0.0207,  0.0182, -0.0107,  0.0163, -0.0038, -0.0279,\n",
      "        -0.0058,  0.0315, -0.0086, -0.0279, -0.0044, -0.0306,  0.0264, -0.0023,\n",
      "         0.0128, -0.0035, -0.0189,  0.0204, -0.0190, -0.0054, -0.0002,  0.0039,\n",
      "         0.0260,  0.0238,  0.0134,  0.0258, -0.0001, -0.0319, -0.0166,  0.0110,\n",
      "         0.0264, -0.0207, -0.0284,  0.0139,  0.0282,  0.0012, -0.0020, -0.0011,\n",
      "         0.0293, -0.0295, -0.0183,  0.0238, -0.0292,  0.0081, -0.0107,  0.0136,\n",
      "        -0.0227,  0.0216,  0.0057,  0.0271, -0.0309,  0.0071,  0.0311, -0.0022,\n",
      "         0.0019,  0.0116, -0.0199, -0.0230,  0.0081,  0.0153, -0.0092,  0.0032,\n",
      "        -0.0256, -0.0189,  0.0025, -0.0013, -0.0211, -0.0233,  0.0162,  0.0127,\n",
      "        -0.0206,  0.0191, -0.0183, -0.0191,  0.0236, -0.0037, -0.0143,  0.0250,\n",
      "         0.0105,  0.0290, -0.0225,  0.0025,  0.0192, -0.0017,  0.0128,  0.0252,\n",
      "        -0.0279,  0.0192,  0.0073, -0.0137, -0.0273, -0.0179,  0.0080,  0.0012,\n",
      "         0.0317, -0.0202, -0.0123, -0.0311,  0.0191, -0.0031, -0.0095, -0.0190,\n",
      "         0.0040, -0.0060,  0.0196, -0.0084,  0.0211, -0.0230,  0.0234,  0.0039,\n",
      "         0.0235, -0.0155, -0.0135,  0.0005, -0.0108,  0.0208,  0.0027,  0.0318,\n",
      "         0.0156,  0.0117,  0.0257,  0.0256,  0.0248,  0.0131,  0.0210, -0.0163,\n",
      "         0.0120, -0.0102, -0.0168,  0.0229, -0.0313,  0.0212, -0.0035, -0.0151,\n",
      "         0.0120, -0.0166, -0.0057, -0.0186, -0.0148, -0.0021, -0.0202, -0.0125,\n",
      "        -0.0212,  0.0308, -0.0101,  0.0111, -0.0206,  0.0181, -0.0016,  0.0216,\n",
      "         0.0227, -0.0165,  0.0099, -0.0051, -0.0056,  0.0108,  0.0070,  0.0102,\n",
      "         0.0167,  0.0212,  0.0294, -0.0013, -0.0210, -0.0016, -0.0238,  0.0050,\n",
      "         0.0219,  0.0083,  0.0003, -0.0070, -0.0066,  0.0307, -0.0024,  0.0213,\n",
      "        -0.0315,  0.0140, -0.0239,  0.0119, -0.0016,  0.0052,  0.0255,  0.0301,\n",
      "         0.0266, -0.0138,  0.0129,  0.0286, -0.0277, -0.0114,  0.0283, -0.0094,\n",
      "        -0.0062,  0.0048, -0.0248, -0.0319, -0.0221, -0.0129,  0.0154, -0.0229,\n",
      "        -0.0144, -0.0190, -0.0054,  0.0153,  0.0220, -0.0062, -0.0226, -0.0119,\n",
      "        -0.0129, -0.0093, -0.0194, -0.0175,  0.0035, -0.0003, -0.0102,  0.0026,\n",
      "         0.0238, -0.0071, -0.0025,  0.0233, -0.0186, -0.0047, -0.0083,  0.0037,\n",
      "        -0.0120,  0.0265,  0.0224,  0.0052, -0.0264, -0.0176])), ('_sequence.4.weight', tensor([[ 0.0064,  0.0413,  0.0349,  ..., -0.0215,  0.0222,  0.0183],\n",
      "        [ 0.0220,  0.0216, -0.0039,  ...,  0.0339, -0.0278, -0.0030],\n",
      "        [ 0.0258, -0.0398, -0.0177,  ..., -0.0300,  0.0413,  0.0205],\n",
      "        ...,\n",
      "        [-0.0385, -0.0302,  0.0034,  ..., -0.0164, -0.0226,  0.0140],\n",
      "        [ 0.0374, -0.0073, -0.0319,  ...,  0.0077,  0.0107,  0.0292],\n",
      "        [-0.0437, -0.0376, -0.0325,  ..., -0.0174, -0.0378,  0.0279]])), ('_sequence.4.bias', tensor([ 0.0090,  0.0196, -0.0024, -0.0425, -0.0117, -0.0286,  0.0313,  0.0113,\n",
      "         0.0399,  0.0420,  0.0075,  0.0191,  0.0284,  0.0154, -0.0389,  0.0189,\n",
      "        -0.0293,  0.0197, -0.0067,  0.0296,  0.0010,  0.0062, -0.0387,  0.0043,\n",
      "         0.0341,  0.0156, -0.0310, -0.0057,  0.0395, -0.0054,  0.0405,  0.0231,\n",
      "        -0.0069, -0.0339, -0.0335,  0.0399,  0.0449,  0.0117, -0.0419,  0.0156,\n",
      "        -0.0263, -0.0272,  0.0226, -0.0145, -0.0435, -0.0043, -0.0322, -0.0338,\n",
      "         0.0309,  0.0311,  0.0429,  0.0255, -0.0358,  0.0042, -0.0451, -0.0156,\n",
      "        -0.0416,  0.0202,  0.0405, -0.0404,  0.0285,  0.0383,  0.0382, -0.0342,\n",
      "         0.0302,  0.0225,  0.0406, -0.0301,  0.0171,  0.0284, -0.0268, -0.0404,\n",
      "         0.0376, -0.0319,  0.0046, -0.0311, -0.0195, -0.0269, -0.0284, -0.0105,\n",
      "         0.0173,  0.0170, -0.0119, -0.0201, -0.0260,  0.0176,  0.0229, -0.0389,\n",
      "         0.0325, -0.0381, -0.0338,  0.0244, -0.0072, -0.0055,  0.0222, -0.0051,\n",
      "         0.0402,  0.0213, -0.0289, -0.0025, -0.0418, -0.0206,  0.0192,  0.0335,\n",
      "        -0.0387,  0.0127,  0.0411, -0.0428, -0.0130, -0.0423,  0.0220,  0.0108,\n",
      "         0.0390, -0.0174, -0.0429,  0.0093,  0.0017, -0.0166, -0.0372, -0.0151,\n",
      "        -0.0061, -0.0259,  0.0402, -0.0305, -0.0047, -0.0452,  0.0079,  0.0352,\n",
      "        -0.0265,  0.0449,  0.0095, -0.0334, -0.0287,  0.0144, -0.0143, -0.0214,\n",
      "         0.0311,  0.0190,  0.0389, -0.0065, -0.0404, -0.0348, -0.0292,  0.0277,\n",
      "         0.0023, -0.0005,  0.0054,  0.0063,  0.0389, -0.0384,  0.0115, -0.0215,\n",
      "        -0.0361, -0.0092, -0.0325,  0.0319, -0.0239, -0.0356, -0.0254, -0.0440])), ('_sequence.6.weight', tensor([[ 0.0517,  0.0339, -0.0109,  ...,  0.0628,  0.0335, -0.0726],\n",
      "        [ 0.0691, -0.0581,  0.0517,  ..., -0.0063,  0.0324,  0.0196],\n",
      "        [ 0.0643, -0.0429,  0.0160,  ..., -0.0498, -0.0246, -0.0589],\n",
      "        ...,\n",
      "        [ 0.0030, -0.0464,  0.0638,  ...,  0.0769, -0.0323,  0.0474],\n",
      "        [ 0.0207, -0.0161,  0.0640,  ...,  0.0608,  0.0402, -0.0685],\n",
      "        [-0.0560, -0.0337,  0.0080,  ..., -0.0126,  0.0785,  0.0654]])), ('_sequence.6.bias', tensor([ 0.0641, -0.0271, -0.0292,  0.0213, -0.0536,  0.0321, -0.0097, -0.0516,\n",
      "         0.0625,  0.0703, -0.0612,  0.0572,  0.0625,  0.0127, -0.0384,  0.0269,\n",
      "         0.0221, -0.0114, -0.0008, -0.0575, -0.0050, -0.0676, -0.0608,  0.0540,\n",
      "        -0.0666,  0.0469,  0.0037,  0.0055, -0.0032, -0.0013,  0.0478,  0.0601,\n",
      "        -0.0237, -0.0542, -0.0096, -0.0522, -0.0562, -0.0173, -0.0706,  0.0579])), ('_sequence.8.weight', tensor([[-0.1457,  0.0772,  0.1461,  0.1523, -0.0341,  0.0321,  0.1276,  0.0378,\n",
      "         -0.0086, -0.1424,  0.0380, -0.1423,  0.0511, -0.1232, -0.0596, -0.0999,\n",
      "          0.0641,  0.1497,  0.0201, -0.1369,  0.1144, -0.0080, -0.1519, -0.1080,\n",
      "         -0.0917,  0.0685,  0.1083,  0.0148, -0.1308,  0.0584,  0.0419, -0.0837,\n",
      "         -0.1306, -0.1079,  0.1104,  0.0067,  0.0081,  0.0778,  0.0743, -0.0933],\n",
      "        [ 0.0397, -0.0053, -0.1255,  0.0008, -0.0579, -0.0430, -0.0705,  0.0547,\n",
      "         -0.1555,  0.0104,  0.0850, -0.0539, -0.0913,  0.0567, -0.0092,  0.0370,\n",
      "          0.0844,  0.0034,  0.1456,  0.1436,  0.0294,  0.0450, -0.0791, -0.1262,\n",
      "          0.0792, -0.0435, -0.0679,  0.1105,  0.0370, -0.1335,  0.0669, -0.1193,\n",
      "          0.0500,  0.1385, -0.1402, -0.0701, -0.0061,  0.0596,  0.0072,  0.0526],\n",
      "        [ 0.1334,  0.1542, -0.1441, -0.0342,  0.0126, -0.0232, -0.0660, -0.0189,\n",
      "          0.0569,  0.0949, -0.1370, -0.0597, -0.0524,  0.0902,  0.1434,  0.1489,\n",
      "         -0.0285, -0.1261, -0.0494,  0.1104, -0.0646,  0.0180,  0.0553, -0.0127,\n",
      "          0.1279, -0.1158, -0.0123,  0.0241,  0.0679, -0.0829, -0.0389, -0.0452,\n",
      "          0.0995,  0.0869,  0.0619, -0.0963, -0.0475, -0.1185,  0.0312,  0.1573]])), ('_sequence.8.bias', tensor([ 0.0305, -0.0870, -0.0818]))])\n"
     ]
    }
   ],
   "source": [
    "print(model_MLPSEQ.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the parameters look similar but have different names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's make a dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced size: 100000\n"
     ]
    }
   ],
   "source": [
    "dset=WCH5Dataset(\"/scratch/fcormier/Public/NUPRISM.h5\",reduced_dataset_size=100000,val_split=0.1,test_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a dataloader and grab a first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "train_dldr=DataLoader(dset,\n",
    "                      batch_size=32,\n",
    "                      shuffle=False,\n",
    "                      sampler=SubsetRandomSampler(dset.train_indices))\n",
    "train_iter=iter(train_dldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch0=next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=batch0[0]\n",
    "labels=batch0[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the model output on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out=model_MLP(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 0, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 0, 0, 1, 0, 2, 2,\n",
      "        2, 2, 1, 0, 1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.2698e-01, 7.9713e-02, 4.9330e-01],\n",
      "        [5.1634e-04, 1.2869e-04, 9.9936e-01],\n",
      "        [6.3779e-06, 4.5251e-05, 9.9995e-01],\n",
      "        [3.4432e-04, 9.9307e-01, 6.5846e-03],\n",
      "        [2.0709e-07, 5.1921e-04, 9.9948e-01],\n",
      "        [3.3927e-04, 8.6314e-03, 9.9103e-01],\n",
      "        [2.5257e-03, 2.1832e-03, 9.9529e-01],\n",
      "        [3.0196e-09, 1.1320e-06, 1.0000e+00],\n",
      "        [5.5010e-06, 1.0070e-06, 9.9999e-01],\n",
      "        [7.8259e-05, 1.3893e-01, 8.6100e-01],\n",
      "        [4.5265e-08, 2.1939e-05, 9.9998e-01],\n",
      "        [9.8275e-01, 1.6069e-05, 1.7234e-02],\n",
      "        [1.1381e-02, 1.0933e-01, 8.7929e-01],\n",
      "        [2.9443e-03, 1.3460e-02, 9.8360e-01],\n",
      "        [2.6970e-07, 3.2683e-06, 1.0000e+00],\n",
      "        [3.6976e-06, 3.2520e-05, 9.9996e-01],\n",
      "        [1.3955e-03, 8.9605e-02, 9.0900e-01],\n",
      "        [1.8714e-03, 3.8200e-04, 9.9775e-01],\n",
      "        [2.6542e-06, 1.6722e-04, 9.9983e-01],\n",
      "        [2.8492e-04, 7.7103e-04, 9.9894e-01],\n",
      "        [3.7364e-01, 4.3748e-02, 5.8261e-01],\n",
      "        [3.5151e-06, 1.5342e-04, 9.9984e-01],\n",
      "        [2.1230e-02, 5.8586e-04, 9.7818e-01],\n",
      "        [2.5385e-04, 1.4785e-06, 9.9974e-01],\n",
      "        [7.5189e-11, 3.2633e-08, 1.0000e+00],\n",
      "        [3.3026e-08, 7.8694e-05, 9.9992e-01],\n",
      "        [2.0692e-05, 5.1363e-01, 4.8635e-01],\n",
      "        [4.8335e-06, 1.4792e-02, 9.8520e-01],\n",
      "        [1.3873e-04, 7.0514e-01, 2.9472e-01],\n",
      "        [5.7507e-07, 1.8954e-04, 9.9981e-01],\n",
      "        [2.0964e-07, 9.5171e-03, 9.9048e-01],\n",
      "        [2.6943e-03, 2.4434e-01, 7.5296e-01]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have model's predictions and we above got 'true' labels from the dataset, so we can now compute the loss - CrossEntropyLoss is the apropropriate one to use here. We will use `CrossEntropyLoss` from `torch.nn` - btw it is also a `Module`. First create it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "loss_module=CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_tensor=loss_module(model_out,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1145, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a 'forward pass'. We should now have a computational graph available - let's plot it for the kicks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can't get torchivz in compute canada\n",
    "#from torchviz import make_dot\n",
    "#make_dot(loss_tensor,params=dict(model_MLP.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we calculate the gradients - let's check what they are now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of a parameter: fc1.weight, gradient: None\n",
      "name of a parameter: fc1.bias, gradient: None\n",
      "name of a parameter: fc2.weight, gradient: None\n",
      "name of a parameter: fc2.bias, gradient: None\n",
      "name of a parameter: fc3.weight, gradient: None\n",
      "name of a parameter: fc3.bias, gradient: None\n",
      "name of a parameter: fc4.weight, gradient: None\n",
      "name of a parameter: fc4.bias, gradient: None\n",
      "name of a parameter: fc5.weight, gradient: None\n",
      "name of a parameter: fc5.bias, gradient: None\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_MLP.named_parameters():\n",
    "    print(\"name of a parameter: {}, gradient: {}\".\n",
    "          format(name, param.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No wonder - let's calculate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tensor.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of a parameter: fc1.weight, gradient: tensor([[ 0.0000e+00,  6.3469e-08, -2.4750e+01,  ...,  0.0000e+00,\n",
      "          2.7509e-04,  2.0785e+04],\n",
      "        [ 0.0000e+00,  0.0000e+00, -1.9043e+01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -8.7695e-08, -2.1030e+01,  ...,  3.3834e+03,\n",
      "          0.0000e+00,  6.9443e+03],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.7934e+00,  0.0000e+00,  ..., -3.2453e-04,\n",
      "          1.8825e+05,  0.0000e+00],\n",
      "        [ 0.0000e+00,  5.1661e+01, -2.4404e+01,  ...,  2.2799e-04,\n",
      "         -9.7513e+04, -2.5372e+04],\n",
      "        [ 0.0000e+00,  1.0332e+02, -1.1783e-05,  ..., -2.2310e-04,\n",
      "          4.2236e+04,  0.0000e+00]])\n",
      "name of a parameter: fc1.bias, gradient: tensor([-8.5976e+00,  8.9086e+00, -4.2629e+00, -3.5771e+01,  2.1132e+02,\n",
      "         2.2888e+01, -3.9025e+01,  6.4370e+01,  9.5124e+01, -5.0377e+01,\n",
      "        -6.0272e+01, -1.5987e+02, -2.0071e+01, -1.1248e+02, -2.2747e-01,\n",
      "        -1.2991e+02, -7.7232e+01, -8.8527e+00, -2.0095e+01,  8.4810e+01,\n",
      "         2.0921e+02,  1.2560e+02, -1.4099e+02, -1.0620e+02, -1.8091e+02,\n",
      "         9.8696e+01,  9.1679e+00,  4.3944e+01,  1.9002e+02, -5.1388e+01,\n",
      "        -4.2088e+01,  4.8432e+00,  8.3239e+01,  3.7092e+01, -1.2137e+02,\n",
      "        -7.1662e+01, -1.7344e+01, -2.2975e+02,  3.2817e+01,  1.1786e+02,\n",
      "         1.5951e+02,  1.7881e+02,  1.2848e+01,  8.1633e+00, -9.9299e+01,\n",
      "         3.3109e+00,  1.5459e+02, -1.0571e+02, -1.1146e+02, -1.7959e+02,\n",
      "        -6.9798e+01, -1.9764e+02, -7.3229e+01,  9.9757e+01, -9.5774e+01,\n",
      "        -7.8888e-03,  4.6689e+01,  1.2625e+02,  2.7393e+02,  2.0757e+01,\n",
      "        -8.1751e+01,  3.7191e+01, -1.7076e+02, -4.0917e+01, -6.6917e+01,\n",
      "        -9.3148e+01, -1.2748e+01,  1.1812e+02, -8.3970e+01, -4.1919e+01,\n",
      "         1.0602e+02, -7.6264e+01, -1.2621e+02,  5.7220e+01,  7.4875e+01,\n",
      "        -1.0453e+01, -1.3999e+02,  1.0055e+01, -5.0375e+01, -5.5315e+01,\n",
      "        -2.1670e+02, -1.0047e+02,  9.7468e+01, -9.9538e+01,  1.7527e+02,\n",
      "        -7.6555e+01,  2.3996e+00, -6.9902e+01,  4.4270e+00, -4.0974e+01,\n",
      "        -1.9317e+02,  9.6916e+01, -8.1555e+01, -1.0181e+02,  6.5968e+01,\n",
      "        -8.3306e+01, -6.5225e+01, -5.8098e+01,  7.5199e+01,  1.8181e+02,\n",
      "        -5.0004e+01,  8.2724e+01,  4.8354e+00, -1.8093e+01, -5.1028e+01,\n",
      "         4.5854e+01,  9.1388e-01, -1.1830e+02,  7.1976e+01,  6.9936e+00,\n",
      "         1.0390e+02,  1.1377e+02, -7.7744e+01, -4.7398e+01,  7.4027e+01,\n",
      "        -9.5139e-03,  4.6336e+01, -3.6495e+01, -1.0614e+02, -1.1964e+02,\n",
      "         1.0756e+02, -1.0639e+02, -1.5238e+02, -4.2258e+01, -1.2769e+02,\n",
      "        -5.1586e+01,  1.4764e+02, -5.1501e+01, -1.4665e+02, -1.2732e+02,\n",
      "        -3.0644e+01, -3.5704e+01, -1.4228e+02, -8.3704e+01, -1.5730e+02,\n",
      "        -6.4241e+00,  2.6225e+01, -4.4428e+01,  6.7155e+01,  4.4573e+01,\n",
      "         8.1881e+01,  1.4126e+02, -1.3720e+01, -1.7624e+02, -1.0143e+02,\n",
      "        -5.6872e+01,  9.7860e+01, -1.2569e+02, -1.0552e+01, -1.3905e+01,\n",
      "        -3.6758e+01,  2.0981e+02, -1.1445e+02,  2.5695e+01, -8.7989e+01,\n",
      "        -2.2718e+02, -1.2524e+02, -1.9596e+02,  7.9691e+01, -7.1114e+01,\n",
      "        -6.7613e+00, -1.3694e+02, -9.4262e+01,  2.2703e+02,  3.2075e+01,\n",
      "        -1.0390e+02, -5.7717e+01,  2.3580e+01,  1.3299e+01,  1.6966e+01,\n",
      "        -3.5659e+01, -3.4843e+01, -5.8204e+00,  1.2389e+01, -1.6290e+01,\n",
      "        -1.9060e+01, -6.9129e+01,  1.0801e+02,  1.9946e+02, -3.7466e+00,\n",
      "        -3.1434e+01,  6.8390e+01,  2.6753e+01, -3.8213e+01, -2.8709e+01,\n",
      "         9.3954e+01, -1.0041e+02,  2.5522e+01,  7.9633e+00, -2.7283e+01,\n",
      "        -9.1877e+01, -1.2253e+00,  2.0977e+01, -2.8936e+01,  2.1411e+01,\n",
      "         2.6370e+02,  9.1322e+01,  1.4865e+01,  2.1715e+02,  1.5567e+02,\n",
      "        -5.1739e+01,  1.2055e+01, -1.0269e+02, -2.2548e+01, -4.9547e+01,\n",
      "         1.1317e+02, -6.6441e+01, -6.3655e+01, -3.4060e+01, -4.9446e+01,\n",
      "         4.9874e+01, -9.6398e+01, -2.0424e+02,  7.7742e+01, -8.5844e+01,\n",
      "        -1.5477e+02, -1.7697e+02, -1.0439e+02, -6.6758e+01, -2.3074e+01,\n",
      "        -1.5544e+02,  9.4860e+00, -3.9906e+01,  2.6964e+01,  6.9459e+00,\n",
      "        -1.9917e+02,  1.7026e+02,  5.8457e+01, -9.4544e+01, -5.4756e+01,\n",
      "        -2.7588e+01, -1.0539e+02,  1.6901e+02,  2.2073e+02, -6.3870e+01,\n",
      "        -1.3259e+02,  9.9480e+00, -6.0546e+01,  1.3642e+02, -1.2436e+02,\n",
      "         1.8607e+02, -2.1716e+02, -2.5456e+02, -1.0461e+02,  8.1254e+01,\n",
      "         2.5057e+02, -3.5598e+01, -6.2965e+01, -2.5290e+01,  1.1411e+02,\n",
      "         5.7451e+01, -5.1176e+01,  2.2101e+01,  2.0291e+02, -6.8115e+01,\n",
      "         1.7190e+02,  2.2875e+02, -1.6957e+02, -9.5839e-01, -8.6786e+01,\n",
      "        -4.0324e+01,  3.9689e+01,  1.0303e+02,  1.7811e+01, -1.6838e+02,\n",
      "         1.4545e+02, -6.3107e+01,  3.0929e+01,  6.7421e+01, -7.7325e+01,\n",
      "         1.5126e+01, -3.6868e+01, -5.0416e+01,  1.0860e+02, -1.0672e+02,\n",
      "        -1.2376e+02, -2.4544e+02,  3.2427e+02,  1.6553e+02,  3.0510e+01,\n",
      "        -2.3198e+01, -5.1391e+01, -2.7353e+01, -1.6215e+01,  7.5042e+01,\n",
      "        -2.6884e+01,  1.5857e+02, -1.8636e+02, -1.2388e+02,  1.4191e+02,\n",
      "         3.3874e+01,  9.5293e+01, -1.2415e+02,  9.9241e+00, -1.3486e+02,\n",
      "         6.2546e+01,  2.7074e+01, -6.1381e+01,  2.5410e+01, -6.0181e+01,\n",
      "        -8.6093e+01, -5.3476e+01, -5.3859e+01, -5.1244e+01,  4.4846e+00,\n",
      "         4.0544e+00,  1.9786e+01, -2.7840e+02,  8.5794e-01,  5.6337e+01,\n",
      "        -4.6685e+01,  7.5972e+01, -4.9610e+01, -3.2738e+01,  6.6267e+01,\n",
      "        -1.3075e+02,  4.2474e+01, -3.2202e+01, -8.2982e+01, -1.0709e+01,\n",
      "        -4.1769e+01, -3.2876e+01, -1.6101e+01, -3.2260e+01, -2.5776e+02,\n",
      "         3.3655e+02,  1.0041e+02,  1.7044e+02, -8.3368e+01, -6.4008e+01,\n",
      "        -9.6058e+01,  5.5770e+01, -9.1918e+01,  1.7346e+02, -2.7266e+01,\n",
      "         7.9077e+01,  2.9099e+01, -4.3013e+01, -1.1301e+02, -9.8163e+00,\n",
      "        -3.5563e+01, -3.1807e+01,  7.4848e+01, -1.9402e-01, -6.0951e+00,\n",
      "         3.5283e+01,  5.6936e+01, -4.0948e+00, -1.2321e+02,  3.8933e+01,\n",
      "        -5.8257e+01,  3.4536e+01,  2.0505e+02,  8.2884e+00,  1.5755e+02,\n",
      "         6.9009e+01,  9.2214e+01, -3.7508e+01, -1.1221e+02,  1.8528e+02,\n",
      "        -2.6552e+02,  8.8181e+01,  2.0827e+02,  9.7090e+00, -3.7686e+01,\n",
      "         3.6310e+01,  3.2711e+01,  1.7906e+02,  6.2308e+01, -1.3675e+02,\n",
      "        -8.3253e+01, -1.7152e+02,  1.0970e+01, -2.7823e+02, -5.0407e+01,\n",
      "         4.4513e+02, -2.4433e+02,  7.5812e+01,  1.3978e+02,  6.4383e+00,\n",
      "         4.8659e+01,  4.6749e+01, -2.0012e+02,  1.5050e+02, -3.8318e+01,\n",
      "        -6.6573e+01, -1.6424e+02,  3.7245e+01,  5.1605e+01,  1.0397e+02,\n",
      "         7.4585e+01,  2.7660e+01, -7.6739e+01, -1.2892e+01, -2.7538e+01,\n",
      "        -1.5288e+02, -1.4036e+02,  1.7714e+01,  2.2424e+01,  8.2039e+01,\n",
      "         1.5316e+02,  8.0850e+01, -7.1928e+01, -2.2648e+02, -1.1410e+02,\n",
      "         6.0562e+01,  2.1611e+02, -1.4368e+01, -3.3212e+01, -2.7434e+02,\n",
      "         1.7748e+02,  1.3584e+02,  1.0167e+02,  9.2973e+01,  1.3367e+02,\n",
      "         4.6622e+01,  9.5067e+01, -1.5126e+01,  1.1389e+02,  8.0819e+01,\n",
      "         2.3082e+01,  2.6603e+01, -3.1115e+01,  3.3685e+01, -1.1811e+02,\n",
      "         7.8968e+01,  1.6417e+02, -1.6854e+02,  6.9578e+00, -3.6825e+01,\n",
      "        -3.3439e+01, -1.6057e+02,  1.8845e+02, -3.5055e+02, -1.8127e+02,\n",
      "         9.8687e+01,  5.7748e+01,  2.0421e+02,  1.6696e+02,  3.0122e+01,\n",
      "        -1.1084e+02, -4.1834e+01, -7.6530e+01, -1.5355e+01, -1.1024e+02,\n",
      "         6.8470e+01, -1.8699e+01,  6.3719e+01,  2.6936e+02,  2.1201e+01,\n",
      "        -8.1887e+01,  6.7126e+01, -1.0047e+02,  5.1425e+01, -9.9130e+01,\n",
      "        -8.8819e+01,  1.7931e+00,  1.2754e+01, -1.1650e+02, -7.9626e+00,\n",
      "         2.6785e+02, -3.5674e+00, -1.3148e+02,  2.6754e+01,  2.4972e+02,\n",
      "        -1.5583e+02,  4.9108e+00, -2.6093e+02, -2.6329e+02,  6.7068e+01,\n",
      "         1.1559e+02,  2.1595e+02, -5.1177e+01,  7.4668e+00, -9.3704e+01,\n",
      "        -2.6364e+02, -1.4580e+02,  1.3292e+02, -1.7557e+02, -1.8168e+01,\n",
      "         1.8605e+02, -1.4892e+02, -1.5825e+02, -9.2909e+01,  5.3709e+01,\n",
      "        -2.4570e+02,  1.5086e+02,  1.2161e+02, -1.6282e+02,  1.1514e+02,\n",
      "         6.9143e+01, -2.0034e+02, -8.5718e+01,  2.0131e+01, -4.6655e+01,\n",
      "         5.6687e+01, -4.0852e+01, -4.2330e+01, -2.6950e+02, -3.9373e+02,\n",
      "        -2.1031e+01,  2.8885e+02,  1.1975e+02, -1.1515e+02, -2.1015e+01,\n",
      "         2.5341e+01,  1.9206e+02,  9.0429e+01,  2.6676e+02, -3.0779e+02,\n",
      "        -1.6081e+02, -5.9015e+01,  1.9657e+02, -2.3860e+02, -9.2538e+01,\n",
      "        -2.2829e+02, -7.0596e+01,  2.0683e+02,  2.7921e+02,  2.2108e+02,\n",
      "        -7.8862e+01, -2.0917e+02,  1.2635e+02, -1.6880e+02,  8.7361e+01,\n",
      "        -2.6977e+01,  9.8862e+01, -1.6756e+01, -2.2084e+02, -1.3528e+02,\n",
      "         1.7558e+01, -2.9833e+02,  1.3314e+02, -1.5509e+02,  1.0761e+02,\n",
      "         1.8194e+02,  1.0242e+02, -1.9130e+02, -1.9636e+01, -5.5194e+01,\n",
      "         1.3078e+02, -4.7567e+01, -5.3869e+01, -5.0212e+01, -1.3910e+02,\n",
      "        -4.9069e+01, -8.8079e+00, -9.7098e+01,  2.5357e+02, -2.8757e+02,\n",
      "         2.6800e+01,  1.5691e+02,  9.8664e+01,  1.3749e+02, -1.7731e+01,\n",
      "         2.3928e+02,  4.5112e+01, -1.6290e+01,  1.2100e+02,  8.7654e+01,\n",
      "         9.3779e+01,  4.8299e+01, -9.8359e+01,  1.4525e+02,  8.3862e+00,\n",
      "        -9.1420e+01,  3.2277e+00,  2.5800e+02,  6.6161e+01,  1.8865e+02,\n",
      "        -1.2304e+02,  1.3452e+01,  5.0208e+01,  6.6831e+01, -1.4267e+02,\n",
      "         1.9347e+02, -2.1286e+02,  3.6620e+02,  1.6686e+02,  2.6117e+00,\n",
      "         7.0802e+01, -1.1146e+02,  1.7681e+02,  2.6260e+02,  5.2561e+01,\n",
      "         8.3655e+01, -2.6361e+02, -1.0842e+02,  1.0834e+02, -3.7982e+01,\n",
      "         8.6919e+01, -7.5030e+00, -1.7957e+02, -1.7493e+02, -3.0982e+01,\n",
      "         2.6222e+02, -1.7880e+02, -1.2419e+02,  2.7027e+02,  2.1354e+02,\n",
      "        -8.7661e+01,  4.9280e+01,  1.6951e+02,  5.7408e+00, -2.7555e+02,\n",
      "        -1.1909e+00,  2.5989e+02,  9.2074e+01, -1.4081e+02, -7.9746e+01,\n",
      "        -1.2385e+02, -1.1125e+02, -1.3341e+01, -1.2773e+02, -7.0309e+00,\n",
      "        -6.4721e+01,  2.4707e+02,  1.8264e+02,  2.7497e+01, -1.3340e+02,\n",
      "        -1.4098e+02, -1.1699e+02,  4.5211e+01,  1.3321e+02, -3.6103e+01,\n",
      "         4.0924e+01, -4.2849e+00, -8.4219e+01,  2.0624e+01, -1.7414e+02,\n",
      "        -2.1004e+02,  3.7256e+01,  2.0555e+02, -1.4849e+02, -7.5837e+01,\n",
      "         4.1045e+01,  2.9651e+01,  2.8441e+01, -1.7262e+01,  6.2369e+00,\n",
      "         5.3878e+01,  4.6191e+01,  2.0340e+02,  1.6661e+01,  7.7693e+01,\n",
      "         1.0737e+02, -2.0438e+02, -9.4668e+01,  1.9026e+01,  2.2001e+01,\n",
      "        -2.7529e+02,  3.9073e+00,  3.1653e+00,  1.2094e+02, -3.0624e+02,\n",
      "        -6.8581e+01, -1.1126e+02,  1.4736e+02, -1.2529e+02,  8.8228e+00,\n",
      "         4.0060e+01,  6.1980e+01,  1.9211e+02,  2.0336e+02,  4.8008e+01,\n",
      "        -2.1656e+02,  1.4948e+01, -1.3583e+02,  1.5359e+02, -1.5258e+02,\n",
      "        -2.4685e+02, -1.7328e+01, -7.8036e+00, -4.0351e+01, -1.6060e+02,\n",
      "        -8.2410e+01, -9.1000e+01, -1.1856e+02, -6.0587e+01,  1.9983e+02,\n",
      "        -1.5594e+02, -1.6719e+02, -3.6116e+01,  9.3374e+01, -8.5860e+01,\n",
      "        -5.0071e+01, -7.5559e+01,  2.6566e+02, -7.0745e+01, -2.9435e+02,\n",
      "        -4.9064e+01,  1.0965e+02,  3.8171e+01, -1.2108e+02,  1.0673e+00,\n",
      "         6.4126e+01, -8.1728e+01, -9.5703e+01, -2.1765e+01,  3.6631e+01,\n",
      "        -1.0254e+02,  9.7908e+01, -9.6740e+01, -7.9506e+01,  4.7695e+01,\n",
      "         1.0885e+02,  1.6320e+02,  2.2766e+02, -8.8543e+01, -1.2053e+02,\n",
      "         1.3062e+02, -6.3747e+01,  1.3775e+02,  2.5792e+02,  1.9339e+02,\n",
      "        -8.4067e+01,  2.3521e+02,  1.1244e+02,  1.5208e+02, -7.8960e+01,\n",
      "         3.2773e+01,  1.6906e+00, -1.4191e+02, -1.3015e+02,  1.7264e+02,\n",
      "         4.1019e+01,  3.3382e+02,  1.3617e+02,  1.6450e+02,  4.0545e+02,\n",
      "        -2.9681e+02,  9.3088e+01,  7.2439e+01,  2.9635e+02, -2.8811e+02,\n",
      "         3.3432e+01, -1.0120e+02,  1.6329e+02,  1.5227e+02,  1.2145e+02,\n",
      "         1.0454e+02,  2.2485e+02, -3.3007e+01,  1.0157e+02, -3.2926e+01,\n",
      "        -1.0542e+02, -2.0997e+02, -1.8236e+02,  4.5243e+01,  7.7331e+01,\n",
      "        -3.3318e+02, -1.8750e+02, -1.3951e+02,  5.4491e+01,  1.9984e+02,\n",
      "         1.8469e+02,  3.0818e+02,  7.2451e+01, -8.5292e+01,  2.5547e+02,\n",
      "         1.1849e+02,  2.6855e+02,  2.0657e+02, -1.5438e+02, -2.4893e+02,\n",
      "         6.4905e+00, -6.0902e+02,  1.4086e+02, -2.2394e+02, -1.7724e+02,\n",
      "        -2.7446e+02,  7.8208e+01,  8.3892e+01, -1.7002e+01, -1.8148e+01,\n",
      "        -3.5115e+01,  2.1584e+02,  4.3124e+02, -1.9370e+01, -1.4325e+02,\n",
      "         2.6534e+01, -3.5683e+01, -4.4213e+02,  1.6392e+02, -3.7926e+01,\n",
      "        -1.4067e+02,  3.0665e+01, -1.1030e+02,  3.4891e+02, -1.3849e+02,\n",
      "         5.6924e+01,  1.7060e+02,  2.8343e+02, -1.7312e+02, -2.4206e+02,\n",
      "        -4.5227e+01, -8.2087e+01, -3.6241e+01, -1.3200e+02, -2.5113e+02,\n",
      "        -9.7751e+01, -3.9026e+01, -2.0094e+02, -1.1084e+02,  1.2191e+02,\n",
      "         2.8989e+00,  9.0083e+00,  2.5868e+01,  1.0657e+02,  2.1008e+02,\n",
      "        -2.3297e+02, -1.6619e+02,  1.2754e+02,  9.2737e+01, -1.6090e+02,\n",
      "        -3.4398e+02,  1.6866e+02, -1.1317e+02,  3.1744e+02,  5.9705e+01,\n",
      "         2.0951e+01, -1.2708e+02, -6.4511e+01,  2.4176e+01, -2.2015e+02,\n",
      "         4.2765e+02,  3.3084e+02,  3.5921e+02, -1.2872e+02, -1.4596e+02,\n",
      "         1.7946e+02, -1.1917e+02,  2.5827e+02, -3.3611e+02, -1.3534e+02,\n",
      "        -2.6785e+02, -1.9428e+02, -1.2948e+02, -9.8423e+01,  1.8083e+02,\n",
      "        -1.4242e+02,  2.3084e+02, -5.2852e+01,  1.1953e+02, -2.3660e+02,\n",
      "        -5.1964e+01,  2.2808e+02,  3.7841e+02,  1.2057e+02,  1.2156e+02,\n",
      "         2.8139e+01,  8.7512e+01, -2.2036e+02, -1.1243e+02,  1.2198e+02,\n",
      "        -1.1957e+02,  2.5624e+02,  3.0074e+02, -1.0117e+02,  4.2962e+01,\n",
      "        -2.0289e+02, -5.0682e+01,  9.3481e+01,  2.2806e+02,  4.5314e+01,\n",
      "         6.2761e+01, -3.8374e+01,  1.4331e+02,  5.6306e+01,  1.1755e+00,\n",
      "        -1.3964e+02,  1.4277e+02, -1.0842e+02,  3.4836e+02, -7.3482e+01,\n",
      "         3.8343e+02,  2.8745e+02, -1.4247e+01, -7.1283e+01, -1.0923e+02,\n",
      "         6.7709e+01,  4.4120e+01,  2.0072e+02, -1.1436e+02,  1.0887e+02,\n",
      "         9.1540e+00, -2.0594e+02, -1.2089e+02, -3.6169e-01,  2.4218e+02,\n",
      "         1.4007e+02,  9.8704e+01, -2.7078e+01,  1.8155e+02,  4.4906e+01,\n",
      "         1.9082e+02,  7.4632e+01,  3.6388e+01,  3.9182e+01,  1.6653e+02,\n",
      "        -4.4368e+00, -8.5800e+01,  1.1865e+01,  1.9362e+02, -1.6428e+02,\n",
      "        -5.8330e+01, -1.8221e+02,  4.7735e+02, -3.4766e+01, -2.3021e+01,\n",
      "        -3.3385e+01, -1.7465e+02, -7.3105e+01,  9.8506e+01,  9.2269e+01,\n",
      "        -3.7799e+02, -8.7583e+01, -4.8546e+01, -1.9180e+02,  7.1297e+01,\n",
      "        -1.7297e+02, -4.9580e+01,  1.3777e+02,  1.4171e+02, -1.2961e+02,\n",
      "        -1.9594e+02,  6.0774e+01, -6.9620e+01,  1.2948e+00,  2.0778e+02,\n",
      "        -5.6618e+01,  1.0595e+02,  2.2234e+01, -6.0140e+00,  2.1250e+02,\n",
      "         1.9892e+02,  1.3596e+01, -6.5392e+01,  1.4816e+02,  2.3697e+02,\n",
      "         4.2118e+01, -6.0665e+01, -1.0598e+02,  9.5580e+00, -9.2840e+00,\n",
      "        -7.1921e+01,  1.1393e+01,  1.4343e+01,  5.4449e+01, -2.6454e+01,\n",
      "        -1.7214e+02,  1.9788e+02, -1.8348e+02, -1.2516e+01, -1.6186e+02,\n",
      "        -4.7243e+00, -2.6380e+01, -1.2122e+01,  4.5880e+01,  7.8672e+01,\n",
      "         1.6000e+02,  4.5230e+01,  1.1049e+02, -1.3439e+02,  2.7389e+01,\n",
      "        -3.8095e+02,  1.0287e+02,  1.2485e+02, -8.7232e+01,  2.3620e+02,\n",
      "         3.0289e+01,  4.6569e+02])\n",
      "name of a parameter: fc2.weight, gradient: tensor([[-1.5560e-03, -1.7006e-04,  5.5011e-04,  ..., -3.8556e-03,\n",
      "         -7.0622e-03, -1.5736e-03],\n",
      "        [ 5.1484e-06,  3.8590e-05,  5.3017e-04,  ..., -1.8841e-06,\n",
      "          1.6272e-03,  4.3464e-04],\n",
      "        [-2.0713e-04, -5.6306e-03, -8.3067e-03,  ...,  1.1736e-04,\n",
      "         -2.1531e-02, -5.1448e-04],\n",
      "        ...,\n",
      "        [-4.5823e-03, -3.8518e-03, -2.9799e-02,  ..., -2.0437e-02,\n",
      "         -1.3100e-02, -1.4031e-03],\n",
      "        [-4.2299e-03, -1.6793e-03, -2.6359e-02,  ..., -1.9529e-02,\n",
      "         -6.7513e-03, -7.8870e-05],\n",
      "        [-3.2251e-03, -5.3854e-04, -4.0504e-04,  ..., -3.3710e-03,\n",
      "         -3.8890e-03,  2.1720e-04]])\n",
      "name of a parameter: fc2.bias, gradient: tensor([-8.3859e-05,  9.4614e-06, -1.3359e-04, -1.2015e-05,  4.9923e-05,\n",
      "         1.0299e-06,  8.7360e-05, -8.3372e-06,  4.9541e-06,  1.8070e-05,\n",
      "        -5.2661e-06,  4.8330e-05,  1.2914e-06, -1.2149e-04,  2.6954e-06,\n",
      "         1.3658e-04,  7.6925e-06, -1.8522e-05, -3.1688e-06,  3.7171e-05,\n",
      "        -2.7362e-06,  4.3355e-05, -5.4365e-06,  4.4171e-07,  1.3384e-05,\n",
      "        -1.4066e-05, -2.2709e-05,  3.1359e-05, -8.8086e-06,  1.2258e-05,\n",
      "         1.4009e-05,  5.4435e-06, -7.6636e-06,  5.7811e-07, -1.6228e-05,\n",
      "         3.9003e-05, -8.1030e-05,  1.0773e-06, -2.2866e-07, -1.1478e-04,\n",
      "        -3.2974e-06, -6.9542e-05,  2.9765e-05, -5.0684e-06,  5.2831e-05,\n",
      "         4.0856e-05, -8.0264e-07, -2.9678e-05,  2.1761e-05,  2.5585e-05,\n",
      "         6.0815e-05, -1.3654e-05, -6.9440e-06,  5.2443e-05,  4.1494e-05,\n",
      "         3.9491e-05, -2.6520e-06, -1.8766e-05, -1.9101e-05,  2.0612e-05,\n",
      "        -4.2555e-06,  5.2313e-06, -3.6504e-05, -1.8946e-06,  1.9222e-06,\n",
      "        -3.9721e-06, -1.0587e-04,  4.9414e-05, -2.1583e-06,  2.3356e-05,\n",
      "        -9.6899e-06, -6.5141e-05,  1.7345e-04, -5.5759e-05, -2.1148e-05,\n",
      "         2.2844e-05,  6.2018e-05, -4.2594e-06, -3.2424e-05,  4.3218e-06,\n",
      "         4.2202e-06,  5.8621e-05,  2.4186e-05,  5.3141e-06,  3.8647e-05,\n",
      "        -1.7987e-05, -2.2473e-05, -4.4586e-05,  8.5573e-05,  8.5550e-06,\n",
      "        -1.0161e-05, -2.6811e-05, -2.9483e-05,  2.9364e-06,  2.0325e-05,\n",
      "        -4.8016e-05,  6.5172e-07, -1.8037e-06,  0.0000e+00, -2.6557e-05,\n",
      "         6.9426e-06,  4.4224e-05,  2.6056e-05, -6.9927e-05,  2.4115e-05,\n",
      "        -7.2937e-06, -2.6549e-05,  1.4053e-05, -1.0145e-05, -8.1725e-06,\n",
      "         1.1318e-04,  7.8939e-05, -7.6847e-05,  5.8463e-05, -5.0792e-05,\n",
      "        -1.4992e-05,  2.7752e-05,  4.5110e-06, -2.6454e-05,  1.3126e-04,\n",
      "        -5.6777e-05,  7.5395e-06,  2.0894e-05, -1.0202e-04, -4.3984e-05,\n",
      "         1.0885e-04, -5.2203e-05,  7.2469e-05, -1.1272e-04,  1.8053e-05,\n",
      "        -3.0980e-05,  3.4471e-06, -2.3022e-05,  3.5473e-05, -1.1446e-05,\n",
      "         8.2606e-05, -1.4173e-05,  9.3671e-06, -6.5391e-05, -3.8731e-06,\n",
      "        -1.8514e-04, -2.0775e-05,  1.8902e-05, -2.3315e-05, -8.9006e-05,\n",
      "         1.2488e-04, -1.2480e-05, -9.6246e-06,  1.4660e-05,  4.2025e-05,\n",
      "        -5.1920e-05,  1.4128e-05,  3.3857e-05, -1.8903e-05, -4.1930e-05,\n",
      "        -7.8659e-05, -8.7497e-05,  6.7088e-06, -2.1923e-05,  8.8009e-05,\n",
      "        -1.1801e-05, -5.6761e-05,  8.8314e-05, -5.5814e-05,  4.2280e-05,\n",
      "         1.2943e-04,  1.1426e-04,  2.5820e-05, -4.3138e-05,  4.3222e-05,\n",
      "         1.9656e-05,  1.0837e-05, -7.0981e-06, -1.1548e-05,  2.0221e-05,\n",
      "         3.8476e-05,  4.0985e-05, -4.8783e-06, -1.2493e-04,  8.8555e-05,\n",
      "         5.4295e-05, -4.0178e-05, -3.2555e-05,  2.5017e-05, -1.4304e-06,\n",
      "         3.5499e-06,  1.5035e-05,  1.1012e-05, -2.0446e-05,  1.5669e-05,\n",
      "        -3.2034e-05, -3.8518e-05, -1.7684e-05, -2.8837e-05,  4.7715e-05,\n",
      "         9.4663e-05,  8.5602e-05,  1.3275e-04, -8.5443e-06, -2.8177e-05,\n",
      "         7.4687e-05,  9.2099e-05,  7.4558e-06, -6.4871e-06,  1.3578e-04,\n",
      "         5.0257e-06, -1.4886e-06, -5.0755e-05,  3.8652e-05,  5.3873e-05,\n",
      "         8.2973e-06, -7.8812e-06, -3.7958e-05,  2.8423e-06,  6.6900e-06,\n",
      "        -2.8917e-05, -6.5801e-05, -4.9716e-05,  8.9243e-05,  7.7755e-05,\n",
      "         1.5263e-05,  3.3805e-05, -4.3485e-06, -9.6188e-06, -1.4742e-05,\n",
      "        -6.5933e-06, -3.9887e-05, -6.5603e-06, -1.1790e-06,  9.9317e-06,\n",
      "        -1.6776e-05, -3.1207e-05, -1.8897e-05, -6.1626e-06, -1.3250e-05,\n",
      "         8.5115e-06,  4.1694e-05, -6.2654e-05,  4.0613e-05, -7.4945e-05,\n",
      "        -6.0893e-05, -4.8755e-05,  2.7845e-06, -3.3210e-05,  1.2828e-05,\n",
      "         1.0018e-05,  4.5242e-06,  3.6224e-05, -3.2551e-05, -2.9715e-05,\n",
      "        -2.2488e-05, -3.2288e-06,  7.4339e-05,  5.0538e-05, -2.0336e-04,\n",
      "        -1.8082e-06, -1.4012e-05,  9.2570e-05,  5.7299e-05,  5.6050e-06,\n",
      "         4.1091e-05,  2.4393e-05, -7.7804e-05, -8.1644e-06,  6.1408e-05,\n",
      "         1.7281e-04, -3.8178e-05, -2.1570e-06, -5.8825e-05, -1.1528e-05,\n",
      "        -7.5631e-06, -4.5405e-06,  1.0237e-05, -1.3964e-05, -2.3241e-05,\n",
      "        -2.6693e-05,  1.3155e-06,  3.9720e-05, -1.0362e-04, -8.4069e-05,\n",
      "        -3.5397e-05, -6.2151e-06,  1.3571e-04,  2.8356e-05,  4.8461e-07,\n",
      "         9.5398e-06, -4.0734e-05, -9.9612e-05,  5.8257e-06,  1.1609e-06,\n",
      "        -5.0336e-06, -4.5839e-05,  4.0358e-06,  7.2391e-05,  6.4650e-05,\n",
      "         9.2294e-05,  1.1678e-05, -2.6171e-05,  1.7811e-05, -2.7707e-05,\n",
      "        -5.5360e-05, -1.1773e-06,  3.3983e-05,  1.3903e-04, -4.8500e-05,\n",
      "         7.9972e-06,  2.8289e-05,  5.0280e-06, -3.4744e-05,  4.1520e-06,\n",
      "        -5.6345e-05, -5.7222e-06, -1.9471e-05, -1.7794e-06,  1.6038e-05,\n",
      "         6.4822e-05,  8.9408e-05,  7.0692e-06,  5.5701e-06,  4.9657e-05,\n",
      "        -1.3383e-05,  2.7144e-06, -3.2052e-05,  1.8152e-05,  5.4398e-05,\n",
      "         1.1386e-04, -3.6598e-05,  2.1234e-05,  6.1684e-06, -4.1390e-06,\n",
      "        -4.7991e-05, -4.7010e-05,  1.1739e-04,  1.4062e-05,  3.2472e-06,\n",
      "         1.1500e-04,  4.1391e-07, -2.9201e-06,  8.7677e-05, -1.9420e-05,\n",
      "         4.5366e-06, -3.4118e-05,  1.4830e-04,  5.4295e-05, -7.5335e-07,\n",
      "        -3.1061e-06,  4.5772e-05,  3.6551e-06,  1.7856e-05, -4.9045e-05,\n",
      "         2.8000e-05,  2.9852e-05,  1.7014e-05, -4.2229e-05, -3.6996e-05,\n",
      "         1.9498e-05,  2.2615e-05,  3.3218e-05,  7.0679e-06,  1.6702e-06,\n",
      "        -4.0286e-05, -2.8774e-05, -9.2975e-07,  9.7450e-06, -9.4561e-06,\n",
      "        -7.4969e-05,  2.0376e-05,  2.3465e-05, -7.6607e-06,  7.6214e-05,\n",
      "        -5.8485e-06,  4.5315e-05,  5.1161e-05,  5.4159e-05, -8.2250e-07,\n",
      "        -2.9723e-05, -6.1707e-05,  1.7795e-05,  2.7498e-05, -8.4997e-07,\n",
      "         4.0693e-05, -4.5918e-05, -4.0317e-05, -2.6329e-05, -3.1599e-06,\n",
      "         6.0237e-05,  3.3890e-05, -1.4507e-05, -3.6748e-05,  7.3965e-06,\n",
      "         5.0795e-05,  1.7695e-05, -4.6958e-06,  2.8740e-05, -6.6372e-05,\n",
      "         9.5579e-05, -2.7780e-05,  5.8912e-06,  9.2982e-05, -1.4656e-05,\n",
      "         3.3413e-05, -2.5761e-05, -1.4180e-05,  2.4365e-06, -4.8605e-06,\n",
      "        -1.0880e-05, -1.2709e-05, -1.1717e-05, -2.0636e-05,  8.7066e-06,\n",
      "        -1.6291e-05, -1.1150e-04, -6.6001e-06, -1.5693e-06, -2.6278e-06,\n",
      "        -6.1146e-05, -7.5802e-05, -1.2589e-05, -1.8855e-06, -2.4390e-05,\n",
      "        -8.3037e-05,  8.9503e-05, -3.6715e-06, -1.6319e-05, -6.0327e-06,\n",
      "         7.5466e-06,  3.0249e-05,  5.0627e-05, -4.8138e-05,  3.1144e-05,\n",
      "         3.0889e-05,  1.1966e-05,  1.8970e-05, -1.0136e-05,  3.7748e-06,\n",
      "        -1.0258e-05, -4.9438e-06,  2.3053e-05,  7.9854e-05,  1.0493e-05,\n",
      "         1.0409e-04,  2.0575e-05,  7.8187e-06, -3.4869e-05,  1.7111e-06,\n",
      "        -7.3205e-06, -6.8381e-05,  5.5677e-05,  2.9053e-05,  2.2441e-06,\n",
      "        -1.3502e-04, -1.2161e-04, -4.5072e-06, -6.3451e-08, -7.1939e-08,\n",
      "        -4.9744e-05,  1.0208e-04, -1.8286e-04, -9.5336e-05,  1.6361e-06,\n",
      "         8.5861e-06,  4.1811e-05, -6.0302e-07, -4.1991e-06,  6.6338e-06,\n",
      "         5.2169e-06, -9.5230e-06, -3.7368e-05, -6.4629e-05, -2.6819e-05,\n",
      "         8.1267e-06, -1.4806e-04, -1.5500e-05,  1.0476e-04, -1.4323e-05,\n",
      "         3.9063e-05,  1.0185e-05, -7.5908e-05,  4.6901e-06, -5.4604e-05,\n",
      "         3.2331e-05,  1.5346e-05,  1.5727e-05, -1.5424e-04, -1.0841e-04,\n",
      "        -4.6505e-05])\n",
      "name of a parameter: fc3.weight, gradient: tensor([[-3.1815e-03, -5.7210e-04, -4.3059e-03,  ..., -6.3111e-03,\n",
      "         -1.8387e-02, -3.6894e-02],\n",
      "        [-1.9040e-03, -1.1204e-04, -1.0670e-03,  ..., -4.3208e-03,\n",
      "         -2.5875e-03,  2.2910e-04],\n",
      "        [ 4.9241e-05, -1.9865e-04,  9.8574e-06,  ...,  5.1721e-04,\n",
      "         -2.2208e-05, -4.0901e-03],\n",
      "        ...,\n",
      "        [ 2.2292e-04,  5.0627e-04,  2.5441e-03,  ...,  4.9096e-03,\n",
      "          1.3414e-02,  4.1392e-03],\n",
      "        [ 8.2349e-04,  3.4683e-04,  5.0551e-03,  ...,  9.7589e-03,\n",
      "          2.3514e-02,  1.9191e-03],\n",
      "        [-3.7676e-03,  2.6256e-04,  4.3356e-03,  ..., -9.8868e-03,\n",
      "          1.5734e-02, -5.5172e-03]])\n",
      "name of a parameter: fc3.bias, gradient: tensor([-4.8231e-04, -9.3275e-05, -4.6901e-05, -1.7141e-04, -1.6654e-04,\n",
      "         4.9875e-05, -3.1396e-04, -3.0890e-05, -2.0086e-05,  1.6126e-04,\n",
      "         6.4546e-06,  3.6109e-04,  6.9389e-05,  7.2642e-05, -6.0483e-05,\n",
      "         1.0849e-04,  6.8525e-04,  9.8260e-06,  4.1410e-04,  2.7872e-04,\n",
      "        -4.5951e-05, -3.9727e-05, -1.0673e-04,  5.2678e-04,  1.3139e-04,\n",
      "         9.3785e-06,  9.2822e-06,  3.1939e-04, -4.7680e-05, -1.5292e-04,\n",
      "         2.2739e-04, -8.4826e-05,  1.2961e-05,  1.2863e-05, -2.3479e-04,\n",
      "        -2.0418e-05, -1.6597e-04, -3.8964e-04,  3.8952e-05,  1.2815e-04,\n",
      "        -1.3029e-04, -1.5927e-04,  1.9075e-04,  7.0930e-05,  1.4195e-04,\n",
      "        -3.0920e-04, -6.6044e-05, -5.7791e-04, -1.2269e-04,  2.4443e-04,\n",
      "         1.5403e-04, -1.9635e-05, -2.7044e-04, -6.3588e-05, -3.1447e-05,\n",
      "        -4.2035e-04,  1.0727e-04, -1.5951e-04, -1.6564e-04,  1.7008e-04,\n",
      "        -4.6599e-04,  5.4615e-04, -8.1731e-05, -4.0868e-05,  1.1821e-04,\n",
      "         7.4737e-05,  1.3823e-04, -1.8642e-04,  4.2826e-05, -3.5582e-04,\n",
      "        -1.1112e-04,  3.5708e-05, -5.0773e-04,  1.2261e-04, -1.2602e-04,\n",
      "         3.1149e-04,  9.9556e-05,  1.5455e-05, -1.6816e-04, -4.5319e-05,\n",
      "         3.1817e-05,  3.0945e-04,  1.3978e-04,  1.9462e-04,  9.0987e-05,\n",
      "         4.7788e-05,  1.6607e-04, -2.0561e-05,  3.7831e-04, -2.0375e-04,\n",
      "        -7.0874e-04, -4.0089e-05, -1.4319e-04,  1.1606e-04,  1.3445e-04,\n",
      "         1.5754e-04, -6.7486e-05,  2.2655e-04,  9.7581e-05,  4.7814e-06,\n",
      "         1.0086e-04,  1.5431e-04,  2.6405e-04, -1.0933e-04, -1.1487e-04,\n",
      "        -1.6522e-04, -3.4430e-04, -1.7058e-05,  2.8156e-04, -1.2286e-05,\n",
      "        -3.1545e-05,  4.3236e-05, -4.0146e-06, -4.2465e-04, -8.4050e-05,\n",
      "         2.2139e-04,  4.0387e-04, -6.2247e-05, -2.6135e-05,  2.3128e-05,\n",
      "         9.2460e-05, -4.0407e-05, -4.7838e-04, -8.6784e-05, -8.9378e-06,\n",
      "         3.0869e-05,  1.3684e-04,  1.1656e-04, -1.0909e-04, -5.1382e-05,\n",
      "         1.2276e-04,  3.7055e-04, -5.7159e-09,  1.9987e-04, -1.3817e-04,\n",
      "        -1.9087e-04,  5.1521e-06, -1.5681e-04,  2.2184e-04,  1.0903e-04,\n",
      "        -9.2274e-05,  1.4159e-04,  6.4937e-05, -3.0299e-04, -1.1982e-04,\n",
      "        -1.2945e-04,  6.2969e-06, -1.7410e-04, -7.9407e-05,  3.3605e-06,\n",
      "        -6.5784e-05, -1.6247e-04, -4.4086e-04, -8.7092e-05,  9.4364e-06,\n",
      "        -2.2672e-04,  7.3392e-05,  2.8246e-04,  4.1954e-04,  1.5141e-04])\n",
      "name of a parameter: fc4.weight, gradient: tensor([[ 3.7287e-02,  8.5285e-02, -3.0571e-04,  ...,  9.3835e-02,\n",
      "          5.5673e-02,  1.3248e-02],\n",
      "        [ 2.2528e-02,  6.8557e-02,  8.9862e-04,  ...,  7.2261e-02,\n",
      "          4.8727e-02, -1.7977e-02],\n",
      "        [-4.5600e-03, -6.3140e-06, -3.8798e-04,  ..., -3.3179e-04,\n",
      "         -4.0666e-04, -2.7323e-02],\n",
      "        ...,\n",
      "        [-1.4972e-02, -3.0485e-02, -5.2142e-04,  ..., -3.4381e-02,\n",
      "         -1.8015e-02, -5.4387e-03],\n",
      "        [-2.2937e-02, -4.2772e-02, -1.4769e-03,  ..., -4.3230e-02,\n",
      "         -2.8215e-02, -3.2762e-02],\n",
      "        [ 7.0448e-03,  1.9721e-02,  5.6907e-05,  ...,  2.0715e-02,\n",
      "          1.3839e-02, -2.4402e-03]])\n",
      "name of a parameter: fc4.bias, gradient: tensor([ 1.8264e-03, -5.8004e-05, -1.3666e-03, -4.7545e-04, -1.6026e-04,\n",
      "        -3.1641e-05, -6.2228e-04,  1.5371e-04, -2.7682e-04, -8.9401e-04,\n",
      "        -6.8379e-05, -2.7580e-04, -2.8880e-04,  8.4073e-04,  6.6347e-04,\n",
      "        -7.4921e-05, -8.2009e-04, -3.9063e-04,  5.8175e-05,  8.6015e-04,\n",
      "         8.4028e-04, -2.8057e-03,  5.2760e-04, -4.1306e-04, -5.9666e-05,\n",
      "        -4.8472e-04,  1.1265e-03, -8.6091e-04, -8.0838e-04,  3.8672e-03,\n",
      "        -4.1287e-04,  7.4790e-04, -3.1722e-04, -8.6062e-04, -3.0510e-04,\n",
      "        -1.1234e-05,  7.8114e-04, -5.9946e-04, -1.9279e-03,  1.1095e-04])\n",
      "name of a parameter: fc5.weight, gradient: tensor([[ 4.4810e-03,  1.0504e-01,  4.6438e-02,  6.5968e-02,  8.8380e-02,\n",
      "         -6.6792e-04, -2.4311e-04,  2.0146e-04,  3.6979e-03,  2.3969e-02,\n",
      "          1.0324e-02,  1.0761e-02,  1.5359e-01,  5.8730e-03,  1.5395e-01,\n",
      "          3.3758e-03,  6.2394e-04,  2.8850e-03, -4.5996e-04,  1.8197e-02,\n",
      "          3.3055e-03,  2.6419e-02,  3.4096e-02, -5.2865e-05,  6.9883e-02,\n",
      "          2.9727e-02,  1.5412e-01,  2.2273e-02,  2.2192e-03, -5.1478e-04,\n",
      "          3.3742e-04,  9.2670e-02,  7.8226e-03,  1.3339e-02,  1.0752e-01,\n",
      "          1.0767e-03,  1.4763e-02,  2.1436e-03,  9.4756e-02,  8.6620e-02],\n",
      "        [-1.6448e-02, -1.5271e-01,  8.1525e-03, -4.4676e-02, -1.0307e-01,\n",
      "         -7.1003e-03,  5.8094e-03, -9.1978e-04,  2.1303e-02,  2.7206e-02,\n",
      "         -5.8249e-02, -7.6362e-02, -1.1044e-01, -1.3273e-01,  6.0929e-02,\n",
      "         -1.5189e-02, -9.0243e-02, -1.3214e-02, -2.9418e-02,  8.2274e-02,\n",
      "         -2.9950e-02,  3.5311e-02,  3.5305e-02, -8.0666e-03,  1.9374e-03,\n",
      "          1.3229e-02,  2.0820e-02,  3.2604e-03, -1.3199e-01, -1.3089e-01,\n",
      "          2.7448e-02,  6.0002e-02, -2.9222e-02,  1.8707e-02, -2.5466e-02,\n",
      "          3.6160e-04, -2.7569e-03, -6.4253e-02, -1.4719e-01, -5.0596e-02],\n",
      "        [ 1.1967e-02,  4.7675e-02, -5.4591e-02, -2.1291e-02,  1.4691e-02,\n",
      "          7.7682e-03, -5.5663e-03,  7.1831e-04, -2.5001e-02, -5.1175e-02,\n",
      "          4.7925e-02,  6.5600e-02, -4.3153e-02,  1.2686e-01, -2.1488e-01,\n",
      "          1.1814e-02,  8.9619e-02,  1.0329e-02,  2.9878e-02, -1.0047e-01,\n",
      "          2.6645e-02, -6.1730e-02, -6.9400e-02,  8.1195e-03, -7.1821e-02,\n",
      "         -4.2956e-02, -1.7494e-01, -2.5534e-02,  1.2977e-01,  1.3141e-01,\n",
      "         -2.7786e-02, -1.5267e-01,  2.1399e-02, -3.2046e-02, -8.2054e-02,\n",
      "         -1.4383e-03, -1.2006e-02,  6.2110e-02,  5.2436e-02, -3.6024e-02]])\n",
      "name of a parameter: fc5.bias, gradient: tensor([ 0.0074, -0.0085,  0.0012])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_MLP.named_parameters():\n",
    "    print(\"name of a parameter: {}, gradient: {}\".\n",
    "          format(name, param.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we have to do now is subtract the gradient of a given parameter from the parameter tensor itself and do it for all parameters of the model - that should decrease the loss. Normally the gradient is multiplied by a learning rate parameter $\\lambda$ so we don't go too far in the loss landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "for param in model_MLP.parameters():\n",
    "    param.data.add_(-lr*param.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "call to backward **accumulates** gradients - so we also need to zero the gradient tensors if we want to keep going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_MLP.parameters():\n",
    "    param.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a much simpler way of doing this - we can use the pytorch [optim](https://pytorch.org/docs/stable/optim.html) classes. This allows us to easily use more advanced optimization options (like momentum or adaptive optimizers like [Adam](https://arxiv.org/abs/1412.6980)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD(model_MLP.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get a new batch of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1=next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=batch1[0]\n",
    "labels=batch1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 2, 1, 1, 1, 2, 0, 2, 0, 1, 2, 1, 0, 2, 0, 2, 2, 0, 2, 1, 1, 1, 1,\n",
      "        0, 1, 1, 2, 2, 0, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out=model_MLP(data)\n",
    "loss_tensor=loss_module(model_out,labels)\n",
    "loss_tensor.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could just put the code above in a loop and be done with it, but the usual practice would be to wrap this functionality in a training object. Here we'll use the [engine](/edit/utils/engine.py) class. Let's examine it. We'll talk about:\n",
    "  1. Implementation of the training loop\n",
    "  2. Evaluation on validation set and training and test modes.\n",
    "  3. Turning evaluation of gradients on and off.\n",
    "  4. Saving and retrieving the model and optimizer state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.engine import Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a configuration object -we'll use this to set up our training engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    pass\n",
    "config=CONFIG()\n",
    "config.batch_size_test =512\n",
    "config.batch_size_train = 256\n",
    "config.batch_size_val = 512\n",
    "config.lr=0.01\n",
    "config.device = 'cpu'\n",
    "config.num_workers_train=2\n",
    "config.num_workers_val=2\n",
    "config.num_workers_test=2\n",
    "config.dump_path = '../model_state_dumps'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sticking to CPU\n",
      "Creating a directory for run dump: ../model_state_dumps/20240425_150733/\n"
     ]
    }
   ],
   "source": [
    "engine=Engine(model_MLP,dset,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size_test': 512, 'batch_size_train': 256, 'batch_size_val': 512, 'lr': 0.01, 'device': 'cpu', 'num_workers_train': 2, 'num_workers_val': 2, 'num_workers_test': 2, 'dump_path': '../model_state_dumps'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Starting @ 2024-04-25 15:07:34\n",
      "Label: tensor([1, 2, 1, 1, 2, 2, 2, 2, 0, 0, 0, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 0, 2,\n",
      "        0, 2, 0, 1, 0, 2, 0, 2, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 2, 1, 2, 1,\n",
      "        0, 0, 1, 2, 0, 1, 1, 1, 0, 1, 1, 0, 0, 2, 0, 0, 2, 0, 2, 2, 2, 0, 2, 0,\n",
      "        0, 1, 1, 1, 2, 1, 1, 0, 2, 2, 1, 2, 1, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 0,\n",
      "        0, 0, 0, 1, 2, 0, 2, 2, 1, 2, 2, 0, 2, 2, 2, 0, 1, 1, 2, 0, 1, 0, 1, 1,\n",
      "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 0,\n",
      "        0, 2, 1, 2, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 2, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 2, 1, 0, 1, 1, 1, 0, 2, 1, 2, 0, 1, 0, 1, 0, 2, 2, 2, 1, 0, 0,\n",
      "        2, 1, 1, 0, 1, 2, 1, 2, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0, 2, 0, 1, 1, 1,\n",
      "        1, 2, 2, 0, 0, 0, 0, 1, 2, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 2, 1, 2, 0,\n",
      "        1, 2, 2, 0, 2, 0, 0, 0, 2, 1, 0, 2, 1, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 0,\n",
      "        2, 1, 1, 1, 1, 1, 2, 0, 0, 2, 2, 0, 2, 0, 1, 0, 2, 1, 2, 2, 2, 2, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1, 2, 0, 1, 2, 0, 2, 0, 0, 0, 2, 1, 0, 1, 1, 1, 2, 0, 1,\n",
      "        1, 0, 2, 2, 2, 2, 0, 1, 0, 1, 2, 0, 1, 0, 2, 2, 1, 1, 0, 0, 1, 0, 0, 2,\n",
      "        0, 0, 1, 1, 1, 2, 0, 0, 0, 2, 1, 0, 0, 1, 0, 2, 1, 0, 2, 1, 2, 0, 0, 0,\n",
      "        1, 2, 0, 2, 2, 1, 0, 2, 0, 2, 0, 1, 2, 0, 0, 1, 1, 0, 1, 1, 0, 2, 0, 1,\n",
      "        1, 0, 2, 1, 1, 2, 2, 1, 1, 2, 0, 0, 0, 0, 2, 2, 1, 0, 0, 2, 2, 0, 1, 0,\n",
      "        1, 1, 1, 2, 2, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 2, 2, 0, 0, 1, 0, 1,\n",
      "        0, 1, 2, 1, 1, 2, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0, 1, 1, 1, 0, 1, 1, 0, 2,\n",
      "        2, 1, 1, 1, 1, 2, 2, 0, 2, 1, 0, 0, 1, 0, 0, 0, 2, 1, 1, 2, 2, 1, 2, 1,\n",
      "        0, 1, 0, 1, 1, 2, 2, 1, 1, 1, 0, 2, 2, 2, 1, 2, 2, 2, 0, 1, 0, 2, 1, 2,\n",
      "        1, 1, 1, 2, 2, 1, 0, 2])\n",
      "... Iteration 0 ... Epoch 0.00 ... Validation Loss 1.225 ... Validation Accuracy 0.326\n",
      "Saved checkpoint as: ../model_state_dumps/20240425_150733/SimpleMLP.pth\n",
      "best validation loss so far!: 1.2252730131149292\n",
      "Saved checkpoint as: ../model_state_dumps/20240425_150733/SimpleMLPBEST.pth\n",
      "Label: tensor([2, 2, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 2, 0, 2, 1, 0, 2, 0, 0, 1, 1, 2, 1,\n",
      "        1, 1, 1, 0, 2, 2, 2, 0, 0, 2, 0, 2, 1, 1, 0, 1, 2, 1, 2, 2, 2, 0, 1, 2,\n",
      "        2, 1, 2, 1, 2, 0, 0, 2, 1, 1, 2, 1, 2, 2, 0, 1, 1, 0, 2, 2, 1, 2, 2, 0,\n",
      "        2, 1, 0, 0, 0, 1, 1, 1, 2, 1, 0, 2, 0, 2, 2, 0, 2, 0, 2, 1, 0, 1, 0, 2,\n",
      "        1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 0, 2, 0, 1, 2, 1, 0, 1, 2, 1, 0, 2, 0,\n",
      "        0, 1, 2, 2, 1, 1, 0, 2, 0, 1, 2, 2, 1, 2, 2, 2, 0, 1, 1, 0, 0, 2, 1, 2,\n",
      "        2, 0, 0, 1, 2, 0, 2, 2, 2, 2, 2, 1, 0, 2, 0, 2, 2, 1, 0, 2, 0, 1, 0, 1,\n",
      "        2, 2, 2, 2, 0, 0, 1, 0, 1, 0, 0, 0, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 2, 1,\n",
      "        2, 2, 1, 1, 0, 0, 0, 0, 2, 1, 1, 2, 2, 0, 2, 0, 0, 2, 2, 1, 2, 2, 1, 0,\n",
      "        2, 1, 2, 1, 2, 1, 2, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 1, 2, 0, 2, 2, 1, 1, 1, 2, 0, 1, 0])\n",
      "... Iteration 1 ... Epoch 0.00 ... Loss 1.196 ... Accuracy 0.355\n",
      "Label: tensor([2, 1, 0, 1, 1, 2, 2, 2, 0, 0, 2, 1, 0, 2, 0, 0, 1, 2, 0, 2, 2, 0, 0, 2,\n",
      "        1, 2, 2, 1, 1, 0, 0, 2, 2, 0, 2, 1, 1, 1, 1, 2, 2, 0, 2, 0, 1, 0, 0, 2,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 2, 1, 2, 2, 1, 0, 0, 2, 2,\n",
      "        1, 2, 1, 0, 1, 1, 2, 2, 2, 0, 2, 2, 1, 2, 1, 2, 2, 1, 1, 0, 1, 2, 0, 0,\n",
      "        1, 2, 0, 2, 0, 1, 2, 2, 1, 0, 0, 0, 0, 0, 1, 0, 2, 2, 1, 2, 2, 0, 2, 1,\n",
      "        2, 2, 2, 0, 1, 0, 2, 1, 1, 2, 0, 1, 2, 2, 2, 0, 1, 1, 1, 1, 2, 0, 0, 2,\n",
      "        2, 2, 0, 0, 0, 1, 1, 2, 1, 0, 0, 0, 0, 2, 1, 1, 2, 0, 2, 1, 1, 1, 2, 1,\n",
      "        2, 2, 0, 2, 1, 2, 2, 1, 2, 1, 2, 0, 1, 1, 0, 2, 1, 2, 1, 0, 2, 1, 0, 0,\n",
      "        2, 1, 2, 1, 1, 1, 0, 0, 1, 2, 1, 2, 0, 0, 2, 2, 2, 0, 1, 1, 2, 2, 0, 1,\n",
      "        0, 0, 2, 2, 0, 0, 2, 2, 1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 2, 1, 1, 2, 0, 2,\n",
      "        2, 2, 0, 1, 1, 1, 0, 0, 1, 2, 1, 2, 0, 1, 0, 2])\n",
      "Label: tensor([1, 2, 2, 1, 2, 0, 2, 0, 0, 2, 1, 2, 0, 2, 0, 0, 0, 0, 0, 2, 1, 2, 1, 2,\n",
      "        2, 0, 2, 1, 0, 1, 2, 2, 1, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0, 1, 2, 0, 2, 1,\n",
      "        0, 0, 0, 2, 2, 2, 1, 2, 0, 1, 0, 1, 0, 1, 1, 2, 1, 1, 0, 1, 2, 1, 0, 2,\n",
      "        2, 2, 2, 2, 0, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 0, 2, 1, 1, 1, 0,\n",
      "        0, 2, 1, 0, 1, 2, 2, 1, 0, 2, 2, 0, 2, 1, 2, 2, 0, 2, 0, 1, 1, 2, 2, 2,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 2, 0, 1, 1, 1, 2, 0, 1, 2, 0, 0, 2, 2, 2, 1, 0,\n",
      "        2, 0, 0, 1, 2, 2, 2, 2, 1, 0, 2, 1, 0, 1, 2, 2, 1, 2, 2, 2, 1, 1, 0, 2,\n",
      "        2, 1, 2, 1, 0, 0, 0, 2, 0, 1, 0, 0, 2, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
      "        2, 2, 1, 0, 0, 0, 0, 0, 2, 2, 2, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 2, 0, 1,\n",
      "        0, 0, 1, 1, 1, 0, 0, 2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 2, 1, 1,\n",
      "        0, 0, 0, 2, 0, 1, 2, 2, 1, 1, 1, 2, 1, 2, 0, 1])\n",
      "Label: tensor([2, 0, 2, 1, 2, 0, 1, 0, 0, 2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 1,\n",
      "        0, 1, 1, 2, 2, 2, 0, 1, 0, 2, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 2, 2, 1, 0,\n",
      "        0, 0, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 2, 2, 0, 2, 2, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 0, 2, 0, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 0, 2, 2, 0, 1, 1, 1, 2, 2, 0, 1, 1, 0, 2, 2, 1, 1,\n",
      "        0, 1, 0, 2, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 1, 2, 2, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 2, 2, 2, 0, 2,\n",
      "        2, 0, 0, 1, 0, 0, 1, 2, 1, 0, 2, 1, 2, 1, 0, 2, 1, 0, 2, 0, 1, 1, 1, 2,\n",
      "        1, 0, 2, 1, 1, 1, 1, 2, 1, 0, 2, 2, 2, 1, 2, 0, 2, 1, 2, 0, 0, 1, 1, 2,\n",
      "        2, 1, 0, 2, 0, 0, 1, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0, 2, 0, 2, 2, 2, 1,\n",
      "        2, 1, 1, 2, 0, 1, 1, 2, 2, 1, 2, 2, 1, 0, 2, 0])\n",
      "Label: tensor([0, 2, 1, 1, 2, 2, 1, 2, 2, 0, 0, 0, 1, 0, 1, 2, 2, 2, 0, 1, 2, 0, 0, 2,\n",
      "        0, 1, 2, 1, 1, 0, 2, 1, 2, 2, 0, 2, 1, 1, 0, 1, 1, 1, 1, 0, 1, 2, 1, 0,\n",
      "        1, 0, 2, 0, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 0, 2, 1, 1, 2, 0, 2, 0, 0, 2,\n",
      "        0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 0, 1, 0, 2, 1, 0, 0, 0, 0, 1, 2, 2, 2, 2,\n",
      "        1, 0, 0, 1, 0, 2, 2, 1, 0, 1, 0, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2,\n",
      "        2, 0, 0, 0, 2, 2, 0, 2, 0, 1, 2, 0, 2, 0, 2, 1, 0, 1, 2, 0, 1, 0, 0, 1,\n",
      "        2, 2, 2, 1, 0, 2, 2, 1, 2, 0, 0, 1, 2, 2, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 0, 2, 0, 1, 0, 0, 1, 2, 2, 1, 0, 2, 2, 0, 1, 2, 2, 0, 0, 2, 2, 0,\n",
      "        1, 2, 0, 2, 0, 1, 0, 0, 1, 2, 1, 2, 0, 0, 1, 1, 1, 2, 2, 0, 2, 2, 1, 0,\n",
      "        2, 2, 1, 1, 2, 1, 2, 1, 2, 0, 2, 2, 2, 2, 1, 0, 0, 2, 1, 2, 0, 0, 0, 0,\n",
      "        2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1])\n",
      "Label: tensor([0, 0, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 2, 1, 2, 2, 2, 2, 1,\n",
      "        1, 2, 0, 1, 1, 1, 1, 0, 2, 2, 2, 2, 0, 2, 0, 1, 2, 2, 2, 0, 0, 1, 1, 2,\n",
      "        2, 1, 1, 1, 0, 1, 2, 0, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 0, 0, 1, 0, 2, 0,\n",
      "        0, 0, 1, 1, 2, 2, 0, 1, 0, 0, 0, 2, 2, 0, 1, 1, 1, 0, 2, 0, 1, 1, 0, 0,\n",
      "        0, 0, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 0, 1, 2, 2, 0, 0, 1, 2, 0, 2, 0,\n",
      "        2, 1, 1, 1, 2, 0, 0, 0, 2, 0, 1, 1, 1, 0, 0, 0, 2, 1, 0, 2, 0, 0, 2, 0,\n",
      "        2, 2, 1, 0, 1, 0, 2, 1, 2, 0, 2, 1, 0, 2, 1, 1, 1, 2, 1, 2, 0, 2, 1, 2,\n",
      "        1, 2, 1, 0, 0, 1, 2, 1, 1, 1, 0, 2, 0, 2, 2, 1, 0, 2, 2, 1, 1, 1, 2, 2,\n",
      "        2, 1, 2, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1, 2, 0, 0, 0, 1, 1, 2, 2, 1, 1, 0,\n",
      "        0, 1, 1, 2, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 2, 1,\n",
      "        1, 1, 2, 0, 1, 2, 0, 0, 2, 2, 0, 0, 1, 0, 2, 1])\n",
      "... Iteration 6 ... Epoch 0.02 ... Loss 1.247 ... Accuracy 0.305\n",
      "Label: tensor([2, 2, 2, 0, 1, 2, 0, 0, 2, 2, 1, 2, 1, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2,\n",
      "        2, 1, 2, 1, 0, 0, 2, 0, 0, 2, 1, 1, 1, 0, 1, 2, 0, 0, 2, 2, 2, 1, 1, 0,\n",
      "        2, 1, 1, 2, 2, 2, 0, 0, 0, 2, 0, 1, 1, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 0,\n",
      "        1, 0, 1, 0, 0, 2, 0, 0, 0, 1, 1, 2, 2, 0, 1, 0, 2, 0, 2, 0, 0, 0, 2, 0,\n",
      "        2, 1, 2, 1, 1, 1, 1, 2, 0, 0, 0, 1, 1, 2, 1, 0, 1, 0, 1, 1, 0, 2, 0, 0,\n",
      "        1, 1, 1, 2, 0, 1, 2, 1, 0, 1, 2, 0, 0, 1, 0, 2, 1, 2, 1, 2, 0, 0, 2, 0,\n",
      "        2, 2, 0, 2, 1, 1, 2, 2, 1, 1, 2, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 0, 2,\n",
      "        2, 2, 2, 2, 1, 2, 1, 0, 0, 2, 0, 1, 2, 1, 2, 0, 2, 0, 2, 1, 0, 0, 1, 0,\n",
      "        1, 2, 1, 2, 0, 1, 0, 2, 1, 1, 0, 2, 0, 2, 0, 0, 1, 1, 2, 2, 1, 2, 0, 0,\n",
      "        2, 0, 1, 0, 2, 2, 0, 2, 1, 2, 1, 1, 2, 0, 2, 1, 0, 0, 2, 0, 2, 0, 0, 0,\n",
      "        2, 1, 1, 1, 0, 2, 1, 0, 2, 0, 0, 1, 1, 0, 2, 2])\n",
      "Label: tensor([1, 1, 0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 0, 1, 0, 0, 1, 0, 2,\n",
      "        2, 1, 0, 0, 0, 0, 2, 2, 1, 1, 2, 1, 1, 1, 0, 1, 1, 2, 0, 1, 0, 0, 0, 1,\n",
      "        2, 2, 2, 2, 1, 1, 2, 0, 0, 1, 1, 1, 2, 1, 2, 1, 1, 0, 2, 0, 1, 0, 2, 0,\n",
      "        2, 2, 2, 1, 1, 2, 0, 2, 2, 2, 2, 2, 0, 1, 1, 1, 2, 2, 2, 0, 2, 1, 1, 1,\n",
      "        1, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 1, 1, 2, 0, 2, 1, 1, 1, 0, 2,\n",
      "        2, 0, 0, 0, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 0, 0, 2, 0, 1, 1, 0, 1, 1, 1,\n",
      "        2, 1, 2, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 2, 1, 2, 0, 1, 2, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 2, 2, 0, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 2, 2, 0, 0, 2, 0, 2, 2,\n",
      "        1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 0, 2, 2, 0, 1, 2, 1, 1, 1, 2, 0, 0, 2,\n",
      "        0, 1, 1, 0, 0, 0, 1, 2, 2, 1, 1, 0, 1, 0, 1, 1, 1, 2, 2, 1, 0, 1, 0, 2,\n",
      "        0, 2, 1, 0, 0, 1, 0, 1, 1, 1, 2, 1, 2, 2, 0, 0])\n",
      "Label: tensor([2, 2, 1, 0, 2, 2, 1, 0, 1, 2, 0, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 0, 1, 2,\n",
      "        2, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 2, 1, 2, 0, 2, 0, 1, 0, 1,\n",
      "        1, 2, 1, 1, 0, 0, 2, 0, 0, 2, 0, 0, 1, 2, 2, 0, 0, 2, 2, 0, 0, 0, 1, 0,\n",
      "        1, 1, 2, 0, 0, 1, 1, 0, 2, 2, 1, 0, 0, 0, 2, 2, 2, 0, 1, 2, 2, 0, 2, 2,\n",
      "        1, 0, 0, 2, 2, 1, 1, 2, 1, 0, 0, 2, 0, 1, 0, 1, 2, 0, 0, 1, 1, 0, 1, 2,\n",
      "        1, 2, 2, 0, 1, 2, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2,\n",
      "        2, 1, 0, 0, 1, 1, 2, 0, 2, 0, 0, 0, 1, 2, 2, 1, 1, 1, 2, 0, 2, 1, 1, 1,\n",
      "        2, 1, 0, 0, 0, 2, 0, 1, 0, 2, 1, 2, 0, 1, 1, 1, 2, 1, 0, 2, 1, 1, 1, 0,\n",
      "        0, 2, 1, 0, 2, 1, 2, 2, 1, 2, 0, 0, 2, 1, 1, 1, 0, 0, 1, 2, 0, 2, 0, 0,\n",
      "        1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 2, 1,\n",
      "        0, 2, 0, 1, 1, 1, 0, 0, 2, 1, 2, 2, 2, 2, 2, 1])\n",
      "Label: tensor([2, 0, 1, 1, 2, 1, 1, 2, 1, 0, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 2, 2, 0, 1,\n",
      "        0, 2, 0, 0, 0, 0, 0, 2, 1, 2, 2, 2, 0, 1, 0, 0, 1, 1, 2, 1, 0, 0, 0, 1,\n",
      "        0, 2, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 2, 1, 1, 2, 0, 2, 1, 0, 0, 0, 1, 2,\n",
      "        2, 2, 2, 2, 1, 0, 1, 2, 0, 2, 2, 0, 0, 2, 1, 1, 1, 0, 0, 2, 2, 0, 0, 2,\n",
      "        0, 2, 1, 0, 1, 1, 1, 1, 0, 1, 2, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 2, 2,\n",
      "        2, 1, 2, 2, 2, 1, 0, 0, 1, 0, 0, 0, 1, 2, 1, 2, 1, 1, 0, 1, 2, 0, 1, 1,\n",
      "        1, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 0, 2, 0, 0,\n",
      "        2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 1, 0, 2, 2, 1,\n",
      "        1, 0, 1, 2, 0, 1, 0, 1, 2, 2, 0, 1, 1, 2, 1, 2, 2, 2, 2, 1, 0, 1, 1, 1,\n",
      "        1, 2, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 2, 0, 0, 2, 1, 2, 2, 1, 2, 0, 1, 2,\n",
      "        0, 1, 1, 2, 1, 1, 1, 1, 2, 0, 0, 1, 1, 0, 2, 2])\n",
      "Label: tensor([2, 0, 0, 2, 0, 1, 2, 0, 0, 1, 0, 1, 1, 0, 1, 2, 1, 2, 0, 2, 0, 0, 0, 1,\n",
      "        2, 0, 0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 1, 2, 2, 1, 1, 2, 0, 1, 0, 1, 1,\n",
      "        2, 1, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 0, 0,\n",
      "        1, 0, 2, 1, 1, 1, 2, 0, 0, 2, 1, 1, 1, 0, 2, 0, 0, 0, 1, 0, 1, 2, 2, 0,\n",
      "        2, 2, 1, 2, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 2, 2, 0, 1, 0, 1, 1, 1,\n",
      "        2, 2, 1, 1, 2, 1, 1, 2, 0, 1, 2, 0, 0, 1, 0, 2, 1, 0, 0, 2, 1, 0, 0, 2,\n",
      "        2, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 2, 1, 2, 0, 0, 1, 2, 1,\n",
      "        2, 0, 1, 0, 1, 0, 0, 1, 1, 2, 1, 1, 0, 2, 1, 2, 2, 0, 2, 1, 0, 1, 0, 1,\n",
      "        2, 2, 0, 0, 0, 0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 1, 1, 2, 0, 0, 1, 1, 2,\n",
      "        2, 1, 1, 0, 1, 1, 2, 0, 2, 1, 0, 2, 0, 1, 0, 1, 0, 2, 1, 1, 1, 0, 2, 0,\n",
      "        2, 1, 1, 0, 0, 2, 2, 0, 0, 1, 0, 2, 0, 2, 0, 1])\n",
      "... Iteration 11 ... Epoch 0.04 ... Loss 1.239 ... Accuracy 0.312\n",
      "Label: tensor([1, 2, 0, 2, 2, 1, 1, 1, 1, 0, 0, 2, 1, 2, 1, 2, 1, 2, 2, 1, 0, 1, 2, 0,\n",
      "        0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 2, 0, 2, 0, 1, 0, 2, 1, 2, 1, 0, 0, 2, 1,\n",
      "        1, 2, 1, 0, 0, 1, 0, 0, 1, 1, 2, 2, 1, 2, 2, 1, 0, 0, 0, 2, 2, 0, 2, 1,\n",
      "        0, 0, 2, 1, 0, 1, 0, 0, 0, 1, 0, 2, 2, 1, 2, 0, 1, 2, 0, 0, 1, 0, 2, 0,\n",
      "        0, 0, 1, 2, 0, 1, 2, 1, 0, 0, 0, 1, 2, 1, 1, 2, 1, 0, 0, 2, 2, 1, 0, 2,\n",
      "        1, 0, 2, 1, 1, 1, 1, 2, 0, 0, 2, 0, 0, 0, 1, 2, 2, 1, 0, 1, 0, 2, 0, 2,\n",
      "        2, 2, 1, 2, 2, 2, 0, 0, 0, 1, 2, 1, 0, 2, 1, 0, 1, 0, 0, 0, 2, 0, 2, 2,\n",
      "        1, 0, 2, 1, 1, 1, 2, 2, 1, 2, 2, 0, 0, 1, 2, 2, 2, 0, 0, 2, 2, 1, 1, 0,\n",
      "        1, 2, 1, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 1, 1, 2, 1, 1, 0,\n",
      "        0, 1, 0, 2, 2, 2, 1, 2, 2, 2, 2, 0, 1, 0, 0, 0, 1, 0, 2, 2, 2, 0, 2, 2,\n",
      "        1, 1, 2, 1, 1, 0, 2, 1, 0, 2, 0, 2, 2, 0, 0, 0])\n",
      "Label: tensor([0, 0, 2, 1, 0, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 0, 2, 1, 0, 0, 2, 0, 0,\n",
      "        2, 0, 0, 2, 0, 2, 1, 0, 0, 1, 2, 0, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 0, 2,\n",
      "        1, 1, 2, 2, 0, 1, 2, 0, 2, 2, 0, 2, 1, 2, 1, 2, 1, 0, 0, 1, 0, 2, 1, 2,\n",
      "        2, 1, 0, 2, 1, 0, 2, 0, 1, 0, 0, 1, 2, 2, 2, 2, 0, 2, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 2, 2, 1, 0, 1, 2, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 0, 1, 1, 0,\n",
      "        2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 0, 0, 0, 2, 1, 2, 2, 2, 0, 1, 1, 0, 0,\n",
      "        0, 0, 2, 0, 1, 2, 0, 2, 0, 2, 2, 2, 0, 1, 1, 2, 0, 2, 2, 2, 2, 1, 2, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 2, 1, 0, 1, 0, 2, 2, 1, 2, 0, 1, 2, 0, 2, 0, 1, 1,\n",
      "        0, 1, 2, 2, 0, 0, 2, 1, 0, 0, 1, 0, 2, 2, 2, 1, 1, 0, 1, 0, 2, 1, 2, 0,\n",
      "        2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 1, 2, 1, 2, 0, 2, 2, 1, 1, 1, 1, 2, 0,\n",
      "        2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1])\n",
      "Label: tensor([1, 2, 1, 0, 2, 0, 1, 1, 2, 0, 1, 0, 1, 1, 2, 1, 1, 0, 2, 0, 1, 2, 2, 1,\n",
      "        0, 1, 2, 1, 2, 0, 2, 1, 1, 1, 0, 0, 2, 1, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2,\n",
      "        2, 1, 0, 2, 2, 1, 0, 2, 0, 1, 0, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 0, 0, 1,\n",
      "        2, 0, 1, 2, 2, 2, 1, 2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 1, 1, 2, 1, 0, 2, 1,\n",
      "        0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 2, 2, 1, 2, 1, 2, 2, 0, 2, 1, 0, 1, 1,\n",
      "        2, 0, 1, 0, 1, 0, 0, 2, 2, 0, 2, 0, 2, 2, 1, 2, 2, 2, 0, 1, 1, 2, 0, 0,\n",
      "        1, 0, 2, 1, 2, 0, 2, 1, 0, 2, 1, 1, 2, 0, 0, 1, 2, 1, 1, 0, 2, 1, 2, 0,\n",
      "        0, 1, 2, 1, 2, 1, 0, 2, 0, 2, 1, 1, 2, 1, 0, 2, 2, 2, 1, 2, 2, 2, 2, 0,\n",
      "        2, 2, 2, 2, 2, 0, 1, 0, 1, 2, 1, 2, 0, 0, 0, 2, 1, 2, 2, 2, 0, 0, 1, 0,\n",
      "        1, 1, 1, 2, 2, 2, 2, 0, 1, 0, 2, 0, 2, 1, 1, 0, 2, 2, 1, 2, 2, 1, 2, 0,\n",
      "        1, 1, 1, 2, 1, 2, 0, 0, 0, 0, 2, 2, 0, 1, 1, 1])\n",
      "Label: tensor([0, 2, 2, 0, 0, 1, 2, 0, 2, 2, 0, 0, 2, 0, 2, 2, 1, 0, 0, 1, 2, 2, 2, 2,\n",
      "        1, 2, 0, 0, 0, 2, 1, 0, 2, 1, 1, 2, 0, 2, 1, 1, 0, 0, 1, 1, 0, 2, 2, 0,\n",
      "        0, 2, 1, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 1, 2, 0, 1, 1, 1, 2, 1, 0,\n",
      "        2, 2, 1, 2, 0, 1, 2, 0, 0, 2, 0, 0, 1, 2, 0, 2, 1, 0, 2, 1, 1, 1, 0, 1,\n",
      "        1, 1, 2, 1, 2, 1, 2, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 2, 1,\n",
      "        1, 0, 0, 1, 0, 2, 1, 0, 1, 0, 2, 1, 0, 0, 0, 1, 0, 1, 0, 2, 2, 1, 0, 2,\n",
      "        2, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 0, 2,\n",
      "        0, 0, 1, 1, 1, 2, 0, 2, 1, 2, 0, 0, 0, 1, 2, 0, 2, 1, 1, 0, 1, 2, 0, 2,\n",
      "        1, 0, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 0, 1, 1, 2, 2, 0, 2, 1, 0,\n",
      "        0, 2, 0, 0, 2, 0, 1, 0, 0, 2, 2, 0, 1, 1, 1, 2, 0, 1, 1, 1, 0, 2, 0, 0,\n",
      "        2, 0, 2, 0, 2, 2, 1, 0, 0, 0, 0, 1, 0, 2, 2, 1])\n",
      "Label: tensor([2, 0, 2, 1, 1, 0, 2, 1, 2, 2, 0, 1, 0, 0, 2, 0, 2, 2, 1, 2, 2, 0, 2, 2,\n",
      "        2, 0, 1, 0, 0, 1, 1, 1, 0, 2, 1, 2, 2, 2, 2, 1, 0, 1, 0, 2, 2, 1, 2, 1,\n",
      "        0, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 0, 2, 1, 2, 2, 2, 0, 0, 1, 1, 1,\n",
      "        1, 1, 0, 2, 2, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 2, 1, 0, 2, 0, 2, 1, 0,\n",
      "        1, 0, 0, 2, 0, 2, 1, 0, 1, 1, 1, 0, 1, 2, 0, 0, 2, 0, 1, 2, 2, 2, 1, 2,\n",
      "        0, 2, 1, 2, 0, 1, 2, 1, 2, 2, 0, 2, 1, 1, 1, 2, 0, 0, 0, 1, 2, 1, 2, 0,\n",
      "        2, 1, 1, 0, 2, 1, 1, 2, 1, 2, 2, 0, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 1, 1, 2, 1, 0, 0, 0, 1, 1, 2,\n",
      "        2, 2, 0, 1, 0, 0, 2, 2, 2, 0, 1, 2, 0, 2, 0, 0, 1, 0, 0, 2, 2, 0, 0, 0,\n",
      "        1, 2, 1, 1, 2, 1, 0, 2, 1, 2, 2, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        2, 1, 1, 2, 2, 0, 0, 1, 2, 1, 2, 2, 0, 0, 1, 0])\n",
      "... Iteration 16 ... Epoch 0.05 ... Loss 1.212 ... Accuracy 0.340\n",
      "Label: tensor([0, 1, 2, 1, 2, 2, 2, 0, 2, 0, 1, 0, 0, 0, 0, 1, 2, 2, 0, 1, 0, 2, 0, 1,\n",
      "        1, 1, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 2, 0, 1, 1, 0, 1, 1, 1, 0, 2, 1, 0,\n",
      "        2, 0, 1, 1, 0, 1, 2, 2, 2, 1, 0, 1, 2, 2, 1, 1, 1, 0, 1, 1, 2, 2, 2, 2,\n",
      "        0, 2, 0, 1, 1, 1, 0, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0, 1, 1, 1, 2, 0, 2,\n",
      "        0, 0, 0, 0, 1, 2, 0, 0, 2, 2, 2, 0, 0, 1, 0, 1, 0, 2, 0, 2, 1, 1, 0, 1,\n",
      "        2, 1, 0, 0, 2, 0, 2, 1, 1, 1, 2, 2, 2, 0, 0, 1, 1, 0, 1, 2, 1, 2, 0, 1,\n",
      "        0, 2, 1, 0, 2, 0, 2, 0, 2, 2, 1, 0, 0, 1, 2, 1, 2, 0, 0, 0, 1, 2, 1, 2,\n",
      "        0, 2, 1, 1, 2, 0, 1, 0, 1, 2, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 1, 1, 0, 1,\n",
      "        2, 2, 0, 1, 1, 0, 2, 2, 0, 2, 1, 0, 2, 0, 1, 2, 0, 0, 2, 1, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 2, 2, 1, 0, 2, 0, 2, 2, 0, 1, 2, 2, 0, 2, 2, 2, 1, 1, 2, 2,\n",
      "        1, 1, 0, 0, 2, 0, 1, 0, 1, 2, 1, 1, 1, 2, 1, 2])\n",
      "Label: tensor([0, 1, 0, 1, 2, 1, 1, 2, 1, 0, 1, 2, 0, 1, 2, 1, 1, 1, 0, 1, 2, 0, 1, 2,\n",
      "        0, 2, 1, 0, 1, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 2, 1, 2, 2,\n",
      "        0, 2, 2, 0, 1, 0, 2, 0, 2, 2, 0, 0, 2, 1, 1, 1, 0, 1, 2, 0, 0, 1, 2, 1,\n",
      "        2, 2, 1, 0, 0, 2, 1, 1, 1, 2, 0, 0, 1, 1, 2, 1, 1, 0, 1, 2, 0, 1, 0, 0,\n",
      "        0, 2, 1, 0, 0, 1, 2, 0, 1, 1, 1, 0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 1, 0,\n",
      "        2, 1, 2, 1, 0, 1, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 1, 1, 2, 2,\n",
      "        1, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 1, 1, 0, 0, 1,\n",
      "        2, 1, 2, 2, 1, 0, 0, 0, 1, 0, 2, 2, 1, 2, 2, 1, 1, 0, 1, 1, 2, 0, 2, 1,\n",
      "        0, 1, 0, 2, 0, 0, 2, 2, 1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0, 2, 2, 2, 1,\n",
      "        0, 0, 2, 0, 2, 0, 0, 0, 0, 2, 0, 2, 1, 2, 0, 2, 0, 2, 2, 2, 1, 2, 0, 0,\n",
      "        2, 0, 2, 1, 2, 2, 2, 1, 2, 2, 0, 1, 2, 1, 2, 0])\n",
      "Label: tensor([2, 0, 0, 1, 0, 2, 2, 0, 2, 1, 1, 1, 1, 0, 0, 2, 1, 2, 1, 2, 0, 2, 0, 2,\n",
      "        0, 0, 0, 0, 1, 2, 1, 1, 1, 0, 2, 1, 1, 2, 1, 0, 2, 1, 1, 1, 0, 0, 0, 2,\n",
      "        1, 2, 2, 1, 0, 2, 1, 1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 2, 1, 2, 0, 1, 1,\n",
      "        2, 2, 0, 1, 2, 2, 2, 0, 1, 0, 1, 1, 2, 2, 2, 1, 2, 1, 2, 0, 0, 2, 2, 1,\n",
      "        0, 1, 2, 0, 2, 0, 2, 1, 2, 0, 0, 1, 1, 0, 1, 0, 2, 2, 1, 0, 2, 0, 0, 2,\n",
      "        0, 1, 2, 1, 2, 1, 0, 1, 2, 2, 2, 1, 1, 1, 1, 0, 2, 2, 0, 2, 1, 0, 1, 1,\n",
      "        1, 2, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 0, 2, 0, 0, 2, 2, 2, 1,\n",
      "        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 2, 2, 0, 0, 0, 1, 2, 0, 0, 1,\n",
      "        2, 2, 0, 1, 2, 0, 0, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 0, 0, 1, 2, 1, 1, 2,\n",
      "        0, 2, 0, 0, 2, 0, 1, 0, 2, 1, 1, 2, 1, 2, 0, 2, 0, 1, 0, 0, 1, 0, 2, 0,\n",
      "        0, 1, 0, 0, 2, 0, 1, 2, 0, 1, 2, 1, 0, 2, 1, 1])\n",
      "Label: tensor([1, 2, 0, 1, 2, 0, 0, 2, 1, 2, 2, 0, 2, 0, 0, 2, 2, 1, 2, 1, 0, 2, 0, 2,\n",
      "        1, 0, 2, 1, 1, 0, 2, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 2, 2, 1, 1, 1, 2,\n",
      "        2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 1, 2, 2, 1, 2, 2, 1, 1, 0, 1, 2, 2, 0,\n",
      "        0, 2, 2, 0, 2, 2, 0, 2, 1, 1, 1, 1, 2, 2, 1, 1, 0, 2, 0, 2, 1, 2, 2, 0,\n",
      "        1, 1, 2, 2, 1, 0, 0, 2, 1, 1, 2, 0, 0, 0, 0, 2, 1, 2, 1, 0, 1, 0, 2, 0,\n",
      "        2, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2, 0, 1, 2, 2, 2, 1, 0, 2, 2,\n",
      "        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 2, 0, 1, 1, 1, 0, 1, 0,\n",
      "        2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 1, 1, 0, 2, 1, 2, 0, 1, 0, 1, 2, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 2, 1, 1, 2, 0, 0, 2, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1,\n",
      "        0, 0, 1, 2, 2, 0, 2, 0, 0, 0, 0, 2, 2, 1, 1, 1])\n",
      "Label: tensor([0, 1, 0, 0, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 0, 0, 0, 2, 1, 1, 2, 2,\n",
      "        2, 2, 1, 2, 1, 0, 1, 0, 0, 0, 2, 2, 2, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 2, 0, 2, 2, 2, 2, 1, 0, 1, 1, 2, 0, 1, 2,\n",
      "        0, 0, 0, 2, 1, 1, 0, 2, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 1, 2, 2, 0, 0,\n",
      "        0, 2, 2, 2, 2, 0, 1, 2, 2, 2, 0, 0, 0, 1, 1, 2, 1, 2, 1, 2, 0, 0, 2, 2,\n",
      "        1, 1, 2, 1, 0, 2, 1, 2, 0, 2, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1,\n",
      "        0, 1, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 2, 0, 0, 1, 0, 2, 2, 0, 0, 0, 0,\n",
      "        0, 0, 2, 2, 2, 1, 0, 2, 1, 0, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 1, 1, 1, 0,\n",
      "        1, 2, 2, 0, 2, 1, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 2, 1, 0, 0, 0, 1, 2, 0,\n",
      "        1, 0, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 0, 2, 2, 0, 1, 2, 2, 0, 1, 1, 2, 0,\n",
      "        0, 1, 2, 1, 2, 0, 0, 2, 2, 0, 0, 2, 1, 2, 0, 0, 2, 1, 1, 2, 1, 0, 2, 2,\n",
      "        2, 0, 0, 2, 2, 0, 2, 1, 2, 2, 0, 0, 0, 2, 2, 1, 0, 0, 2, 2, 1, 2, 1, 1,\n",
      "        0, 2, 0, 0, 0, 2, 1, 1, 1, 1, 2, 2, 0, 2, 1, 2, 2, 1, 2, 0, 0, 1, 1, 2,\n",
      "        0, 2, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 1, 2, 1, 0, 0, 0,\n",
      "        2, 1, 0, 1, 2, 2, 0, 1, 0, 1, 1, 2, 1, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, 2,\n",
      "        0, 0, 0, 0, 1, 0, 0, 2, 0, 2, 0, 0, 2, 2, 1, 0, 1, 2, 0, 1, 0, 2, 2, 2,\n",
      "        2, 2, 1, 1, 0, 2, 1, 0, 2, 2, 2, 0, 2, 1, 2, 0, 1, 1, 0, 1, 2, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 1, 0, 1, 2, 2, 1, 1, 2, 0, 2, 2, 0, 0, 0, 2, 0, 2, 2, 0,\n",
      "        0, 2, 0, 1, 1, 1, 2, 2, 0, 0, 2, 0, 2, 1, 1, 2, 2, 1, 1, 0, 0, 1, 2, 0,\n",
      "        0, 2, 0, 0, 1, 0, 2, 1, 2, 0, 0, 2, 1, 2, 0, 2, 2, 2, 0, 0, 2, 2, 1, 1,\n",
      "        2, 2, 0, 1, 1, 1, 0, 2, 0, 1, 2, 1, 0, 2, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 0, 1, 0])\n",
      "... Iteration 20 ... Epoch 0.06 ... Validation Loss 1.200 ... Validation Accuracy 0.352\n",
      "Saved checkpoint as: ../model_state_dumps/20240425_150733/SimpleMLP.pth\n",
      "best validation loss so far!: 1.1998823881149292\n",
      "Saved checkpoint as: ../model_state_dumps/20240425_150733/SimpleMLPBEST.pth\n",
      "Label: tensor([0, 0, 1, 0, 1, 1, 2, 1, 2, 2, 1, 1, 2, 0, 1, 2, 2, 0, 0, 1, 2, 1, 2, 0,\n",
      "        0, 1, 2, 1, 0, 2, 1, 1, 1, 0, 1, 0, 2, 1, 2, 1, 1, 2, 0, 0, 1, 2, 0, 1,\n",
      "        2, 1, 2, 0, 2, 2, 1, 2, 2, 2, 2, 1, 1, 0, 1, 0, 1, 2, 2, 0, 2, 2, 1, 2,\n",
      "        1, 2, 2, 0, 0, 0, 1, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 0, 2,\n",
      "        0, 1, 2, 1, 2, 0, 1, 2, 1, 0, 0, 2, 1, 1, 1, 1, 2, 0, 0, 1, 2, 2, 0, 1,\n",
      "        0, 2, 0, 0, 0, 1, 2, 1, 2, 0, 0, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 0, 1, 2,\n",
      "        1, 2, 2, 2, 2, 0, 2, 1, 1, 0, 0, 0, 0, 2, 1, 0, 1, 1, 2, 2, 1, 1, 0, 2,\n",
      "        0, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 1, 2, 0, 0, 0, 1,\n",
      "        1, 2, 1, 1, 0, 2, 2, 2, 2, 0, 1, 0, 0, 0, 2, 1, 0, 2, 1, 2, 2, 1, 2, 2,\n",
      "        2, 0, 2, 1, 1, 1, 2, 1, 2, 2, 1, 0, 1, 1, 1, 0, 1, 1, 0, 2, 0, 0, 2, 1,\n",
      "        0, 2, 2, 1, 0, 0, 0, 1, 2, 2, 2, 1, 1, 0, 1, 1])\n",
      "... Iteration 21 ... Epoch 0.07 ... Loss 1.192 ... Accuracy 0.359\n",
      "Label: tensor([1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 2, 1, 2, 0, 1, 0, 0, 0, 0, 1, 1, 2, 1, 1,\n",
      "        0, 1, 0, 1, 2, 2, 0, 1, 2, 0, 1, 2, 0, 0, 0, 2, 2, 1, 2, 0, 1, 2, 0, 1,\n",
      "        0, 0, 2, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1,\n",
      "        1, 0, 2, 0, 0, 2, 1, 2, 2, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 0, 2, 2, 1, 1,\n",
      "        2, 0, 2, 1, 2, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 2,\n",
      "        0, 2, 2, 2, 1, 2, 1, 0, 2, 1, 2, 0, 0, 0, 2, 1, 0, 2, 1, 2, 1, 1, 2, 1,\n",
      "        1, 1, 0, 0, 0, 2, 1, 2, 0, 0, 1, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 0, 0, 0,\n",
      "        2, 1, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 2, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 1, 2, 1,\n",
      "        1, 1, 2, 2, 1, 0, 1, 2, 1, 2, 0, 0, 1, 0, 1, 0, 2, 2, 2, 0, 2, 2, 1, 0,\n",
      "        0, 0, 0, 1, 2, 0, 1, 1, 1, 1, 0, 2, 0, 2, 1, 0])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m/project/6008045/fcormier/t2k_ml/tutorials/Science_Week_ML_tutorial/utils/engine.py:151\u001b[0m, in \u001b[0;36mEngine.train\u001b[0;34m(self, epochs, report_interval, valid_interval)\u001b[0m\n\u001b[1;32m    149\u001b[0m j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Loop over data samples and into the network forward function\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dldr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    152\u001b[0m \n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# once in a while run valiation\u001b[39;49;00m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# as a sanity check run validation before we start training\u001b[39;49;00m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43mvalid_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_test_py3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/jupyter_test_py3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_test_py3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/jupyter_test_py3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/multiprocessing/connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/multiprocessing/connection.py:423\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 423\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "engine.train(epochs=1,report_interval=5,valid_interval=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a simple Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open [simpleCNN](http://localhost:8888/edit/models/simpleCNN.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simpleCNN import SimpleCNN\n",
    "model_CNN=SimpleCNN(num_input_channels=38,num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def rotate_chan(x):\n",
    "    return np.transpose(x,(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset=WCH5Dataset(\"/scratch/fcormier/Public/NUPRISM.h5\",val_split=0.1,test_split=0.1,transform=rotate_chan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sticking to CPU\n",
      "Creating a directory for run dump: ../model_state_dumps/20240424_223542/\n"
     ]
    }
   ],
   "source": [
    "engine=Engine(model_CNN,dset,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of a parameter: f_embed.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_embed.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv1.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv1.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv2a.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv2a.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv2b.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv2b.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv3a.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv3a.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv3b.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv3b.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv4.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: f_conv4.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc1.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc1.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc2.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc2.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc3.weight, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n",
      "name of a parameter: fc3.bias, type: <class 'torch.nn.parameter.Parameter'>, parameter requires a gradient?: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_CNN.named_parameters():\n",
    "    print(\"name of a parameter: {}, type: {}, parameter requires a gradient?: {}\".\n",
    "          format(name, type(param),param.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Starting @ 2024-04-24 22:35:47\n",
      "tensor([[-0.2139,  0.2188,  0.1590],\n",
      "        [-0.2356,  0.1918,  0.1596]])\n",
      "... Iteration 0 ... Epoch 0.00 ... Validation Loss 1.004 ... Validation Accuracy 0.000\n",
      "Saved checkpoint as: ../model_state_dumps/20240424_223542/SimpleCNN.pth\n",
      "best validation loss so far!: 1.0036206245422363\n",
      "Saved checkpoint as: ../model_state_dumps/20240424_223542/SimpleCNNBEST.pth\n",
      "tensor([[-0.2629,  0.2181,  0.2056]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Unfortunately this seems to hang and not train\n",
    "%%time\n",
    "engine.train(epochs=5,report_interval=1,valid_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HKCA2 Python 3.x Kernel",
   "language": "python",
   "name": "hk_ca_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
